<!doctypehtml><html lang=zh-CN><meta charset=UTF-8><meta content=width=device-width name=viewport><meta content=#222 name=theme-color><meta content="Hexo 7.3.0" name=generator><link href=/images/apple-touch-icon-next.png rel=apple-touch-icon sizes=180x180><link href=/images/favicon-32x32-next.png rel=icon sizes=32x32 type=image/png><link href=/images/favicon-16x16-next.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><style>:root{--body-bg-color:#eee;--content-bg-color:rgba(255,255,255,0.9);--card-bg-color:#f5f5f5;--text-color:#555;--selection-bg:#262a30;--selection-color:#eee;--blockquote-color:#666;--link-color:#555;--link-hover-color:#222;--brand-color:#fff;--brand-hover-color:#fff;--table-row-odd-bg-color:#f9f9f9;--table-row-hover-bg-color:#f5f5f5;--menu-item-bg-color:#f5f5f5;--theme-color:#222;--btn-default-bg:#fff;--btn-default-color:#555;--btn-default-border-color:#555;--btn-default-hover-bg:#222;--btn-default-hover-color:#fff;--btn-default-hover-border-color:#222;--highlight-background:#0d1117;--highlight-foreground:#c9d1d9;--highlight-gutter-background:#1f242a;--highlight-gutter-foreground:#b6bdc5;color-scheme:light}html{line-height:1.15;-webkit-text-size-adjust:100%}details,main{display:block}pre{font-size:1em;overflow:auto;padding:10px}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:ButtonText dotted 1px}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}.table-container,textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}summary{display:list-item}[hidden],template{display:none}::selection{background:var(--selection-bg);color:var(--selection-color)}body,html{height:100%}body{margin:0;background:var(--body-bg-color);box-sizing:border-box;color:var(--text-color);font-family:Noto Serif SC,PingFang SC,Microsoft YaHei,sans-serif,Lato,"Noto Serif SC","PingFang SC","Microsoft YaHei",sans-serif;font-size:1.1em;line-height:2;min-height:100%;position:relative;transition:padding .2s ease-in-out}h1,h2,h3,h4,h5,h6{font-family:Noto Serif SC,PingFang SC,Microsoft YaHei,sans-serif,Lato,'PingFang SC','Microsoft YaHei',sans-serif;font-weight:700;line-height:1.5;margin:30px 0 15px}p{margin:0 0 20px}a{background:0 0;border-bottom:1px solid #999;color:var(--link-color);cursor:pointer;outline:0;text-decoration:none;overflow-wrap:break-word}a:hover{border-bottom-color:var(--link-hover-color);color:var(--link-hover-color)}embed,iframe,img,video{display:block;margin-left:auto;margin-right:auto;max-width:100%}hr{box-sizing:content-box;overflow:visible;background-image:repeating-linear-gradient(-45deg,#ddd,#ddd 4px,transparent 4px,transparent 8px);border:0;height:3px;margin:40px 0}blockquote{border-left:4px solid #ddd;color:var(--blockquote-color);margin:0;padding:0 15px}blockquote cite::before{content:'-';padding:0 5px}dt{font-weight:700}dd{margin:0;padding:0}table{border-collapse:collapse;border-spacing:0;font-size:.875em;margin:0 0 20px;width:100%}tbody tr:nth-of-type(odd){background:var(--table-row-odd-bg-color)}tbody tr:hover{background:var(--table-row-hover-bg-color)}caption,td,th{padding:8px}td,th{border:1px solid #ddd;border-bottom:3px solid #ddd}th{font-weight:700;padding-bottom:10px}td{border-bottom-width:1px}.btn{background:var(--btn-default-bg);border:2px solid var(--btn-default-border-color);border-radius:2px;color:var(--btn-default-color);display:inline-block;font-size:.875em;line-height:2;padding:0 20px;transition:background-color .2s ease-in-out}.btn:hover{background:var(--btn-default-hover-bg);border-color:var(--btn-default-hover-border-color);color:var(--btn-default-hover-color)}.btn+.btn{margin:0 0 8px 8px}.btn .fa-fw{text-align:left;width:1.285714285714286em}.toggle{line-height:0}.toggle .toggle-line{background:#fff;display:block;height:2px;left:0;position:relative;top:0;transition:left .4s,opacity .4s,top .4s,transform .4s,width .4s;width:100%}.toggle .toggle-line:first-child{margin-top:1px}.toggle .toggle-line:not(:first-child){margin-top:4px}.toggle.toggle-arrow :first-child{left:50%;top:2px;transform:rotate(45deg);width:50%}.toggle.toggle-arrow :last-child{left:50%;top:-2px;transform:rotate(-45deg);width:50%}.toggle.toggle-close :nth-child(2){opacity:0}.toggle.toggle-close :first-child{top:6px;transform:rotate(45deg)}.toggle.toggle-close :last-child{top:-6px;transform:rotate(-45deg)}pre code.hljs{display:block;overflow-x:auto;padding:1em}code.hljs{padding:3px 5px}/*!
  Theme: GitHub Dark
  Description: Dark theme as seen on github.com
  Author: github.com
  Maintainer: @Hirse
  Updated: 2021-05-15

  Outdated base version: https://github.com/primer/github-syntax-dark
  Current colors taken from GitHub's CSS
*/.hljs{color:#c9d1d9;background:#0d1117}.hljs-doctag,.hljs-keyword,.hljs-meta .hljs-keyword,.hljs-template-tag,.hljs-template-variable,.hljs-type,.hljs-variable.language_{color:#ff7b72}.hljs-title,.hljs-title.class_,.hljs-title.class_.inherited__,.hljs-title.function_{color:#d2a8ff}.hljs-attr,.hljs-attribute,.hljs-literal,.hljs-meta,.hljs-number,.hljs-operator,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-id,.hljs-variable{color:#79c0ff}.hljs-meta .hljs-string,.hljs-regexp,.hljs-string{color:#a5d6ff}.hljs-built_in,.hljs-symbol{color:#ffa657}.hljs-code,.hljs-comment,.hljs-formula{color:#8b949e}.hljs-name,.hljs-quote,.hljs-selector-pseudo,.hljs-selector-tag{color:#7ee787}.hljs-subst{color:#c9d1d9}.hljs-section{color:#1f6feb;font-weight:700}.hljs-bullet{color:#f2cc60}.hljs-emphasis{color:#c9d1d9;font-style:italic}.hljs-strong{color:#c9d1d9;font-weight:700}.hljs-addition{color:#aff5b4;background-color:#033a16}.hljs-deletion{color:#ffdcd7;background-color:#67060c}.code-container:hover .copy-btn,.highlight:hover .copy-btn{opacity:1}.code-container{position:relative}.copy-btn{color:#333;cursor:pointer;line-height:1.6;opacity:0;padding:2px 6px;position:absolute;transition:opacity .2s ease-in-out;color:var(--highlight-foreground);font-size:14px;right:0;top:2px}figure.highlight{border-radius:5px;box-shadow:0 10px 30px 0 rgba(0,0,0,.4);padding-top:30px;overflow:auto;position:relative}figure.highlight .table-container{border-radius:0 0 5px 5px}figure.highlight::before{background:#fc625d;box-shadow:20px 0 #fdbc40,40px 0 #35cd4b;left:12px;margin-top:-20px;position:absolute;border-radius:50%;content:' ';height:12px;width:12px}.expand-btn{bottom:0;color:var(--highlight-foreground);cursor:pointer;display:none;left:0;right:0;position:absolute;text-align:center}.fold-cover{background-image:linear-gradient(to top,var(--highlight-background) 0,rgba(0,0,0,0) 100%);bottom:0;display:none;height:50px;left:0;right:0;position:absolute}.highlight-fold{max-height:500px;overflow-y:hidden!important}.highlight-fold .expand-btn,.highlight-fold .fold-cover{display:block}code,figure.highlight,kbd,pre{background:var(--highlight-background);color:var(--highlight-foreground)}figure.highlight,pre{line-height:1.6;margin:0 auto 20px}figure.highlight figcaption,pre .caption{background:var(--highlight-gutter-background);color:var(--highlight-foreground);display:flow-root;font-size:.875em;line-height:1.2;padding:.5em}figure.highlight figcaption a,pre .caption a{color:var(--highlight-foreground);float:right}figure.highlight figcaption a:hover,pre .caption a:hover{border-bottom-color:var(--highlight-foreground)}code,pre{font-family:Roboto Mono,consolas,Menlo,monospace,'PingFang SC','Microsoft YaHei'}code{border-radius:3px;font-size:.875em;padding:2px 4px;overflow-wrap:break-word;background:#f1f2f6;color:#e74c3c}kbd{border:2px solid #ccc;border-radius:.2em;box-shadow:.1em .1em .2em rgba(0,0,0,.1);font-family:inherit;padding:.1em .3em;white-space:nowrap}figure.highlight pre{border:0;margin:0;padding:10px 0}figure.highlight table{border:0;margin:0;width:auto}figure.highlight td{border:0;padding:0}figure.highlight .gutter{-webkit-user-select:none;user-select:none}figure.highlight .gutter pre{background:var(--highlight-gutter-background);color:var(--highlight-gutter-foreground);padding-left:10px;padding-right:10px;text-align:right}figure.highlight .code pre{padding-left:10px;width:100%}figure.highlight .marked{background:rgba(0,0,0,.3)}pre .caption{margin-bottom:10px}.gist table{width:auto}.gist table td{border:0}pre code{background:0 0;padding:0;text-shadow:none}.blockquote-center{border-left:0;margin:40px 0;padding:0;position:relative;text-align:center}.blockquote-center::after,.blockquote-center::before{left:0;line-height:1;opacity:.6;position:absolute;width:100%}.blockquote-center::before{border-top:1px solid #ccc;text-align:left;top:-20px;content:'\f10d';font-family:'Font Awesome 6 Free';font-weight:900}.blockquote-center::after{border-bottom:1px solid #ccc;bottom:-20px;text-align:right;content:'\f10e';font-family:'Font Awesome 6 Free';font-weight:900}.blockquote-center div,.blockquote-center p{text-align:center}.group-picture{margin-bottom:20px}.group-picture .group-picture-row{display:flex;gap:3px;margin-bottom:3px}.group-picture .group-picture-column{flex:1}.group-picture .group-picture-column img{height:100%;margin:0;object-fit:cover;width:100%}.post-body .label{color:#555;padding:0 2px}.post-body .label.default{background:#f0f0f0}.post-body .label.primary{background:#efe6f7}.post-body .label.info{background:#e5f2f8}.post-body .label.success{background:#e7f4e9}.post-body .label.warning{background:#fcf6e1}.post-body .label.danger{background:#fae8eb}.post-body .link-grid{display:grid;grid-gap:1.5rem;gap:1.5rem;grid-template-columns:repeat(auto-fill,minmax(300px,1fr));margin-bottom:20px;padding:1rem}.post-body .link-grid .link-grid-container{border:solid #ddd;box-shadow:1rem 1rem .5rem rgba(0,0,0,.5);min-height:5rem;min-width:0;padding:.5rem;position:relative;transition:background .3s}.post-body .link-grid .link-grid-container:hover{animation:.5s next-shake;background:var(--card-bg-color)}.post-body .link-grid .link-grid-container:active{box-shadow:.5rem .5rem .25rem rgba(0,0,0,.5);transform:translate(.2rem,.2rem)}.post-body .link-grid .link-grid-container .link-grid-image{border:1px solid #ddd;border-radius:50%;box-sizing:border-box;height:5rem;padding:3px;position:absolute;width:5rem}.post-body .link-grid .link-grid-container p{margin:0 1rem 0 6rem}.post-body .link-grid .link-grid-container p:first-of-type{font-size:1.2em}.post-body .link-grid .link-grid-container p:last-of-type{font-size:.8em;line-height:1.3rem;opacity:.7}.post-body .link-grid .link-grid-container a{border:0;height:100%;left:0;position:absolute;top:0;width:100%}@keyframes next-shake{0%{transform:translate(1pt,1pt) rotate(0)}10%{transform:translate(-1pt,-2pt) rotate(-1deg)}20%{transform:translate(-3pt,0) rotate(1deg)}30%{transform:translate(3pt,2pt) rotate(0)}40%{transform:translate(1pt,-1pt) rotate(1deg)}50%{transform:translate(-1pt,2pt) rotate(-1deg)}60%{transform:translate(-3pt,1pt) rotate(0)}70%{transform:translate(3pt,1pt) rotate(-1deg)}80%{transform:translate(-1pt,-1pt) rotate(1deg)}90%{transform:translate(1pt,2pt) rotate(0)}100%{transform:translate(1pt,-2pt) rotate(-1deg)}}.post-body .note{border-radius:3px;margin-bottom:20px;padding:1em;position:relative;border:1px solid #eee;border-left-width:5px}.post-body .note summary{cursor:pointer;outline:0}.post-body .note summary p{display:inline}.post-body .note h2,.post-body .note h3,.post-body .note h4,.post-body .note h5,.post-body .note h6{border-bottom:initial;margin:0;padding-top:0}.post-body .note :first-child{margin-top:0}.post-body .note :last-child{margin-bottom:0}.post-body .note.default{border-left-color:#777}.post-body .note.default h2,.post-body .note.default h3,.post-body .note.default h4,.post-body .note.default h5,.post-body .note.default h6{color:#777}.post-body .note.primary{border-left-color:#6f42c1}.post-body .note.primary h2,.post-body .note.primary h3,.post-body .note.primary h4,.post-body .note.primary h5,.post-body .note.primary h6{color:#6f42c1}.post-body .note.info{border-left-color:#428bca}.post-body .note.info h2,.post-body .note.info h3,.post-body .note.info h4,.post-body .note.info h5,.post-body .note.info h6{color:#428bca}.post-body .note.success{border-left-color:#5cb85c}.post-body .note.success h2,.post-body .note.success h3,.post-body .note.success h4,.post-body .note.success h5,.post-body .note.success h6{color:#5cb85c}.post-body .note.warning{border-left-color:#f0ad4e}.post-body .note.warning h2,.post-body .note.warning h3,.post-body .note.warning h4,.post-body .note.warning h5,.post-body .note.warning h6{color:#f0ad4e}.post-body .note.danger{border-left-color:#d9534f}.post-body .note.danger h2,.post-body .note.danger h3,.post-body .note.danger h4,.post-body .note.danger h5,.post-body .note.danger h6{color:#d9534f}.pdfobject-container embed,.pdfobject-container iframe{height:800px;width:100%}.post-body .tabs{margin-bottom:20px}.post-body .tabs,.tabs-comment{padding-top:10px}.post-body .tabs ul.nav-tabs,.tabs-comment ul.nav-tabs{background:var(--content-bg-color);display:flex;display:flex;flex-wrap:wrap;justify-content:center;margin:0;padding:0;position:sticky;top:0;z-index:5}.post-body .tabs ul.nav-tabs li.tab,.tabs-comment ul.nav-tabs li.tab{border-bottom:1px solid #ddd;border-left:1px solid transparent;border-right:1px solid transparent;border-radius:0;border-top:3px solid transparent;flex-grow:1;list-style-type:none}@media (max-width:413px){.post-body .tabs ul.nav-tabs,.tabs-comment ul.nav-tabs{display:block;margin-bottom:5px}.post-body .tabs ul.nav-tabs li.tab,.tabs-comment ul.nav-tabs li.tab{border-bottom:1px solid transparent;border-left:3px solid transparent;border-right:1px solid transparent;border-top:1px solid transparent;border-radius:0}}.post-body .tabs ul.nav-tabs li.tab a,.tabs-comment ul.nav-tabs li.tab a{border-bottom:initial;display:block;line-height:1.8;padding:.25em .75em;text-align:center;transition:.2s ease-out}.post-body .tabs ul.nav-tabs li.tab a i[class^=fa],.tabs-comment ul.nav-tabs li.tab a i[class^=fa]{width:1.285714285714286em}.post-body .tabs ul.nav-tabs li.tab.active,.tabs-comment ul.nav-tabs li.tab.active{border-color:#fc6423 #ddd transparent}@media (max-width:413px){.post-body .tabs ul.nav-tabs li.tab.active,.tabs-comment ul.nav-tabs li.tab.active{border-color:#ddd #ddd #ddd #fc6423}}.post-body .tabs ul.nav-tabs li.tab.active a,.tabs-comment ul.nav-tabs li.tab.active a{cursor:default}.post-body .tabs .tab-content,.tabs-comment .tab-content{border:1px solid #ddd;border-radius:0;border-top-color:transparent}@media (max-width:413px){.post-body .tabs .tab-content,.tabs-comment .tab-content{border-radius:0;border-top-color:#ddd}}.post-body .tabs .tab-content .tab-pane,.tabs-comment .tab-content .tab-pane{padding:20px 20px 0}.post-body .tabs .tab-content .tab-pane:not(.active),.tabs-comment .tab-content .tab-pane:not(.active){display:none}.pagination .next,.pagination .page-number,.pagination .prev,.pagination .space{display:inline-block;margin:-1px 10px 0;padding:0 10px}.pagination .page-number.current{background:#ccc;border-color:#ccc;color:var(--content-bg-color)}.pagination{border-top:1px solid #eee;margin:120px 0 0;text-align:center}.pagination .next,.pagination .page-number,.pagination .prev{border-bottom:0;border-top:1px solid #eee;transition:border-color .2s ease-in-out}.pagination .next:hover,.pagination .page-number:hover,.pagination .prev:hover{border-top-color:var(--link-hover-color)}@media (max-width:767px){.pagination .next,.pagination .page-number,.pagination .prev,.pagination .space{margin:0 5px}.pagination{border-top:0}.pagination .next,.pagination .page-number,.pagination .prev{border-bottom:1px solid #eee;border-top:0}.pagination .next:hover,.pagination .page-number:hover,.pagination .prev:hover{border-bottom-color:var(--link-hover-color)}.site-meta{text-align:center}}.pagination .space{margin:0;padding:0}.comments{margin-top:60px;overflow:hidden}.comment-button-group{display:flex;display:flex;flex-wrap:wrap;justify-content:center;justify-content:center;margin:1em 0}.comment-button-group .comment-button{margin:.1em .2em}.comment-button-group .comment-button.active{background:var(--btn-default-hover-bg);border-color:var(--btn-default-hover-border-color);color:var(--btn-default-hover-color)}.comment-position{display:none}.comment-position.active{display:block}.tabs-comment{margin-top:4em;padding-top:0}.tabs-comment .comments{margin-top:0;padding-top:0}.headband{background:var(--theme-color);height:3px}@media (max-width:991px){.headband{display:none}}.site-brand-container{display:flex;flex-shrink:0;padding:0 10px}.use-motion .column,.use-motion .site-brand-container .toggle{opacity:0}.site-meta{flex-grow:1;text-align:center}.custom-logo-image{margin-top:20px}@media (max-width:991px){.custom-logo-image{display:none}}.brand{border-bottom:0;color:var(--brand-color);display:inline-block;padding:0}.brand:hover{color:var(--brand-hover-color)}.site-title{font-family:Noto Serif SC,PingFang SC,Microsoft YaHei,sans-serif,Lato,'PingFang SC','Microsoft YaHei',sans-serif;font-size:1.375em;font-weight:400;line-height:1.5;margin:0}.site-subtitle{color:#ddd;margin:10px 10px 0}.use-motion .custom-logo-image,.use-motion .site-subtitle,.use-motion .site-title{opacity:0;position:relative;top:-10px}.site-nav-right,.site-nav-toggle{display:none}.site-nav-right .toggle,.site-nav-toggle .toggle{color:var(--text-color);padding:10px;width:22px}.site-nav-right .toggle .toggle-line,.site-nav-toggle .toggle .toggle-line{background:var(--text-color);border-radius:1px}@media (max-width:767px){.site-nav-right,.site-nav-toggle{display:flex;flex-direction:column;justify-content:center}.site-nav{--scroll-height:0;height:0;overflow:hidden;transition:height .2s ease-in-out,visibility .2s ease-in-out;visibility:hidden}body:not(.site-nav-on) .site-nav .animated{animation:none}body.site-nav-on .site-nav{height:var(--scroll-height);visibility:unset}}.menu{margin:0;padding:1em 0;text-align:center}.menu-item{display:inline-block;list-style:none;margin:0 10px}@media (max-width:767px){.menu-item{display:block;margin-top:10px}.menu-item.menu-item-search{display:none}}.menu-item a{border-bottom:0;display:block;font-size:.8125em;transition:border-color .2s ease-in-out}.menu-item a.menu-item-active,.menu-item a:hover{background:var(--menu-item-bg-color)}.menu-item i[class^=fa]{margin-right:8px}.menu-item .badge{background:#ccc;border-radius:10px;color:var(--content-bg-color);font-weight:700;line-height:1;margin-left:.35em;padding:2px 5px;text-shadow:1px 1px 0 rgba(0,0,0,.1)}.use-motion .menu-item{visibility:hidden}.book-mark-link{border-bottom:0;position:fixed;top:-10px;transition:top .3s;right:30px}.book-mark-link::before{color:#222;font-size:32px;line-height:1;content:'\f02e';font-family:'Font Awesome 6 Free';font-weight:900}.book-mark-link-fixed,.book-mark-link:hover{top:-2px}@media (max-width:991px){.book-mark-link{right:20px;display:none}.sidebar{left:-320px;background:#222;bottom:0;max-height:100vh;overflow-y:auto;position:fixed;top:0;transition:left .2s ease-out,right .2s ease-out;width:320px;z-index:20}.sidebar-active .sidebar{left:0}.sidebar a{border-bottom-color:#555;color:#999}.sidebar a:hover{border-bottom-color:#eee;color:#eee}.links-of-author:not(:first-child){margin-top:15px}.links-of-author a{border-bottom-color:#555;display:inline-block;margin-bottom:10px;margin-right:10px;vertical-align:middle}.links-of-author a::before{background:#fff5f4;display:inline-block;margin-right:3px;transform:translateY(-2px);border-radius:50%;content:' ';height:4px;width:4px}.links-of-blogroll-item{padding:0 5px}.popular-posts .popular-posts-item .popular-posts-link:hover{background:0 0}.sidebar-dimmer{background:#000;height:100%;left:0;opacity:0;position:fixed;top:0;transition:visibility .4s,opacity .4s;visibility:hidden;width:100%;z-index:10}.sidebar-active .sidebar-dimmer{opacity:.7;visibility:visible}}.sidebar-inner{color:#999;padding:18px 10px;text-align:center;display:flex;flex-direction:column;justify-content:center}.sidebar-toggle{bottom:61px;height:16px;padding:5px;width:16px;background:#222;cursor:pointer;opacity:.6;position:fixed;z-index:30;left:30px}.sidebar-toggle:hover{opacity:.8}@media (max-width:991px){.sidebar-toggle{left:20px;opacity:.8}}.sidebar-toggle:hover .toggle-line{background:#fc6423}@media (any-hover:hover){body:not(.sidebar-active) .sidebar-toggle:hover :first-child{left:50%;top:2px;transform:rotate(45deg);width:50%}body:not(.sidebar-active) .sidebar-toggle:hover :last-child{left:50%;top:-2px;transform:rotate(-45deg);width:50%}}.sidebar-active .sidebar-toggle :nth-child(2){opacity:0}.sidebar-active .sidebar-toggle :first-child{top:6px;transform:rotate(45deg)}.sidebar-active .sidebar-toggle :last-child{top:-6px;transform:rotate(-45deg)}.sidebar-nav{font-size:.875em;height:0;margin:0;overflow:hidden;padding-left:0;pointer-events:none;transition:height .2s ease-in-out,visibility .2s ease-in-out;visibility:hidden}.sidebar-nav-active .sidebar-nav{height:calc(2em + 1px);pointer-events:unset;visibility:unset}.sidebar-nav li{border-bottom:1px solid transparent;color:var(--text-color);cursor:pointer;display:inline-block;transition:border-bottom-color .2s ease-in-out,color .2s ease-in-out}.sidebar-nav li.sidebar-nav-overview{margin-left:10px}.sidebar-nav li:hover{color:#fc6423}.sidebar-overview-active .sidebar-nav-overview,.sidebar-toc-active .sidebar-nav-toc{border-bottom-color:#fc6423;color:#fc6423;transition-delay:0.2s}.sidebar-overview-active .sidebar-nav-overview:hover,.sidebar-toc-active .sidebar-nav-toc:hover{color:#fc6423}.sidebar-panel-container{align-items:start;display:grid;flex:1;overflow-x:hidden;overflow-y:auto;padding-top:0;transition:padding-top .2s ease-in-out}.sidebar-nav-active .sidebar-panel-container{padding-top:20px}.sidebar-panel{animation:.2s ease-in-out deactivate-sidebar-panel;grid-area:1/1;height:0;opacity:0;overflow:hidden;pointer-events:none;transform:translateY(0);transition:.2s ease-in-out;transition-property:opacity,transform,visibility;visibility:hidden}.sidebar-nav-active .sidebar-panel,.sidebar-overview-active .sidebar-panel.post-toc-wrap{transform:translateY(-20px)}.sidebar-overview-active:not(.sidebar-nav-active) .sidebar-panel.post-toc-wrap{transition-delay:0s,0.2s,0s}.sidebar-overview-active .sidebar-panel.site-overview-wrap,.sidebar-toc-active .sidebar-panel.post-toc-wrap{animation-name:activate-sidebar-panel;height:auto;opacity:1;pointer-events:unset;transform:translateY(0);transition-delay:0.2s,0.2s,0s;visibility:unset}.sidebar-panel.site-overview-wrap{display:flex;flex-direction:column;justify-content:center;gap:10px;justify-content:flex-start}@keyframes deactivate-sidebar-panel{from{height:var(--inactive-panel-height,0)}to{height:var(--active-panel-height,0)}}@keyframes activate-sidebar-panel{from{height:var(--inactive-panel-height,auto)}to{height:var(--active-panel-height,auto)}}.post-toc{font-size:.875em}.post-toc ol{list-style:none;margin:0;padding:0 2px 0 10px;text-align:left}.post-toc ol>:last-child{margin-bottom:5px}.post-toc ol>ol{padding-left:0}.post-toc ol a{transition:.2s ease-in-out}.post-toc .nav-item{line-height:1.8;overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.post-toc .nav .nav-child{--height:0;height:0;opacity:0;overflow:hidden;transition:.2s ease-in-out;visibility:hidden}.post-toc .nav .active>.nav-child{height:var(--height,auto);opacity:1;visibility:unset}.post-toc .nav .active>a{border-bottom-color:#fc6423;color:#fc6423}.post-toc .nav .active-current>a,.post-toc .nav .active-current>a:hover{color:#fc6423}.site-author-image{border:1px solid #eee;max-width:120px;padding:2px;border-radius:50%}.site-author-name{color:var(--text-color);font-weight:600;margin:0}.site-description{color:#999;font-size:.8125em;margin-top:0}.site-state{display:flex;flex-wrap:wrap;justify-content:center;line-height:1.4}.site-state-item{padding:0 15px}.site-state-item a{border-bottom:0;display:block}.site-state-item-count{display:block;font-size:1em;font-weight:600}.site-state-item-name{color:#999;font-size:.8125em}.sidebar .sidebar-button:not(:first-child){margin-top:15px}.sidebar .sidebar-button button{background:0 0;color:#fc6423;cursor:pointer;line-height:2;padding:0 15px;border:1px solid #fc6423;border-radius:4px}.sidebar .sidebar-button button:hover{background:#fc6423;color:#fff}.sidebar .sidebar-button button i[class^=fa]{margin-right:5px}.links-of-author a{font-size:.8125em}.links-of-author i[class^=fa]{margin-right:2px}.cc-license .cc-opacity{border-bottom:0;opacity:.7}.cc-license .cc-opacity:hover{opacity:.9}.cc-license img{display:inline-block}.links-of-blogroll{font-size:.8125em}.links-of-blogroll-title{font-size:.875em;font-weight:600}.links-of-blogroll-list{list-style:none;gap:5px;margin:5px 0 0;padding:0;display:flex;flex-wrap:wrap;justify-content:center}.links-of-blogroll-item{max-width:calc(100% - 20px)}.links-of-blogroll-item a{box-sizing:border-box;display:inline-block;max-width:100%;overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.footer{color:#999;font-size:.875em;padding:20px 0;transition:left .2s ease-in-out,right .2s ease-in-out}.footer.footer-fixed{bottom:0;left:0;position:absolute;right:0}.footer-inner{box-sizing:border-box;text-align:center;display:flex;flex-direction:column;justify-content:center;margin:0 auto;width:calc(100% - 20px)}@media (max-width:767px){.footer-inner{width:auto}}@media (min-width:1200px){.footer-inner{width:1160px}}@media (min-width:1600px){.footer-inner{width:73%}}.use-motion .footer{opacity:0}.languages{display:inline-block;font-size:1.125em;position:relative}.languages .lang-select-label span{margin:0 .5em}.languages .lang-select{height:100%;left:0;opacity:0;position:absolute;top:0;width:100%}.with-love{color:red;display:inline-block;margin:0 5px}.busuanzi-count #busuanzi_container_site_pv,.busuanzi-count #busuanzi_container_site_uv{display:none}@keyframes icon-animate{0%,100%{transform:scale(1)}10%,30%{transform:scale(.9)}20%,40%,50%,60%,70%,80%{transform:scale(1.1)}}@media (max-width:567px){.main-inner{padding:initial!important}.posts-expand .post-header{margin-bottom:10px!important}.post-block{margin-top:initial!important;padding:8px 18px!important}.post-body h1,.post-body h2,.post-body h3,.post-body h4,.post-body h5,.post-body h6{margin:20px 0 8px}.post-body .note h1,.post-body .note h2,.post-body .note h3,.post-body .note h4,.post-body .note h5,.post-body .note h6,.post-body .tabs .tab-content .tab-pane h1,.post-body .tabs .tab-content .tab-pane h2,.post-body .tabs .tab-content .tab-pane h3,.post-body .tabs .tab-content .tab-pane h4,.post-body .tabs .tab-content .tab-pane h5,.post-body .tabs .tab-content .tab-pane h6{margin:0 5px}.post-body>p{margin:0 0 10px}.post-body .note>p,.post-body .tabs .tab-content .tab-pane>p{padding:0 5px}.post-body img,.post-body video{margin-bottom:10px!important}.post-body figure:not(.highlight) figcaption{margin:-5px auto 15px!important}.post-body .note{margin-bottom:10px!important;padding:10px!important}.post-body .tabs .tab-content .tab-pane{padding:10px 10px 0!important}.post-eof{margin:40px auto 20px!important}.pagination{margin-top:40px}}.back-to-top{font-size:12px;margin:8px -10px -20px;opacity:0;transition:opacity .2s ease-in-out}.back-to-top span{margin-right:8px}.back-to-top .fa{text-align:center;width:26px}.back-to-top.back-to-top-on{cursor:pointer;opacity:.6}.back-to-top.back-to-top-on:hover{opacity:.8}.rtl.post-body a,.rtl.post-body h1,.rtl.post-body h2,.rtl.post-body h3,.rtl.post-body h4,.rtl.post-body h5,.rtl.post-body h6,.rtl.post-body li,.rtl.post-body ol,.rtl.post-body p,.rtl.post-body ul{direction:rtl;font-family:UKIJ Ekran}.rtl.post-title{font-family:UKIJ Ekran}.post-button{margin-top:40px;text-align:center}.use-motion .collection-header,.use-motion .comments,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{visibility:hidden}.posts-collapse .post-content{margin-bottom:35px;margin-left:35px;position:relative}@media (max-width:767px){.posts-collapse .post-content{margin-left:0;margin-right:0}}.posts-collapse .post-content .collection-title{font-size:1.125em;position:relative}.posts-collapse .post-content .collection-title::before{background:#999;border:1px solid #fff;margin-left:-6px;margin-top:-4px;position:absolute;top:50%;border-radius:50%;content:' ';height:10px;width:10px}.posts-collapse .post-content .collection-year{font-size:1.5em;font-weight:700;margin:60px 0;position:relative}.posts-collapse .post-content .collection-year .collection-year-count{font-size:.75em;background:#ccc;border-radius:10px;color:var(--content-bg-color);font-weight:700;line-height:1;margin-left:.35em;padding:2px 5px;text-shadow:1px 1px 0 rgba(0,0,0,.1)}.posts-collapse .post-content .collection-year::before{background:#bbb;margin-left:-4px;margin-top:-4px;position:absolute;top:50%;border-radius:50%;content:' ';height:8px;width:8px}.posts-collapse .post-content .collection-header{display:block;margin-left:20px}.posts-collapse .post-content .collection-header small{color:#bbb;margin-left:5px}.posts-collapse .post-content .post-header{border-bottom:1px dashed #ccc;margin:30px 2px 0;padding-left:15px;position:relative;transition:border .2s ease-in-out}.posts-collapse .post-content .post-header::before{background:#bbb;border:1px solid #fff;left:-6px;position:absolute;top:.75em;transition:background .2s ease-in-out;border-radius:50%;content:' ';height:6px;width:6px}.posts-collapse .post-content .post-header:hover{border-bottom-color:#666}.posts-collapse .post-content .post-header:hover::before{background:#222}.posts-collapse .post-content .post-meta-container{display:inline;font-size:.75em;margin-right:10px}.posts-collapse .post-content .post-title{display:inline}.posts-collapse .post-content .post-title a{border-bottom:0;color:var(--link-color)}.posts-collapse .post-content .post-title .fa{font-size:.875em;margin-left:5px}.posts-collapse .post-content::before{background:#f5f5f5;content:' ';height:100%;margin-left:-2px;position:absolute;top:1.25em;width:4px}.post-body{overflow-wrap:break-word}@media (min-width:1200px){.post-body{font-size:1.125em}}@media (min-width:992px){.post-body{text-align:left}}.post-body h1 .header-anchor,.post-body h1 .headerlink,.post-body h2 .header-anchor,.post-body h2 .headerlink,.post-body h3 .header-anchor,.post-body h3 .headerlink,.post-body h4 .header-anchor,.post-body h4 .headerlink,.post-body h5 .header-anchor,.post-body h5 .headerlink,.post-body h6 .header-anchor,.post-body h6 .headerlink{border-bottom-style:none;color:inherit;float:right;font-size:.875em;margin-left:10px;opacity:0}.post-body h1 .header-anchor::before,.post-body h1 .headerlink::before,.post-body h2 .header-anchor::before,.post-body h2 .headerlink::before,.post-body h3 .header-anchor::before,.post-body h3 .headerlink::before,.post-body h4 .header-anchor::before,.post-body h4 .headerlink::before,.post-body h5 .header-anchor::before,.post-body h5 .headerlink::before,.post-body h6 .header-anchor::before,.post-body h6 .headerlink::before{content:'\f0c1';font-family:'Font Awesome 6 Free';font-weight:900}.post-body h1:hover .header-anchor,.post-body h1:hover .headerlink,.post-body h2:hover .header-anchor,.post-body h2:hover .headerlink,.post-body h3:hover .header-anchor,.post-body h3:hover .headerlink,.post-body h4:hover .header-anchor,.post-body h4:hover .headerlink,.post-body h5:hover .header-anchor,.post-body h5:hover .headerlink,.post-body h6:hover .header-anchor,.post-body h6:hover .headerlink{opacity:.5}.post-body h1:hover .header-anchor:hover,.post-body h1:hover .headerlink:hover,.post-body h2:hover .header-anchor:hover,.post-body h2:hover .headerlink:hover,.post-body h3:hover .header-anchor:hover,.post-body h3:hover .headerlink:hover,.post-body h4:hover .header-anchor:hover,.post-body h4:hover .headerlink:hover,.post-body h5:hover .header-anchor:hover,.post-body h5:hover .headerlink:hover,.post-body h6:hover .header-anchor:hover,.post-body h6:hover .headerlink:hover{opacity:1}.post-body .exturl .fa{font-size:.875em;margin-left:4px}.post-body figure:not(.highlight) figcaption{color:#999;font-size:.875em;font-weight:700;line-height:1;margin:-15px auto 15px;text-align:center}.post-body embed,.post-body iframe,.post-body img,.post-body video{margin-bottom:20px}.post-body .video-container{height:0;margin-bottom:20px;overflow:hidden;padding-top:75%;position:relative;width:100%}.post-body .video-container embed,.post-body .video-container iframe,.post-body .video-container object{height:100%;left:0;margin:0;position:absolute;top:0;width:100%}.post-gallery{display:flex;min-height:200px}.post-gallery .post-gallery-image{flex:1}.post-gallery .post-gallery-image:not(:first-child){clip-path:polygon(40px 0,100% 0,100% 100%,0 100%);margin-left:-20px}.post-gallery .post-gallery-image:not(:last-child){margin-right:-20px}.post-gallery .post-gallery-image img{height:100%;object-fit:cover;opacity:1;width:100%}.posts-expand .post-gallery{margin-bottom:60px}.posts-collapse .post-gallery{margin:15px 0}.posts-expand .post-header{font-size:1.125em;margin-bottom:60px;text-align:center}.posts-expand .post-title{font-size:1.5em;font-weight:400;margin:initial;overflow-wrap:break-word}.posts-expand .post-title-link{border-bottom:0;color:var(--link-color);display:inline-block;position:relative}.posts-expand .post-title-link::before{background:var(--link-color);bottom:0;content:'';height:2px;left:0;position:absolute;transform:scaleX(0);transition:transform .2s ease-in-out;width:100%}.posts-expand .post-title-link:hover::before{transform:scaleX(1)}.posts-expand .post-title-link .fa{font-size:.875em;margin-left:5px}.post-sticky-flag{display:inline-block;margin-right:8px;transform:rotate(30deg)}.posts-expand .post-meta-container{color:#999;font-family:Noto Serif SC,PingFang SC,Microsoft YaHei,sans-serif,Lato,'PingFang SC','Microsoft YaHei',sans-serif;font-size:.75em;margin-top:3px}.posts-expand .post-meta-container .post-description{font-size:.875em;margin-top:2px}.posts-expand .post-meta-container time{border-bottom:1px dashed #999}.post-meta{display:flex;flex-wrap:wrap;justify-content:center}:not(.post-meta-break)+.post-meta-item::before{content:'|';margin:0 .5em}.post-meta-item-icon{margin-right:3px}@media (max-width:991px){.post-body{text-align:justify}.post-meta-item-text{display:none}}.post-meta-break{flex-basis:100%;height:0}#busuanzi_container_page_pv{display:none}.post-nav{border-top:1px solid #eee;display:flex;gap:30px;justify-content:space-between;margin-top:1em;padding:10px 5px 0}.post-nav-item{flex:1}.post-nav-item a{border-bottom:0;display:block;font-size:.875em;line-height:1.6}.post-nav-item a:active{top:2px}.post-nav-item .fa{font-size:.75em}.post-nav-item:first-child .fa{margin-right:5px}.post-nav-item:last-child{text-align:right}.post-nav-item:last-child .fa{margin-left:5px}.post-footer{display:flex;flex-direction:column;justify-content:center}.post-eof{background:#ccc;height:1px;margin:80px auto 60px;width:8%}.post-block:last-of-type .post-eof{display:none}.post-copyright ul{list-style:none;overflow:hidden;padding:.5em 1em;position:relative;background:var(--card-bg-color);border-left:3px solid #ff2a2a;margin:1em 0 0}.post-copyright ul::after{content:'\f25e';font-family:'Font Awesome 6 Brands';font-size:200px;opacity:.1;position:absolute;right:-50px;top:-150px}.post-tags{margin-top:40px;text-align:center}.post-tags a{display:inline-block;font-size:.8125em}.post-tags a:not(:last-child){margin-right:10px}.social-like{border-top:1px solid #eee;font-size:.875em;margin-top:1em;padding-top:1em;display:flex;flex-wrap:wrap;justify-content:center}.social-like a{border-bottom:none}.reward-container{margin:1em 0 0;padding:1em 0;text-align:center}.reward-container button{background:0 0;color:#fc6423;cursor:pointer;line-height:2;padding:0 15px;border:2px solid #fc6423;border-radius:2px;outline:0;vertical-align:text-top}.reward-container button:hover{background:#fc6423;color:#fff}.post-reward{display:none;padding-top:20px}.post-reward.active{display:block}.post-reward div{display:inline-block}.post-reward div span{display:block}.post-reward img{display:inline-block;margin:.8em 2em 0;max-width:100%;width:180px}@keyframes next-roll{from{transform:rotateZ(30deg)}to{transform:rotateZ(-30deg)}}.category-all-page .category-all-title{text-align:center}.category-all-page .category-all{margin-top:20px}.category-all-page .category-list{list-style:none;margin:0;padding:0}.category-all-page .category-list-item{margin:5px 10px}.category-all-page .category-list-count{font-size:.75em;background:#ccc;border-radius:10px;color:var(--content-bg-color);font-weight:700;line-height:1;margin-left:.35em;padding:2px 5px;text-shadow:1px 1px 0 rgba(0,0,0,.1)}.category-all-page .category-list-child{padding-left:10px}.event-list hr{background:#222;margin:20px 0 45px}.event-list hr::after{background:#222;color:#fff;content:'NOW';display:inline-block;font-weight:700;padding:0 5px}.event-list .event{--event-background:#222;--event-foreground:#bbb;--event-title:#fff;background:var(--event-background);padding:15px}.event-list .event .event-summary{border-bottom:0;color:var(--event-title);margin:0;padding:0 0 0 35px;position:relative}.event-list .event .event-summary::before{animation:1s ease-in-out infinite alternate dot-flash;background:var(--event-title);left:0;margin-top:-6px;position:absolute;top:50%;border-radius:50%;content:' ';height:12px;width:12px}.event-list .event:nth-of-type(odd) .event-summary::before{animation-delay:.5s}.event-list .event:not(:last-child){margin-bottom:20px}.event-list .event .event-relative-time{color:var(--event-foreground);display:inline-block;font-size:12px;font-weight:400;padding-left:12px}.event-list .event .event-details{color:var(--event-foreground);display:block;line-height:18px;padding:6px 0 6px 35px}.event-list .event .event-details::before{color:var(--event-foreground);display:inline-block;margin-right:9px;width:14px;font-family:'Font Awesome 6 Free';font-weight:900}.event-list .event .event-details.event-location::before{content:'\f041'}.event-list .event .event-details.event-duration::before{content:'\f017'}.event-list .event .event-details.event-description::before{content:'\f024'}.event-list .event-past{--event-background:#f5f5f5;--event-foreground:#999;--event-title:#222}@keyframes dot-flash{from{opacity:1;transform:scale(1)}to{opacity:0;transform:scale(.8)}}ul.breadcrumb{font-size:.75em;list-style:none;margin:1em 0;padding:0 2em;text-align:center}ul.breadcrumb li{display:inline}ul.breadcrumb li:not(:first-child)::before{content:'/\00a0';font-weight:400;padding:.5em}ul.breadcrumb li:last-child{font-weight:700}.tag-cloud{text-align:center}.tag-cloud a{display:inline-block;margin:10px}.tag-cloud-0{border-bottom-color:#aaa;color:#aaa}.tag-cloud-1{border-bottom-color:#9a9a9a;color:#9a9a9a}.tag-cloud-2{border-bottom-color:#8b8b8b;color:#8b8b8b}.tag-cloud-3{border-bottom-color:#7c7c7c;color:#7c7c7c}.tag-cloud-4{border-bottom-color:#6c6c6c;color:#6c6c6c}.tag-cloud-5{border-bottom-color:#5d5d5d;color:#5d5d5d}.tag-cloud-6{border-bottom-color:#4e4e4e;color:#4e4e4e}.tag-cloud-7{border-bottom-color:#3e3e3e;color:#3e3e3e}.tag-cloud-8{border-bottom-color:#2f2f2f;color:#2f2f2f}.tag-cloud-9{border-bottom-color:#202020;color:#202020}.tag-cloud-10{border-bottom-color:#111;color:#111}.search-active{overflow:hidden}.search-pop-overlay{background:rgba(0,0,0,0);display:flex;height:100%;left:0;position:fixed;top:0;transition:visibility .4s,background .4s;visibility:hidden;width:100%;z-index:40}.search-active .search-pop-overlay{background:rgba(0,0,0,.3);visibility:visible}.search-popup{background:var(--card-bg-color);border-radius:5px;height:80%;margin:auto;transform:scale(0);transition:transform .4s;width:700px}.search-active .search-popup{transform:scale(1)}@media (max-width:767px){.search-popup{border-radius:0;height:100%;width:100%}}.search-popup .popup-btn-close,.search-popup .search-icon{color:#999;font-size:18px;padding:0 10px}.search-popup .popup-btn-close{cursor:pointer}.search-popup .popup-btn-close:hover .fa{color:#222}.search-popup .search-header{background:#eee;border-top-left-radius:5px;border-top-right-radius:5px;display:flex;padding:5px}.search-popup input.search-input{background:0 0;border:0;outline:0;width:100%}.search-popup input.search-input::-webkit-search-cancel-button{display:none}.search-popup .search-result-container{display:flex;flex-direction:column;height:calc(100% - 55px);overflow:auto;padding:5px 25px}.search-popup .search-result-container hr{flex-shrink:0;margin:5px 0 10px}.search-popup .search-result-container hr:first-child{display:none}.search-popup .search-result-list{margin:0 5px;padding:0}.search-popup a.search-result-title{font-weight:700}.search-popup p.search-result{border-bottom:1px dashed #ccc;padding:5px 0}.search-popup .search-input-container{flex-grow:1;padding:2px}.search-popup .search-result-icon{color:#ccc;margin:auto}mark.search-keyword{background:0 0;border-bottom:1px dashed #ff2a2a;color:#ff2a2a;font-weight:700}.use-motion .animated{animation-fill-mode:none;visibility:inherit}.use-motion .sidebar .animated{animation-fill-mode:both}header.header{background:var(--content-bg-color);border-radius:20px;box-shadow:0 2px 2px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.06),0 1px 5px 0 rgba(0,0,0,.12)}.main{align-items:stretch;display:flex;justify-content:space-between;margin:0 auto;width:calc(100% - 20px)}@media (max-width:767px){.main{width:auto}}@media (min-width:1200px){.main{width:1160px}}@media (min-width:1600px){.main{width:73%}}@media (max-width:991px){header.header{border-radius:initial}.main{display:block;width:auto}}.main-inner{border-radius:20px;box-sizing:border-box;width:calc(100% - 252px)}.footer-inner{padding-left:252px}@media (max-width:991px){.main-inner{border-radius:initial;width:100%}.footer-inner{padding-left:0;padding-right:0;width:auto}}.column{width:240px}.site-brand-container{background:var(--theme-color)}.site-meta{padding:20px 0}.site-nav-right .toggle,.site-nav-toggle .toggle{color:#fff}.site-nav-right .toggle .toggle-line,.site-nav-toggle .toggle .toggle-line{background:#fff}@media (min-width:768px) and (max-width:991px){.site-nav-right,.site-nav-toggle{display:flex;flex-direction:column;justify-content:center}.site-nav{--scroll-height:0;height:0;overflow:hidden;transition:height .2s ease-in-out,visibility .2s ease-in-out;visibility:hidden}body:not(.site-nav-on) .site-nav .animated{animation:none}body.site-nav-on .site-nav{height:var(--scroll-height);visibility:unset}}.menu .menu-item{display:block;margin:0}.menu .menu-item a{padding:5px 20px;position:relative;transition-property:background-color;display:flex;align-items:center}.menu .menu-item a .badge{margin-left:auto}@media (max-width:991px){.column{width:auto}.site-nav-on .site-brand-container{box-shadow:0 0 16px rgba(0,0,0,.5)}.menu .menu-item.menu-item-search{display:none}}.main-menu .menu-item-active::after{background:#bbb;border-radius:50%;content:' ';height:6px;margin-top:-3px;position:absolute;right:15px;top:50%;width:6px}.sub-menu{margin:0;padding:6px 0}.sub-menu .menu-item{display:inline-block}.sub-menu .menu-item a{background:0 0;margin:5px 10px;padding:initial}.sub-menu .menu-item a:hover{background:0 0;color:#fc6423}.sub-menu .menu-item-active{border-bottom-color:#fc6423;color:#fc6423}.sub-menu .menu-item-active:hover{border-bottom-color:#fc6423}@media (min-width:992px){.sidebar{position:sticky;top:12px}.sidebar-toggle{display:none}.sidebar-inner{background:var(--content-bg-color);border-radius:20px;box-shadow:0 2px 2px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.06),0 1px 5px 0 rgba(0,0,0,.12),0 -1px .5px 0 rgba(0,0,0,.09);box-sizing:border-box;color:var(--text-color);margin-top:12px;max-height:calc(100vh - 24px)}.site-state-item{padding:0 10px}.sidebar .sidebar-button{border-bottom:1px dotted #ccc;border-top:1px dotted #ccc}.sidebar .sidebar-button button{border:0;color:#fc6423;display:block;width:100%}.sidebar .sidebar-button button:hover{background:0 0;border:0;color:#e34603}.links-of-author{display:flex;flex-wrap:wrap;justify-content:center}.links-of-author-item{margin:5px 0 0;width:50%}.links-of-author-item a{border-bottom:0;border-radius:4px;padding:0 5px;box-sizing:border-box;display:inline-block;max-width:100%;overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.links-of-author-item a:hover{background:var(--body-bg-color)}.links-of-blogroll-item a{padding:0 5px}.back-to-top{background:var(--body-bg-color);margin:-4px -10px -18px;transition-property:bottom,margin-top}.back-to-top.back-to-top-on{margin-top:16px}}.main-inner .comment-position .comments,.main-inner .pagination,.main-inner .post-block,.main-inner .sub-menu,.main-inner .tabs-comment,.main-inner>.comments{background:var(--content-bg-color);border-radius:20px;box-shadow:0 2px 2px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.06),0 1px 5px 0 rgba(0,0,0,.12)}.main-inner .post-block:not(:first-child):not(:first-child){border-radius:20px;box-shadow:0 2px 2px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.06),0 1px 5px 0 rgba(0,0,0,.12),0 -1px .5px 0 rgba(0,0,0,.09);margin-top:12px}@media (min-width:768px) and (max-width:991px){.main-inner .post-block:not(:first-child):not(:first-child){margin-top:10px}}@media (max-width:767px){.main-inner .post-block:not(:first-child):not(:first-child){margin-top:8px}}.main-inner .comment-position .comments,.main-inner .pagination,.main-inner .tabs-comment,.main-inner>.comments{border-radius:20px;box-shadow:0 2px 2px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.06),0 1px 5px 0 rgba(0,0,0,.12),0 -1px .5px 0 rgba(0,0,0,.09);margin-top:12px}@media (min-width:768px) and (max-width:991px){.main-inner .comment-position .comments,.main-inner .pagination,.main-inner .tabs-comment,.main-inner>.comments{margin-top:10px}}@media (max-width:767px){.main-inner .comment-position .comments,.main-inner .pagination,.main-inner .tabs-comment,.main-inner>.comments{margin-top:8px}}.comments,.post-block{padding:40px}.post-eof{display:none}.pagination{border-top:initial;padding:10px 0}.post-body h1,.post-body h2{border-bottom:1px solid #eee}.post-body h3{border-bottom:1px dotted #eee}@media (min-width:768px) and (max-width:991px){.main-inner{padding:10px}.posts-expand .post-button{margin-top:20px}.post-block{padding:20px}.comments{padding:10px 20px}}.post-block{margin-top:12px!important}.main-inner .post-block:not(:first-child):not(:first-child)+mobile (),.post-block+mobile (){border-radius:0!important}ul.sub-menu{margin-top:12px}.site-title{opacity:0}.site-subtitle{font-family:"华文行楷";font-size:1em}.back-to-top{border-radius:0 0 100px}@media (max-width:767px){.main-inner{padding:8px}.posts-expand .post-button{margin:12px 0}.post-block{padding:12px}.comments{padding:10px 12px}ul.sub-menu{margin-top:8px;border-radius:0!important}.back-to-top,.back-to-top-on{bottom:61px;position:absolute;right:20px;color:#fff;font-size:1em;height:26px;line-height:26px;margin:0}.followme{display:none}}.btn{background:rgba(255,255,255,.5)}.site-brand-container{background:url("/images/avatar.jpg") 50% 50%/cover}.post-body{font-family:'Noto Serif SC','Microsoft YaHei';text-align:start;color:#000}.menu .menu-item a,.posts-expand .post-title,.posts-expand .post-title-link{font-family:'Noto Serif SC','Microsoft YaHei';color:#000}.site-description{font-family:'Noto Serif SC','Microsoft YaHei'}.post-copyright ul{font-family:'Noto Serif SC','Microsoft YaHei';color:#000}.sidebar-inner{font-family:'Noto Serif SC','Microsoft YaHei'}@media (max-width:767px){.sidebar-blogroll{display:none;bottom:90px;position:absolute}}.search-popup .search-input-container,.search-popup .search-result-container{font-family:'Noto Serif SC','Microsoft YaHei'}h1{font-size:1.5em}h2{font-size:1.375em}h3{font-size:1.25em}h4{font-size:1.125em}h5{font-size:1em}h6{font-size:.875em}</style><link href=https://use.sevencdn.com/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=PingFang+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Microsoft+YaHei:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=sans-serif:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Lato:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Roboto+Mono:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext rel=stylesheet><link crossorigin href=https://use.sevencdn.com/ajax/libs/font-awesome/6.6.0/css/all.min.css integrity=sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E= rel=stylesheet><link crossorigin href=https://use.sevencdn.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css integrity=sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w= rel=stylesheet><script class=next-config data-name=main type=application/json>{"hostname":"xiaojianzheng.cn","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":true,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.json","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":true,"trigger":"auto"}}</script><script src=https://use.sevencdn.com/ajax/libs/hexo-theme-next/8.21.1/config.min.js></script><meta content="转载：https://www.cnblogs.com/kevingrace/p/5575666.html  一、Kubernetes 介绍Kubernetes是一个全新的基于容器技术的分布式架构领先方案, 它是Google在2014年6月开源的一个容器集群管理系统，使用Go语言开发，Kubernetes也叫K8S。K8S是Google内部一个叫Borg的容器集群管理系统衍生出来的，Borg已经在" name=description><meta content=article property=og:type><meta content="Kubernetes 运维学习笔记" property=og:title><meta content=http://xiaojianzheng.cn/archives/cf9b7124.html property=og:url><meta content=JiaHe property=og:site_name><meta content="转载：https://www.cnblogs.com/kevingrace/p/5575666.html  一、Kubernetes 介绍Kubernetes是一个全新的基于容器技术的分布式架构领先方案, 它是Google在2014年6月开源的一个容器集群管理系统，使用Go语言开发，Kubernetes也叫K8S。K8S是Google内部一个叫Borg的容器集群管理系统衍生出来的，Borg已经在" property=og:description><meta content=zh_CN property=og:locale><meta content=https://cdn.jsdelivr.net/gh/xiao1272209235/hexo-image@main/16500693799211650069379860.png property=og:image><meta content=https://cdn.jsdelivr.net/gh/xiao1272209235/hexo-image@main/16500714498471650071449718.png property=og:image><meta content=https://cdn.jsdelivr.net/gh/xiao1272209235/hexo-image@main/16500714258481650071425292.png property=og:image><meta content=https://cdn.jsdelivr.net/gh/xiao1272209235/hexo-image@main/16500714018481650071401660.png property=og:image><meta content=https://img2018.cnblogs.com/blog/907596/201908/907596-20190806151141561-1435899737.png property=og:image><meta content=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313232215198-1967204451.png property=og:image><meta content=https://img2018.cnblogs.com/blog/907596/201905/907596-20190531165628671-14705824.png property=og:image><meta content=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313222141535-880155421.png property=og:image><meta content=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313222650824-93538855.png property=og:image><meta content=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313223036439-466023285.png property=og:image><meta content=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313223346653-822874121.png property=og:image><meta content=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313223917071-1603229987.png property=og:image><meta content=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313224329386-903560638.png property=og:image><meta content=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313224553325-108461685.png property=og:image><meta content=2022-04-15T16:00:00.000Z property=article:published_time><meta content=2022-04-15T16:00:00.000Z property=article:modified_time><meta content=JiaHe property=article:author><meta content=运维 property=article:tag><meta content=Kubernetes property=article:tag><meta content=summary name=twitter:card><meta content=https://cdn.jsdelivr.net/gh/xiao1272209235/hexo-image@main/16500693799211650069379860.png name=twitter:image><link href=http://xiaojianzheng.cn/archives/cf9b7124.html rel=canonical><script class=next-config data-name=page type=application/json>{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://xiaojianzheng.cn/archives/cf9b7124.html","path":"archives/cf9b7124.html","title":"Kubernetes 运维学习笔记"}</script><script class=next-config data-name=calendar type=application/json>""</script><title>Kubernetes 运维学习笔记 | JiaHe</title><noscript><link href=/css/noscript.css rel=stylesheet></noscript><link href=/atom.xml rel=alternate title=JiaHe type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><div class=column><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle role=button><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <i class=logo-line></i> <p class=site-title>JiaHe</p> <i class=logo-line></i> </a><p class=site-subtitle itemprop=description>相遇即是缘</div><div class=site-nav-right><div class="toggle popup-trigger" aria-label=搜索 role=button><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-归档"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-导航"><a href=/navigation/ rel=section><i class="fa fa-location-arrow fa-fw"></i>导航</a><li class="menu-item menu-item-文档"><a href=/docs/ rel=section><i class="fa fa-book fa-fw fa-fw"></i>文档</a><li class="menu-item menu-item-应用下载"><a href=/app-download/ rel=section><i class="fas fa-download fa-fw fa-fw"></i>应用下载</a><li class="menu-item menu-item-gpt提示词"><a href=/chatgpt-prompt/ rel=section><i class="fas fa-message fa-fw"></i>GPT提示词</a><li class="menu-item menu-item-小贴士"><a href=/cheat-sheet/ rel=section><i class="fas fa-pen fa-fw"></i>小贴士</a><li class="menu-item menu-item-在线工具"><a href=https://tool.xiaojianzheng.cn/ rel=section target=_blank><i class="fas fa-regular fa-screwdriver-wrench fa-fw"></i>在线工具</a><li class="menu-item menu-item-在线转换"><a href=/online-convert/ rel=section><i class="fas fa-arrow-right-arrow-left fa-fw"></i>在线转换</a><li class="menu-item menu-item-软件安装部署"><a href=/software-install-and-deploy/ rel=section><i class="fab fa-windows fa-fw"></i>软件安装部署</a><li class="menu-item menu-item-linux命令"><a href=https://wangchujiang.com/linux-command/ rel=section target=_blank><i class="fab fa-linux fa-fw"></i>Linux命令</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container><input autocapitalize=off autocomplete=off class=search-input maxlength=80 placeholder=搜索... spellcheck=false type=search></div><span class=popup-btn-close role=button> <i class="fa fa-times-circle"></i> </span></div><div class=search-result-container><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><div class=sidebar-panel-container><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class=nav><li class="nav-item nav-level-1"><a class=nav-link href=#%E4%B8%80%E3%80%81Kubernetes-%E4%BB%8B%E7%BB%8D><span class=nav-number>1.</span> <span class=nav-text>一、Kubernetes 介绍</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#%E4%BA%8C%E3%80%81Kubernetes%E4%B8%BB%E8%A6%81%E5%8A%9F%E8%83%BD><span class=nav-number>2.</span> <span class=nav-text>二、Kubernetes主要功能</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#%E4%B8%89%E3%80%81Kubernetes%E6%9E%B6%E6%9E%84%E5%92%8C%E7%BB%84%E4%BB%B6><span class=nav-number>3.</span> <span class=nav-text>三、Kubernetes架构和组件</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Kubernetes-Master%E6%8E%A7%E5%88%B6%E7%BB%84%E4%BB%B6><span class=nav-number>3.1.</span> <span class=nav-text>Kubernetes Master控制组件</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Kubernetes%E7%9A%84%E5%88%86%E5%B1%82%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5><span class=nav-number>3.2.</span> <span class=nav-text>Kubernetes的分层设计理念</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link href=#%E5%9B%9B%E3%80%81Kubernetes%E5%9F%BA%E6%9C%AC%E5%AF%B9%E8%B1%A1%E6%A6%82%E5%BF%B5><span class=nav-number>4.</span> <span class=nav-text>四、Kubernetes基本对象概念</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#%E4%BA%94%E3%80%81Kubernetes%E9%9B%86%E7%BE%A4%E9%87%8C%E5%AE%B9%E5%99%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E9%80%9A%E8%AE%AF%E6%96%B9%E5%BC%8F><span class=nav-number>5.</span> <span class=nav-text>五、Kubernetes集群里容器之间的通讯方式</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#pod%E5%86%85%E9%83%A8%E5%AE%B9%E5%99%A8%E4%B9%8B%E9%97%B4><span class=nav-number>5.1.</span> <span class=nav-text>pod内部容器之间</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#pod%E4%B8%8Epod%E5%AE%B9%E5%99%A8%E4%B9%8B%E9%97%B4><span class=nav-number>5.2.</span> <span class=nav-text>pod与pod容器之间</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#pod-%E8%AE%BF%E9%97%AEservice%E6%9C%8D%E5%8A%A1><span class=nav-number>5.3.</span> <span class=nav-text>pod 访问service服务</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link href=#%E5%85%AD%E3%80%81Kubernetes%E6%97%A5%E5%B8%B8%E7%BB%B4%E6%8A%A4%E5%91%BD%E4%BB%A4><span class=nav-number>6.</span> <span class=nav-text>六、Kubernetes日常维护命令</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#%E4%B8%83%E3%80%81Kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E5%A4%B1%E8%B4%A5%E7%9A%84%E4%B8%80%E8%88%AC%E5%8E%9F%E5%9B%A0><span class=nav-number>7.</span> <span class=nav-text>七、Kubernetes集群部署失败的一般原因</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#1-%E9%94%99%E8%AF%AF%E7%9A%84%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F-%E9%9D%9E%E6%B3%95%E7%9A%84%E4%BB%93%E5%BA%93%E6%9D%83%E9%99%90><span class=nav-number>7.1.</span> <span class=nav-text>1. 错误的容器镜像/非法的仓库权限</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#2-%E5%BA%94%E7%94%A8%E5%90%AF%E5%8A%A8%E4%B9%8B%E5%90%8E%E5%8F%88%E6%8C%82%E6%8E%89><span class=nav-number>7.2.</span> <span class=nav-text>2. 应用启动之后又挂掉</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#3-%E7%BC%BA%E5%A4%B1-ConfigMap-%E6%88%96%E8%80%85-Secret><span class=nav-number>7.3.</span> <span class=nav-text>3. 缺失 ConfigMap 或者 Secret</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%BC%BA%E5%A4%B1-ConfigMap><span class=nav-number>7.3.1.</span> <span class=nav-text>缺失 ConfigMap</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%BC%BA%E5%A4%B1-Secrets><span class=nav-number>7.3.2.</span> <span class=nav-text>缺失 Secrets</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#4-%E6%B4%BB%E8%B7%83%E5%BA%A6-%E5%B0%B1%E7%BB%AA%E7%8A%B6%E6%80%81%E6%8E%A2%E6%B5%8B%E5%A4%B1%E8%B4%A5><span class=nav-number>7.4.</span> <span class=nav-text>4. 活跃度/就绪状态探测失败</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#5-%E8%B6%85%E5%87%BACPU-%E5%86%85%E5%AD%98%E7%9A%84%E9%99%90%E5%88%B6><span class=nav-number>7.5.</span> <span class=nav-text>5. 超出CPU/内存的限制</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#6-%E8%B5%84%E6%BA%90%E9%85%8D%E9%A2%9D><span class=nav-number>7.6.</span> <span class=nav-text>6. 资源配额</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#7-%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E4%B8%8D%E8%B6%B3><span class=nav-number>7.7.</span> <span class=nav-text>7. 集群资源不足</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#8-%E6%8C%81%E4%B9%85%E5%8C%96%E5%8D%B7%E6%8C%82%E8%BD%BD%E5%A4%B1%E8%B4%A5><span class=nav-number>7.8.</span> <span class=nav-text>8. 持久化卷挂载失败</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#9-%E6%A0%A1%E9%AA%8C%E9%94%99%E8%AF%AF><span class=nav-number>7.9.</span> <span class=nav-text>9. 校验错误</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#10-%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%E6%B2%A1%E6%9C%89%E6%9B%B4%E6%96%B0><span class=nav-number>7.10.</span> <span class=nav-text>10. 容器镜像没有更新</span></a></ol></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=JiaHe class=site-author-image itemprop=image src=/images/avatar.jpg><p class=site-author-name itemprop=name>JiaHe<div class=site-description itemprop=description>无缘等有缘</div></div><div class="site-state-wrap animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>160</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>24</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>121</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author animated"><span class=links-of-author-item> <a rel="noopener me" title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span></div><div style="margin-top: 10px" class=digit-canvas><canvas id=canvas style=width:58%;></canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

//定义一个函数判断是手机端还是pc端
function isMobile(){
    if(window.navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i)) {
      document.querySelector('.digit-canvas').style.display = 'none'
      return true; // 移动端
    }else{
      return false; // PC端
    }
}

if(!isMobile() && canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><div class="site-overview-wrap sidebar-panel sidebar-panel-active"><div class=site-overview></div></div></div></div><div class="back-to-top animated" aria-label=返回顶部 role=button><i class="fa fa-arrow-up"></i><span>0%</span></div></div><div class="sidebar-inner sidebar-blogroll"><div class="links-of-blogroll animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i> 链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://cn.bing.com/ rel=noopener target=_blank title=https://cn.bing.com/>Bing</a><li class=links-of-blogroll-item><a href=https://www.google.com/ rel=noopener target=_blank title=https://www.google.com/>Google</a><li class=links-of-blogroll-item><a href=https://zh.wikipedia.org/wiki/Wikipedia:%E9%A6%96%E9%A1%B5 rel=noopener target=_blank title=https://zh.wikipedia.org/wiki/Wikipedia:%E9%A6%96%E9%A1%B5>Wiki</a><li class=links-of-blogroll-item><a href=https://projectlombok.org/ rel=noopener target=_blank title=https://projectlombok.org/>lombok</a><li class=links-of-blogroll-item><a href=https://logging.apache.org/log4j/2.x/ rel=noopener target=_blank title=https://logging.apache.org/log4j/2.x/>log4j</a><li class=links-of-blogroll-item><a href=http://logback.qos.ch/ rel=noopener target=_blank title=http://logback.qos.ch/>logback</a><li class=links-of-blogroll-item><a href=https://junit.org/junit5/ rel=noopener target=_blank title=https://junit.org/junit5/>junit</a><li class=links-of-blogroll-item><a href=https://element.eleme.cn/#/zh-CN rel=noopener target=_blank title=https://element.eleme.cn/#/zh-CN>ElementUI</a><li class=links-of-blogroll-item><a href=https://zipkin.io/ rel=noopener target=_blank title=https://zipkin.io/>Zipkin</a><li class=links-of-blogroll-item><a href=https://www.selenium.dev/zh-cn/ rel=noopener target=_blank title=https://www.selenium.dev/zh-cn/>Selenium</a><li class=links-of-blogroll-item><a href=https://nacos.io/zh-cn/docs/what-is-nacos.html rel=noopener target=_blank title=https://nacos.io/zh-cn/docs/what-is-nacos.html>Nacos</a><li class=links-of-blogroll-item><a href=https://redis.io/ rel=noopener target=_blank title=https://redis.io/>Redis</a><li class=links-of-blogroll-item><a href=https://www.jetbrains.com/help/idea/discover-intellij-idea.html rel=noopener target=_blank title=https://www.jetbrains.com/help/idea/discover-intellij-idea.html>IDEA-Document</a><li class=links-of-blogroll-item><a href=https://jdk.java.net/ rel=noopener target=_blank title=https://jdk.java.net/>JDK</a><li class=links-of-blogroll-item><a href=https://www.java.com/en/download/manual.jsp rel=noopener target=_blank title=https://www.java.com/en/download/manual.jsp>JDK8</a><li class=links-of-blogroll-item><a href=https://dev.java/ rel=noopener target=_blank title=https://dev.java/>JAVA</a><li class=links-of-blogroll-item><a href=https://poi.apache.org/components/index.html rel=noopener target=_blank title=https://poi.apache.org/components/index.html>POI</a><li class=links-of-blogroll-item><a href=https://assertj.github.io/doc/ rel=noopener target=_blank title=https://assertj.github.io/doc/>AssertJ</a><li class=links-of-blogroll-item><a href=https://docshome.gitbook.io/nginx-docs/ rel=noopener target=_blank title=https://docshome.gitbook.io/nginx-docs/>Nginx-CN</a><li class=links-of-blogroll-item><a href=https://github.com/cglib/cglib rel=noopener target=_blank title=https://github.com/cglib/cglib>cglib</a><li class=links-of-blogroll-item><a href=https://docs.gitlab.com/ee/api/api_resources.html rel=noopener target=_blank title=https://docs.gitlab.com/ee/api/api_resources.html>Gitlab Api</a><li class=links-of-blogroll-item><a href=http://mockjs.com/examples.html rel=noopener target=_blank title=http://mockjs.com/examples.html>Mockjs</a><li class=links-of-blogroll-item><a href=https://cook.aiurs.co/ rel=noopener target=_blank title=https://cook.aiurs.co/>程序员做饭指南</a><li class=links-of-blogroll-item><a href=https://wangdoc.com/ rel=noopener target=_blank title=https://wangdoc.com/>网道</a><li class=links-of-blogroll-item><a href=https://docs.k3s.io/zh/ rel=noopener target=_blank title=https://docs.k3s.io/zh/>K3S</a><li class=links-of-blogroll-item><a href=https://lazyvim-github-io.vercel.app/ rel=noopener target=_blank title=https://lazyvim-github-io.vercel.app/>LazyVim</a></ul></div></div></aside></div><div class="main-inner post posts-expand"><div class=post-block><article class=post-content itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=http://xiaojianzheng.cn/archives/cf9b7124.html itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=/images/avatar.jpg itemprop=image> <meta content=JiaHe itemprop=name> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=JiaHe itemprop=name> <meta content=无缘等有缘 itemprop=description> </span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork> <meta content="Kubernetes 运维学习笔记 | JiaHe" itemprop=name> <meta itemprop=description> </span><header class=post-header><h1 itemprop="name headline" class=post-title>Kubernetes 运维学习笔记</h1><div class=post-meta-container><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2022-04-16 00:00:00" datetime=2022-04-16T00:00:00+08:00>2022-04-16</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-folder"></i> </span> <span class=post-meta-item-text>分类于</span> <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/%E7%BC%96%E7%A8%8B%E6%8A%80%E6%9C%AF/ itemprop=url rel=index><span itemprop=name>编程技术</span></a> </span> </span><span class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="far fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><span class=post-meta-break></span><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>23k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>1:22</span> </span></div></div></header><div class=post-body itemprop=articleBody><blockquote><p>转载：<a href=https://www.cnblogs.com/kevingrace/p/5575666.html rel=noopener target=_blank>https://www.cnblogs.com/kevingrace/p/5575666.html</a></blockquote><h1 id=一、Kubernetes-介绍><a title="一、Kubernetes 介绍" class=headerlink href=#一、Kubernetes-介绍></a><strong>一、Kubernetes 介绍</strong></h1><p>Kubernetes是一个全新的基于容器技术的分布式架构领先方案, 它是Google在2014年6月开源的一个容器集群管理系统，使用Go语言开发，Kubernetes也叫K8S。K8S是Google内部一个叫Borg的容器集群管理系统衍生出来的，Borg已经在Google大规模生产运行十年之久。K8S主要用于自动化部署、扩展和管理容器应用，提供了资源调度、部署管理、服务发现、扩容缩容、监控等一整套功能。2015年7月，Kubernetes v1.0正式发布，截止到2017年9月29日最新稳定版本是v1.8。Kubernetes目标是让部署容器化应用简单高效。<p>Kubernetes最初源于谷歌内部的Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理/虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的workflows 和更高级的自动化任务。<p>Kubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。</p><span id=more></span><h1 id=二、Kubernetes主要功能><a class=headerlink href=#二、Kubernetes主要功能 title=二、Kubernetes主要功能></a><strong>二、Kubernetes主要功能</strong></h1><p>Kubernetes是docker容器用来编排和管理的工具，它是基于Docker构建一个容器的调度服务，提供资源调度、均衡容灾、服务注册、动态扩缩容等功能套件。Kubernetes提供应用部署、维护、 扩展机制等功能，利用Kubernetes能方便地管理跨机器运行容器化的应用，其主要功能如下：<ul><li><p><strong>数据卷:</strong> Pod中容器之间共享数据，可以使用数据卷。</p><li><p><strong>应用程序健康检查:</strong> 容器内服务可能进程堵塞无法处理请求，可以设置监控检查策略保证应用健壮性。</p><li><p><strong>复制应用程序实例:</strong> 控制器维护着Pod副本数量，保证一个Pod或一组同类的Pod数量始终可用。</p><li><p><strong>弹性伸缩:</strong> 根据设定的指标（CPU利用率）自动缩放Pod副本数。</p><li><p><strong>服务发现:</strong> 使用环境变量或DNS服务插件保证容器中程序发现Pod入口访问地址。</p><li><p><strong>负载均衡:</strong> 一组Pod副本分配一个私有的集群IP地址，负载均衡转发请求到后端容器。在集群内部其他Pod可通过这个ClusterIP访问应用。</p><li><p><strong>滚动更新:</strong> 更新服务不中断，一次更新一个Pod，而不是同时删除整个服务。</p><li><p><strong>服务编排:</strong> 通过文件描述部署服务，使得应用程序部署变得更高效。</p><li><p><strong>资源监控:</strong> Node节点组件集成cAdvisor资源收集工具，可通过Heapster汇总整个集群节点资源数据，然后存储到InfluxDB时序数据库，再由Grafana展示。</p><li><p><strong>提供认证和授权:</strong> 支持属性访问控制（ABAC）、角色访问控制（RBAC）认证授权策略。</p></ul><p>除此之外, Kubernetes主要功能还体现在:<ul><li>使用Docker对应用程序包装(package)、实例化(instantiate)、运行(run)。<li>将多台Docker主机抽象为一个资源，以集群的方式运行、管理跨机器的容器，包括任务调度、资源管理、弹性伸缩、滚动升级等功能。<li>使用编排系统（YAML File）快速构建容器集群，提供负载均衡，解决容器直接关联及通信问题<li>解决Docker跨机器容器之间的通讯问题。<li>自动管理和修复容器，简单说，比如创建一个集群，里面有十个容器，如果某个容器异常关闭，那么，会尝试重启或重新分配容器，始终保证会有十个容器在运行，反而杀死多余的。Kubernetes的自我修复机制使得容器集群总是运行在用户期望的状态. 当前Kubernetes支持GCE、vShpere、CoreOS、OpenShift。</ul><p>kubernetes的集群至少有两个主机组成：master + node ，即为master/node架构。master为集群的控制面板，master主机需要做冗余，一般建议为3台；而node主机不需要做冗余，因为node的主要作用是运行pod，贡献计算能力和存储能力，而pod控制器会自动管控pod资源，如果资源少，pod控制器会自动创建pod，即pod控制器会严格按照用户指定的副本来管理pod的数量。客户端的请求下发给master,即把创建和启动容器的请求发给master，master中的调度器分析各node现有的资源状态，把请求调用到对应的node启动容器。<p>可以理解为kubernetes把容器抽象为pod来管理1到多个彼此间有非常紧密联系的容器，但是LAMP的容器主机A,M,P只是有关联，不能说是非常紧密联系，因此A,M,P都要运行在三个不同的pod上。在kubernetes中，要运行几个pod，是需要定义一个配置文件，在这个配置文件里定义用哪个控制器启动和控制几个pod，在每个pod里要定义那几台容器，kubernetes通过这个配置文件，去创建一个控制器，由此控制器来管控这些pod,如果这些pod的某几个down掉后，控制器会通过健康监控功能，随时监控pod，发现pod异常后,根据定义的策略进行操作，即可以进行自愈。<p><strong>kubernetes内部需要5套证书，手动创建或者自动生成，分别为：</strong><ol><li>etcd内部通信需要一套ca和对应证书。<li>etcd与外部通信也要有一套ca和对应证书。<li>APIserver间通信需要一套证书。<li>apiserver与node间通信需要一套证书。<li>node和pod间通信需要一套ca证书。</ol><p>目前来说还不能实现把所有的业务都迁到kubernetes上，如存储，因为这个是有状态应用，出现错误排查很麻烦，所以目前kubernetes主要是运行无状态应用。<p>所以一般而言，负载均衡器运行在kubernetes之外，nginx或者tomcat这种无状态的应用运行于kubernetes集群内部，而数据库如mysql，zabbix，zoopkeeper等有状态的，一般运行于kubernetes外部，通过网络连接，实现kubernetes集群的pod调用这些外部的有状态应用。<h1 id=三、Kubernetes架构和组件><a class=headerlink href=#三、Kubernetes架构和组件 title=三、Kubernetes架构和组件></a><strong>三、Kubernetes架构和组件</strong></h1><p><img data-src=https://cdn.jsdelivr.net/gh/xiao1272209235/hexo-image@main/16500693799211650069379860.png><p><strong>kubernetes主要由以下几个核心组件组成：</strong><p><strong>etcd:</strong> 集群的主数据库，保存了整个集群的状态; etcd负责节点间的服务发现和配置共享。etcd分布式键值存储系统, 用于保持集群状态，比如Pod、Service等对象信息。<p><strong>kube-apiserver:</strong> 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制；这是kubernetes API，作为集群的统一入口，各组件协调者，以HTTPAPI提供接口服务，所有对象资源的增删改查和监听操作都交给APIServer处理后再提交给Etcd存储。<p><strong>kube-controller-manager:</strong> 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；它用来执行整个系统中的后台任务，包括节点状态状况、Pod个数、Pods和Service的关联等, 一个资源对应一个控制器，而ControllerManager就是负责管理这些控制器的。<p><strong>kube-scheduler:</strong> 资源调度，按照预定的调度策略将Pod调度到相应的机器上；它负责节点资源管理，接受来自kube-apiserver创建Pods任务，并分配到某个节点。它会根据调度算法为新创建的Pod选择一个Node节点。<p><strong>kubectl:</strong> 客户端命令行工具，将接受的命令格式化后发送给kube-apiserver，作为整个系统的操作入口。<p><strong>kubelet:</strong> 负责维护容器的生命周期，负责管理pods和它们上面的容器，images镜像、volumes、etc。同时也负责Volume（CVI）和网络（CNI）的管理；kubelet运行在每个计算节点上，作为agent，接受分配该节点的Pods任务及管理容器，周期性获取容器状态，反馈给kube-apiserver; kubelet是Master在Node节点上的Agent，管理本机运行容器的生命周期，比如创建容器、Pod挂载数据卷、下载secret、获取容器和节点状态等工作。kubelet将每个Pod转换成一组容器。<p><strong>container runtime:</strong> 负责镜像管理以及Pod和容器的真正运行（CRI）；<p><strong>kube-proxy:</strong> 负责为Service提供cluster内部的服务发现和负载均衡；它运行在每个计算节点上，负责Pod网络代理。定时从etcd获取到service信息来做相应的策略。它在Node节点上实现Pod网络代理，维护网络规则和四层负载均衡工作。<br>docker或rocket(rkt): 运行容器。<p><strong>除了上面的几个核心组建, 还有一些常用插件(Add-ons)：</strong><p><strong>kube-dns:</strong> 负责为整个集群提供DNS服务;<br><strong>Ingress Controller:</strong> 为服务提供外网入口;<br><strong>Heapster:</strong> 提供资源监控;<br><strong>Dashboard:</strong> 提供GUI;<br><strong>Federation:</strong> 提供跨可用区的集群;<br><strong>Fluentd-elasticsearch:</strong> 提供集群日志采集、存储与查询;<p>其中:<br>master组件包括: kube-apiserver, kube-controller-manager, kube-scheduler;<br>Node组件包括: kubelet, kube-proxy, docker或rocket(rkt);<br>第三方服务：etcd<h2 id=Kubernetes-Master控制组件><a title="Kubernetes Master控制组件" class=headerlink href=#Kubernetes-Master控制组件></a>Kubernetes Master控制组件</h2><blockquote><p><strong>调度管理整个系统（集群），包含如下组件:</strong></blockquote><p><strong>Kubernetes API Server:</strong> 作为Kubernetes系统入口，其封装了核心对象的增删改查操作，以RESTful API接口方式提供给外部客户和内部组件调用,维护的REST对象持久化到Etcd中存储。<p><strong>Kubernetes Scheduler:</strong> 为新建立的Pod进行节点(node)选择(即分配机器)，负责集群的资源调度。组件抽离，可以方便替换成其他调度器。<p><strong>Kubernetes Controller:</strong> 负责执行各种控制器，目前已经提供了很多控制器来保证Kubernetes的正常运行。<p><strong>Replication Controller:</strong> 管理维护Replication Controller，关联Replication Controller和Pod，保证Replication Controller定义的副本数量与实际运行Pod数量一致。<p><strong>Node Controller:</strong> 管理维护Node，定期检查Node的健康状态，标识出(失效|未失效)的Node节点。<p><strong>Namespace Controller:</strong> 管理维护Namespace，定期清理无效的Namespace，包括Namesapce下的API对象，比如Pod、Service等。<p><strong>Service Controller:</strong> 管理维护Service，提供负载以及服务代理。<p><strong>EndPoints Controller:</strong> 管理维护Endpoints，关联Service和Pod，创建Endpoints为Service的后端，当Pod发生变化时，实时更新Endpoints (即Pod Ip + Container Port)。<p><strong>Service Account Controller:</strong> 管理维护Service Account，为每个Namespace创建默认的Service Account，同时为Service Account创建Service Account Secret。<p><strong>Persistent Volume Controller:</strong> 管理维护Persistent Volume和Persistent Volume Claim，为新的Persistent Volume Claim分配Persistent Volume进行绑定，为释放的Persistent Volume执行清理回收。<p><strong>Daemon Set Controller:</strong> 管理维护Daemon Set，负责创建Daemon Pod，保证指定的Node上正常的运行Daemon Pod。<p><strong>Deployment Controller:</strong> 管理维护Deployment，关联Deployment和Replication Controller，保证运行指定数量的Pod。当Deployment更新时，控制实现Replication Controller和　Pod的更新。<p><strong>Job Controller:</strong> 管理维护Job，为Jod创建一次性任务Pod，保证完成Job指定完成的任务数目<p><strong>Pod Autoscaler Controller:</strong> 实现Pod的自动伸缩，定时获取监控数据，进行策略匹配，当满足条件时执行Pod的伸缩动作。<p><img data-src=https://cdn.jsdelivr.net/gh/xiao1272209235/hexo-image@main/16500714498471650071449718.png><p><strong>Kubernetes Node运行节点，运行管理业务容器，包含如下组件:</strong><p><strong>Kubelet</strong>: 负责管控容器，Kubelet会从Kubernetes API Server接收Pod的创建请求，启动和停止容器，监控容器运行状态并汇报给Kubernetes API Server。<p><strong>Kubernetes Proxy</strong>: 负责为Pod创建代理服务，Kubernetes Proxy会从Kubernetes API Server获取所有的Service信息，并根据Service的信息创建代理服务，实现Service到Pod的请求路由和转发，从而实现Kubernetes层级的虚拟转发网络。<p><strong>Docker</strong>: Node上需要运行容器服务<p><img data-src=https://cdn.jsdelivr.net/gh/xiao1272209235/hexo-image@main/16500714258481650071425292.png><h2 id=Kubernetes的分层设计理念><a class=headerlink href=#Kubernetes的分层设计理念 title=Kubernetes的分层设计理念></a>Kubernetes的分层设计理念</h2><p>Kubernetes设计理念和功能类似Linux的分层架构，如下图:<p><img data-src=https://cdn.jsdelivr.net/gh/xiao1272209235/hexo-image@main/16500714018481650071401660.png><p><strong>核心层：</strong>Kubernetes最核心的功能，对外提供API构建高层的应用，对内提供插件式应用执行环境;<p><strong>应用层：</strong>部署(无状态应用、有状态应用、批处理任务、集群应用等)和路由(服务发现、DNS解析等);<p><strong>管理层：</strong>系统度量(如基础设施、容器和网络的度量)，自动化(如自动扩展、动态Provision等)以及策略管理(RBAC、Quota、PSP、NetworkPolicy等);<p><strong>接口层：</strong>kubectl命令行工具、客户端SDK以及集群联邦;<p><strong>生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴:</strong><ul><li>Kubernetes外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS应用、ChatOps等;<li>Kubernetes内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等;</ul><h1 id=四、Kubernetes基本对象概念><a class=headerlink href=#四、Kubernetes基本对象概念 title=四、Kubernetes基本对象概念></a><strong>四、Kubernetes基本对象概念</strong></h1><p>Kubernetes中的大部分概念Node、Pod、Replication Controller、Service等都可以看作一种“资源对象”，几乎所有的资源对象都可以通过kubectl工具（API调用）执行增、删、改、查等操作并将其保存在etcd中持久化存储。从这个角度来看，kubernetes其实是一个高度自动化的资源控制系统，通过跟踪对比etcd库里保存的“资源期望状态”与当前环境中的“实际资源状态”的差异来实现自动控制和自动纠错的高级功能。<p>基本对象：<br><strong>Pod</strong>: Pod是最小部署单元，一个Pod有一个或多个容器组成，Pod中容器共享存储和网络，在同一台Docker主机上运行; Pod 中的容器会作为一个整体被Master调度到一个Node上运行。pod 是一组container，pod里面的container是共享网络栈和存储卷等资源，是一个整体. pod 可以认为是容器组的概念，里面有个infra container 负责pod内所有container 共享 namespace。docker的容器可以类比成OS中的进程，而K8S的pod则更像是OS中的“进程组”概念。<p><strong>Service</strong> : Service一个应用服务抽象，定义了Pod逻辑集合和访问这个Pod集合的策略。Service代理Pod集合对外表现是为一个访问入口，分配一个集群IP地址，来自这个IP的请求将负载均衡转发后端Pod中的容器。Service通过LableSelector选择一组Pod提供服务。<p><strong>Volume</strong>: 数据卷，共享Pod中容器使用的数据。<p><strong>Namespace</strong>: 命名空间将对象逻辑上分配到不同Namespace，可以是不同的项目、用户等区分管理，并设定控制策略，从而实现多租户。命名空间也称为虚拟集群。<p><strong>Lable</strong>: 标签用于区分对象（比如Pod、Service），键/值对存在；每个对象可以有多个标签，通过标签关联对象。<p>基于基本对象更高层次抽象：<p><strong>ReplicaSet:</strong> 下一代ReplicationController。确保任何给定时间指定的Pod副本数量，并提供声明式更新等功能。RC与RS唯一区别就是lableselector支持不同，RS支持新的基于集合的标签，RC仅支持基于等式的标签。<p><strong>Deployment</strong>: Deployment是一个更高层次的API对象，它管理ReplicaSets和Pod，并提供声明式更新等功能。官方建议使用Deployment管理ReplicaSets，而不是直接使用ReplicaSets，这就意味着可能永远不需要直接操作ReplicaSet对象。负责无状态应用pod控制，支持二级控制器（HPA，HorizontalPodAutoscaler水平pod自动控制器）。<p><strong>StatefulSet</strong>: StatefulSet适合持久性的应用程序，有唯一的网络标识符（IP），持久存储，有序的部署、扩展、删除和滚动更新。负责有状态应用pod控制。<p><strong>DaemonSet</strong>: DaemonSet确保所有（或一些）节点运行同一个Pod。当节点加入Kubernetes集群中，Pod会被调度到该节点上运行，当节点从集群中移除时，DaemonSet的Pod会被删除。删除DaemonSet会清理它所有创建的Pod。<p><strong>Job</strong>: 一次性任务，运行完成后Pod销毁，不再重新启动新容器。还可以任务定时运行。Kubernetes中的Job 用于运行结束就删除的应用。<p>API对象是K8s集群中管理操作单元。K8s集群系每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。例如副本集Replica Set对应的API对象是RS。Kubernetes中所有的配置都是通过API对象的spec去设置的，也就是用户通过配置系统的理想状态来改变系统，这是k8s重要设计理念之一，即所有的操作都是声明式 (Declarative) 的而不是命令式(Imperative)的。声明式操作在分布式系统中好处是稳定，不怕丢操作或运行多次，例如设置副本数为3的操作运行多次也还是一个结果, 而给副本数加1的操作就不是声明式的, 运行多次结果就错了。<p><strong>Cluster</strong><br>Cluster 是计算、存储和网络资源的集合，Kubernetes 利用这些资源运行各种基于容器的应用<p><strong>Master</strong><br>kubernetes集群的管理节点，负责管理集群，提供集群的资源数据访问入口。拥有Etcd存储服务（可选），运行Api Server进程，Controller Manager服务进程及Scheduler服务进程，关联工作节点Node。Kubernetes API server提供HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口。也是集群控制的入口进程；Kubernetes Controller Manager是Kubernetes所有资源对象的自动化控制中心；Kubernetes Schedule是负责资源调度（Pod调度）的进程.<p><strong>Node</strong><br>Node是Kubernetes集群架构中运行Pod的服务节点（亦叫agent或minion）。Node是Kubernetes集群操作的单元，用来承载被分配Pod的运行，是Pod运行的宿主机。关联Master管理节点，拥有名称和IP、系统资源信息。运行docker eninge服务，守护进程kunelet及负载均衡器kube-proxy. 每个Node节点都运行着以下一组关键进程:<br>- kubelet：负责对Pod对于的容器的创建、启停等任务<br>- kube-proxy：实现Kubernetes Service的通信与负载均衡机制的重要组件<br>- Docker Engine（Docker）：Docker引擎，负责本机容器的创建和管理工作<p>Node节点可以在运行期间动态增加到Kubernetes集群中，默认情况下，kubelet会想master注册自己，这也是Kubernetes推荐的Node管理方式，kubelet进程会定时向Master汇报自身情报，如操作系统、Docker版本、CPU和内存，以及有哪些Pod在运行等等，这样Master可以获知每个Node节点的资源使用情况，冰实现高效均衡的资源调度策略。、<p><strong>Pod</strong><br>运行于Node节点上，若干相关容器的组合。Pod内包含的容器运行在同一宿主机上，使用相同的网络命名空间、IP地址和端口，能够通过localhost进行通。Pod是Kurbernetes进行创建、调度和管理的最小单位，它提供了比容器更高层次的抽象，使得部署和管理更加灵活。一个Pod可以包含一个容器或者多个相关容器。<p>Pod其实有两种类型：普通Pod和静态Pod，后者比较特殊，它并不存在Kubernetes的etcd存储中，而是存放在某个具体的Node上的一个具体文件中，并且只在此Node上启动。普通Pod一旦被创建，就会被放入etcd存储中，随后会被Kubernetes Master调度到摸个具体的Node上进行绑定，随后该Pod被对应的Node上的kubelet进程实例化成一组相关的Docker容器并启动起来。在默认情况下，当Pod里的某个容器停止时，Kubernetes会自动检测到这个问起并且重启这个Pod（重启Pod里的所有容器），如果Pod所在的Node宕机，则会将这个Node上的所有Pod重新调度到其他节点上。<p>Pod是在K8s集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务.比如你运行一个操作系统发行版的软件仓库，一个Nginx容器用来发布软件，另一个容器专门用来从源仓库做同步，这两个容器的镜像不太可能是一个团队开发的，但是他们一块儿工作才能提供一个微服务；这种情况下，不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。<p>kubernetes的最核心功能就是为了运行pod，其他组件是为了pod能够正常运行而执行的。pod可以分为两类：<br><strong>1.</strong> 自主式pod<br><strong>2.</strong> 控制器管理的pod<p>一个pod上有两类元数据，label 和 annotation<br>label：标签，对数据类型和程度要求严格，<br>annotation：注解，用于存储自己定义的复杂元数据，用来描述pod的属性<p>外部请求访问内部的pod经过了三级转发，第一级先到nodeip（宿主机ip）对应的端口，然后被转为cluster ip的service 端口，然后转换为PodIP的containerPort。<p><strong>Kubernetes 引入 Pod 主要基于下面两个目的：</strong><br>- 可管理性<br>有些容器天生就是需要紧密联系, 一起工作。Pod 提供了比容器更高层次的抽象，将它们封装到一个部署单元中。Kubernetes 以 Pod 为最小单位进行调度、扩展、共享资源、管理生命周期。<p>- 通信和资源共享<br>Pod 中的所有容器使用同一个网络 namespace，即相同的 IP 地址和 Port 空间。它们可以直接用 localhost 通信。同样的，这些容器可以共享存储，当 Kubernetes 挂载 volume 到 Pod，本质上是将 volume 挂载到 Pod 中的每一个容器。<p><img alt=img data-src=https://img2018.cnblogs.com/blog/907596/201908/907596-20190806151141561-1435899737.png><p>File Puller 会定期从外部的 Content Manager 中拉取最新的文件，将其存放在共享的 volume 中。Web Server 从 volume 读取文件，响应 Consumer 的请求。这两个容器是紧密协作的，它们一起为 Consumer 提供最新的数据；同时它们也通过 volume 共享数据。所以放到一个 Pod 是合适的。<p><strong>Controller</strong><br>Kubernetes 通常不会直接创建 Pod，而是通过 Controller 来管理 Pod 的。Controller 中定义了 Pod 的部署特性，比如有几个副本，在什么样的 Node 上运行等。为了满足不同的业务场景, Kubernetes 提供了多种 Controller，包括 Deployment、ReplicaSet、DaemonSet、StatefuleSet、Job 等.<p><strong>Replication Controller (副本集RC)</strong><br>Replication Controller用来管理Pod的副本，保证集群中存在指定数量的Pod副本。集群中副本的数量大于指定数量，则会停止指定数量之外的多余容器数量，反之，则会启动少于指定数量个数的容器，保证数量不变。Replication Controller是实现弹性伸缩、动态扩容和滚动升级的核心。<p>通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。少于指定数目，RC就会启动运行新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本 (这是k8s早期技术概念)<p><strong>Replica Set (副本集RS）</strong><br>RS是新一代RC，提供同样的高可用能力，区别主要在于RS后来居上，能支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为Deployment的理想状态参数使用. Replica Set 实现了 Pod 的多副本管理。使用 Deployment 时会自动创建 ReplicaSet，也就是说 Deployment 是通过 ReplicaSet 来管理 Pod 的多个副本，我们通常不需要直接使用 ReplicaSet。<p><strong>Deployment (部署)</strong><br>Deployment 是最常用的 Controller，Deployment 可以管理 Pod 的多个副本，并确保 Pod 按照期望的状态运行。Deployment是一个比RS应用模式更广的API对象，支持动态扩展。可以创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的RS，然后逐渐将新RS中副本数增加到理想状态，将旧RS中的副本数减小到0的复合操作 (逐步升级新得副本，剔除旧的副本).<br><strong>总结：</strong>RC、RS和Deployment只是保证了支撑服务的微服务Pod的数量.<p><strong>DaemonSet</strong><br>DaemonSet 用于每个 Node 最多只运行一个 Pod 副本的场景。正如其名称所揭示的，DaemonSet 通常用于运行 daemon。<p><strong>StatefuleSet</strong><br>StatefuleSet 能够保证 Pod 的每个副本在整个生命周期中名称是不变的。而其他 Controller 不提供这个功能，当某个 Pod 发生故障需要删除并重新启动时，Pod 的名称会发生变化。同时 StatefuleSet 会保证副本按照固定的顺序启动、更新或者删除。<p><strong>Service</strong><p>Service定义了Pod逻辑集合和访问该集合的策略，是真实服务的抽象。Service提供了统一的服务访问入口以及服务代理和发现机制，关联多个相同Label的Pod，用户不需要了解后台Pod是如何运行。<br>外部系统访问Service的问题:<br><strong>-></strong> 首先需要弄明白Kubernetes的三种IP这个问题<br><strong>-</strong> Node IP：Node节点的IP地址<br>　 <strong>-</strong> Pod IP： Pod的IP地址<br>　 <strong>-</strong> Cluster IP：Service的IP地址<br><strong>-></strong> 首先,Node IP是Kubernetes集群中节点的物理网卡IP地址，所有属于这个网络的服务器之间都能通过这个网络直接通信。这也表明Kubernetes集群之外的节点访问Kubernetes集群之内的某个节点或者TCP/IP服务的时候，必须通过Node IP进行通信<br><strong>-></strong> 其次，Pod IP是每个Pod的IP地址，他是Docker Engine根据docker0网桥的IP地址段进行分配的，通常是一个虚拟的二层网络。<p>最后Cluster IP是一个虚拟的IP，但更像是一个伪造的IP网络，原因有以下几点:<br><strong>-></strong> Cluster IP仅仅作用于Kubernetes Service这个对象，并由Kubernetes管理和分配P地址<br><strong>-></strong> Cluster IP无法被ping，他没有一个“实体网络对象”来响应<br><strong>-></strong> Cluster IP只能结合Service Port组成一个具体的通信端口，单独的Cluster IP不具备通信的基础，并且他们属于Kubernetes集群这样一个封闭的空间。<br><strong>-></strong> Kubernetes集群之内，Node IP网、Pod IP网于Cluster IP网之间的通信，采用的是Kubernetes自己设计的一种编程方式的特殊路由规则。<p>RC、RS和Deployment只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod，因此不能以确定的IP和端口号提供服务。要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的的后端服务实例。在K8s集群中，客户端需要访问的服务就是Service对象。每个Service会对应一个集群内部有效的虚拟IP，集群内部通过虚拟IP访问一个服务。在K8s集群中微服务的负载均衡是由Kube-proxy实现的。Kube-proxy是K8s集群内部的负载均衡器。它是一个分布式代理服务器，在K8s的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。与之相比，我们平时在服务器端做个反向代理做负载均衡，还要进一步解决反向代理的负载均衡和高可用问题。<p>Kubernetes 运行容器（Pod）与访问容器（Pod）这两项任务分别由 Controller 和 Service 执行。<p><strong>Namespace</strong><br>名字空间为K8s集群提供虚拟的隔离作用，K8s集群初始有两个名字空间，分别是默认名字空间default和系统名字空间kube-system，除此以外，管理员可以可以创建新的名字空间满足需要。<p><img alt=img data-src=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313232215198-1967204451.png><p><strong>Label</strong><br>Kubernetes中任意API对象都是通过Label进行标识，Label的实质是一系列的Key/Value键值对，其中key于value由用户自己指定。Label可以附加在各种资源对象上，如Node、Pod、Service、RC等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上去。Label是Replication Controller和Service运行的基础，二者通过Label来进行关联Node上运行的Pod。<p>我们可以通过给指定的资源对象捆绑一个或者多个不同的Label来实现多维度的资源分组管理功能，以便于灵活、方便的进行资源分配、调度、配置等管理工作。<br>一些常用的Label如下：<br>版本标签："release":"stable","release":"canary"......<br>环境标签："environment":"dev","environment":"qa","environment":"production"<br>架构标签："tier":"frontend","tier":"backend","tier":"middleware"<br>分区标签："partition":"customerA","partition":"customerB"<br>质量管控标签："track":"daily","track":"weekly"<p>Label相当于我们熟悉的标签，给某个资源对象定义一个Label就相当于给它大了一个标签，随后可以通过Label Selector（标签选择器）查询和筛选拥有某些Label的资源对象，Kubernetes通过这种方式实现了类似SQL的简单又通用的对象查询机制。<p>Label Selector在Kubernetes中重要使用场景如下:<br><strong>-></strong> kube-Controller进程通过资源对象RC上定义Label Selector来筛选要监控的Pod副本的数量，从而实现副本数量始终符合预期设定的全自动控制流程;<br><strong>-></strong> kube-proxy进程通过Service的Label Selector来选择对应的Pod，自动建立起每个Service岛对应Pod的请求转发路由表，从而实现Service的智能负载均衡;<br><strong>-></strong> 通过对某些Node定义特定的Label，并且在Pod定义文件中使用Nodeselector这种标签调度策略，kuber-scheduler进程可以实现Pod”定向调度“的特性;<p>​<br><strong>Master管理节点和Node工作节点的各组件关系:</strong><p><img alt=img data-src=https://img2018.cnblogs.com/blog/907596/201905/907596-20190531165628671-14705824.png><p>Kuberneter工作流程：<br>1）通过kubectl向kubernetes Master发出指令, Master节点主要提供API Server、Scheduler、Controller组件，接收kubectl命令，从Node节点获取Node资源信息，并发出调度任务。<br>2）Node节点提供kubelet、kube-proxy，每个node节点都安装docker，是实际的执行者。kubernetes不负责网络，所以一般是用flannel或者weave。<br>3）etcd是一个键值存储仓库，etcd负责服务发现和node信息存储。<strong>不过需要注意的是：</strong>由于etcd是负责存储，所以不建议搭建单点集群，如zookeeper一样，由于存在选举策略，所以一般推荐奇数个集群，如3，5，7。只要集群半数以上的结点存活，那么集群就可以正常运行，否则集群可能无法正常使用。<p><strong>Master</strong>：集群控制管理节点，所有的命令都经由master处理。<p><img alt=img data-src=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313222141535-880155421.png><p><strong>Node</strong>：是kubernetes集群的工作负载节点。Master为其分配工作，当某个Node宕机时，Master会将其工作负载自动转移到其他节点。<p><img alt=img data-src=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313222650824-93538855.png><p>Node节点可动态增加到kubernetes集群中，前提是这个节点已经正确安装、配置和启动了上述的关键进程，默认情况下，kubelet会向Master注册自己，这也kubernetes推荐的Node管理方式。一旦Node被纳入集群管理范围，kubelet会定时向Master汇报自身的情况，以及之前有哪些Pod在运行等，这样Master可以获知每个Node的资源使用情况，并实现高效均衡的资源调度策略。如果Node没有按时上报信息，则会被Master判断为失联，Node状态会被标记为Not Ready，随后Master会触发工作负载转移流程。<p><strong>Pod</strong>：是kubernetes最重要也是最基本的概念。每个Pod都会包含一个 “根容器”，还会包含一个或者多个紧密相连的业务容器。<p><img alt=img data-src=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313223036439-466023285.png><p>Kubernetes为每个Pod都分配了唯一IP地址, 称之为PodIP, 一个Pod里多个容器共享PodIP地址. 要求底层网络支持集群内任意两个Pod之间的直接通信，通常采用虚拟二层网络技术来实现 (Flannel).<p><strong>Label</strong>：是一个key=value的键值对，其中key与value由用户指定, 可以附加到各种资源对象上, 一个资源对象可以定义任意数量的Label。可以通过LabelSelector（标签选择器）查询和筛选资源对象。<p><img alt=img data-src=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313223346653-822874121.png><p><strong>RC</strong>：Replication Controller声明某个Pod的副本数在任意时刻都符合某个预期值。定义包含如下：<br>- Pod期待的副本数（replicas）;<br>- 用于筛选目标Pod的Label Selector;<br>- 当Pod副本数小于期望时，用于新的创建Pod的模板template;<p><strong>需要注意</strong><br>- 通过改变RC里的Pod副本数量，可以实现Pod的扩容或缩容功能;<br>- 通过改变RC里Pod模板中的镜像版本，可以实现Pod的滚动升级功能;<p><img alt=img data-src=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313223917071-1603229987.png><p><strong>Service</strong>：“微服务”，kubernetes中的核心。通过分析、识别并建模系统中的所有服务为微服务，最终系统有多个提供不同业务能力而又彼此独立的微服务单元所组成，服务之间通过TCP/IP进行通信。每个Pod都会被分配一个单独的IP地址，而且每个Pod都提供了一个独立的Endpoint以被客户端访问。<p>客户端如何访问？<br>部署负载均衡器，为Pod开启对外服务端口，将Pod的Endpoint列表加入转发列表中，客户端通过负载均衡器的对外IP+Port来访问此服务。每个Service都有一个全局唯一的虚拟ClusterIP，这样每个服务就变成了具备唯一IP地址的“通信节点”，服务调用就变成了最基础的TCP网络通信问题。<p><strong>Volume</strong>：是Pod中能够被多个容器访问的共享目录。定义在Pod之上，被一个Pod里的多个容器挂载到具体的文件目录之下；Volume与Pod生命周期相同。Volume可以让一个Pod里的多个容器共享文件、让容器的数据写到宿主机的磁盘上或者写文件到 网络存储中，具体如下图所示：<br><img alt=img data-src=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313224329386-903560638.png><p>在kubernetes1.2的时候，RC就由Replication Controller升级成Replica Set，“下一代RC”。命令兼容适用，Replica Set主要被Deployment这个更高层的资源对象所使用，从而形成一套Pod创建、删除、更新的编排机制。当我们使用Deployment时，无需关心它是如何创建和维护ReplicaSet的，这一切是自动发生的。<p><strong>Docker</strong>: 既然k8s是基于容器的，那么就不得不提到docker。2013年初，docker横空出世，孕育着新思想的“容器”，Docker选择容器作为核心和基础，以容器为资源分割和调度的基本单位，封装整个软件运行时环境，为开发者和系统管理员设计，用于构建、发布和运行分布式应用的平台。是一个跨平台、可移植并且简单易用的容器解决方案。通过操作系统内核技术（namespaces、cgroups等）为容器提供资源隔离与安全保障。<br><img alt=img data-src=https://img2018.cnblogs.com/blog/907596/201903/907596-20190313224553325-108461685.png><p>上图是一个image的简单使用。我们可以通过一个dockerfile来build自己的image。可以把image上传（push）到自己的私有镜像仓库，也可以从私有仓库pull到本地进行使用。可以单独使用命令行，直接run container，可以对container进行stop、start、restart操作。也可以对image进行save保存操作以及加载load操作，大家具体可以根据自己的使用，选择不同的操作即可。<p>Docker资源隔离技术<br>Docker选择容器作为核心和基础，以容器为资源分割和调度的基本单位，封装整个软件运行时环境，为开发者和系统管理员设计，用于构建、发布和运行分布式应用的平台。Docker是一个跨平台、可移植并且简单易用的容器解决方案, 通过操作系统内核技术（namespaces、cgroups等）为容器提供资源隔离与安全保障。<p>Docker监控<br>cAdvisor（Container Advisor）是Google开发的用于分析运行中容器的资源占用和性能指标的开源工具。cAdvisor是一个运行时的守护进程，负责收集、聚合、处理和输出运行中容器的信息。对于每个容器，cAdvisor都有资源隔离参数、资源使用历史情况以及完整的历史资源使用和网络统计信息的柱状图。cAdvisor不但可以为用户提供监控服务，还可以结合其他应用为用户提供良好的服务移植和定制。包括结合InfluxDB对数据进行存储，以及结合Grafana提供web控制台，自定义查询指标，并进行展示:<p><strong>当下配合Kubernetes集群比较成熟的监控方案是: Prometheus +Grafana</strong><h1 id=五、Kubernetes集群里容器之间的通讯方式><a class=headerlink href=#五、Kubernetes集群里容器之间的通讯方式 title=五、Kubernetes集群里容器之间的通讯方式></a>五、Kubernetes集群里容器之间的通讯方式</h1><p>Kubernetes集群里面容器是存在于pod里面的，所以容器之间通讯，一般分为三种类型：<ul><li><strong>pod内部容器之间</strong><li><strong>pod与pod容器之间</strong><li><strong>pod访问service服务</strong></ul><h2 id=pod内部容器之间><a class=headerlink href=#pod内部容器之间 title=pod内部容器之间></a>pod内部容器之间</h2><p>这种情况下容器通讯比较简单，因为k8s pod内部容器是共享网络空间的，所以容器直接可以使用localhost访问其他容器。k8s在启动容器的时候会先启动一个pause容器，这个容器就是实现这个功能的。<h2 id=pod与pod容器之间><a class=headerlink href=#pod与pod容器之间 title=pod与pod容器之间></a>pod与pod容器之间</h2><p>这种类型又可以分为两种情况：<ul><li>两个pod在同一台主机上面<li>两个pod分布在不同主机之上</ul><p>第一种情况，就比较简单了，就是docker默认的docker网桥互连容器。<br>第二种情况需要更为复杂的网络模型了，k8s官方推荐的是使用flannel组建一个大二层扁平网络，pod的ip分配由flannel统一分配，通讯过程也是走flannel的网桥。比如:<figure class="highlight shell"><table><tr><td class=code><pre><span class=line><span class="meta prompt_"># </span><span class=language-bash>docker --daemon --bip=172.17.18.1/24</span></span><br><span class=line>注意，这其中的"--bip=172.17.18.1/24"这个参数，它限制了所在节点容器获得的IP范围。</span><br></pre></table></figure><p>每个node上面都会创建一个flannel0虚拟网卡，用于跨node之间通讯。所以容器直接可以直接使用pod id进行通讯。跨节点通讯时，发送端数据会从docker0路由到flannel0虚拟网卡，接收端数据会从flannel0路由到docker0，这是因为flannel会添加一个路由。<figure class="highlight shell"><table><tr><td class=code><pre><span class=line>发送端：</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>route -n</span></span><br><span class=line>172.17.0.0    0.0.0.0    255.255.0.0      U  0  0  0   flannel0</span><br><span class=line>172.17.13.0   0.0.0.0    255.255.255.0    U  0  0  0   docker0</span><br><span class=line></span><br><span class=line>接收端：</span><br><span class=line>172.18.0.0    0.0.0.0    255.255.0.0      U  0  0  0   flannel0</span><br><span class=line>172.17.12.0   0.0.0.0    255.255.255.0    U  0  0  0   docker0</span><br></pre></table></figure><p>例如现在有一个数据包要从IP为172.17.13.2的容器发到IP为172.17.12.2的容器。根据数据发送节点的路由表，它只与172.17.0.0/16匹配这条记录匹配，因此数据从docker0出来以后就被投递到了flannel0。同理在目标节点，由于投递的地址是一个容器，因此目的地址一定会落在docker0对于的172.17.12.0/24这个记录上，自然的被投递到了docker0网卡。<p><strong>flannel的原理: 是将网络包封装在udp里面，所以发送端和接收端需要装包和解包，对性能有一定的影响。</strong>除了flannel，k8s也支持其他的网络模型，比较有名的还有calico。<h2 id=pod-访问service服务><a title="pod 访问service服务" class=headerlink href=#pod-访问service服务></a>pod 访问service服务</h2><p>这里涉及到k8s里面一个重要的概念service。它是一个服务的抽象，通过label（<strong>k8s会根据service和pod直接的关系创建endpoint</strong>，可以通过“<strong>kubectl get ep</strong>”查看）关联到后端的pod容器。<strong>Service分配的ip叫cluster ip是一个虚拟ip（相对固定，除非删除service）</strong>，这个ip只能在k8s集群内部使用，如果service需要对外提供，只能使用Nodeport方式映射到主机上，使用主机的ip和端口对外提供服务。（另外还可以使用LoadBalance方式，但这种方式是在gce这样的云环境里面使用的 ）。<p>节点上面有个kube-proxy进程，这个进程从master apiserver获取信息，感知service和endpoint的创建，然后做下面两个事情：<ul><li>为每个service 在集群中每个节点上面创建一个随机端口，任何该端口上面的连接会代理到相应的pod<li>集群中每个节点安装iptables规则，用于clusterip + port路由到上一步定义的随机端口上面，所以集群中每个node上面都有service的转发规则：</ul><figure class="highlight shell"><table><tr><td class=code><pre><span class=line>KUBE-PORTALS-CONTAINER 从容器中通过service cluster ip和端口访问service的请求</span><br><span class=line>KUBE-PORTALS-HOST 从主机中通过service cluster ip和端口访问service的请求</span><br><span class=line>KUBE-NODEPORT-CONTAINER 从容器中通过service nodeport端口访问service的请求</span><br><span class=line>KUBE-NODEPORT-HOST 从主机中通过service nodeport端口访问service的请求。</span><br></pre></table></figure><p>比如下面是一个测试环境内容:<figure class="highlight shell"><table><tr><td class=code><pre><span class=line>-A KUBE-NODEPORT-CONTAINER -p tcp -m comment --comment "smart/ccdb:port1521"  -m tcp --dport 50171 -j REDIRECT --to-ports 52244</span><br><span class=line>-A KUBE-NODEPORT-HOST -p tcp -m comment --comment "smart/ccdb:port1521" -m tcp --dport 50171 -j DNAT --to-destination 10.45.25.227:52244</span><br><span class=line>-A KUBE-PORTALS-CONTAINER -d 10.254.120.169/32 -p tcp -m comment --comment "smart/ccdb:port1521" -m tcp --dport 1521 -j REDIRECT --to-ports 52244</span><br><span class=line>-A KUBE-PORTALS-HOST -d 10.254.120.169/32 -p tcp -m comment --comment "smart/ccdb:port1521" -m tcp --dport 1521 -j DNAT --to-destination 10.45.25.227:5224452244</span><br><span class=line></span><br></pre></table></figure><p>这些就是kube-proxy针对service “"smart/ccdb:port1521"” 在节点上面监听的端口。<h1 id=六、Kubernetes日常维护命令><a class=headerlink href=#六、Kubernetes日常维护命令 title=六、Kubernetes日常维护命令></a>六、Kubernetes日常维护命令</h1><figure class="highlight shell"><table><tr><td class=code><pre><span class=line><span class="meta prompt_"># </span><span class=language-bash>一. 查看集群信息</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl cluster-info</span><br><span class=line>[root@k8s-master01 ~]# kubectl cluster-info dump</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>二. 查看各组件状态</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl -s http://localhost:8080 get componentstatuses</span><br><span class=line>NAME                 STATUS    MESSAGE             ERROR</span><br><span class=line>controller-manager   Healthy   ok</span><br><span class=line>scheduler            Healthy   ok</span><br><span class=line>etcd-0               Healthy   {"health":"true"}</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>或者</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl -s http://172.16.60.220:8080 get componentstatuses</span><br><span class=line>NAME                 STATUS    MESSAGE             ERROR</span><br><span class=line>scheduler            Healthy   ok</span><br><span class=line>controller-manager   Healthy   ok</span><br><span class=line>etcd-0               Healthy   {"health":"true"}</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>三. GET信息</span></span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>1) 查看节点 (k8s-master01 对应的是 172.16.60.220的主机名)</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>将命令中的node变为nodes也是可以的</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl get node</span><br><span class=line>NAME         STATUS    AGE</span><br><span class=line>k8s-node01   Ready     1d</span><br><span class=line>k8s-node02   Ready     1d</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>将命令中的node变为nodes也是可以的</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl -s http://k8s-master01:8080 get node</span><br><span class=line>NAME         STATUS    AGE</span><br><span class=line>k8s-node01   Ready     1d</span><br><span class=line>k8s-node02   Ready     1d</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>2) 查看pods清单（查看pod ip地址，下面命令加上<span class=string>"-o wide"</span>）</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>将pod变为pods也可以。如果有namespace，需要跟上<span class=string>"-n namespace名字"</span> 或 <span class=string>"--all-namespaces"</span></span></span><br><span class=line>[root@k8s-master01 ~]# kubectl get pod</span><br><span class=line>NAME                      READY     STATUS    RESTARTS   AGE</span><br><span class=line>nginx-controller-d97wj    1/1       Running   0          1h</span><br><span class=line>nginx-controller-lf11n    1/1       Running   0          1h</span><br><span class=line>tomcat-controller-35kzb   1/1       Running   0          18m</span><br><span class=line>tomcat-controller-lsph4   1/1       Running   0          18m</span><br><span class=line></span><br><span class=line>[root@k8s-master01 ~]# kubectl -s http://k8s-master01:8080 get pod          #将命令中的pod变为pods也是可以的</span><br><span class=line>NAME                      READY     STATUS    RESTARTS   AGE</span><br><span class=line>nginx-controller-d97wj    1/1       Running   0          1h</span><br><span class=line>nginx-controller-lf11n    1/1       Running   0          1h</span><br><span class=line>tomcat-controller-35kzb   1/1       Running   0          18m</span><br><span class=line>tomcat-controller-lsph4   1/1       Running   0          18m</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>3) 查看service清单</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>将命令中的service变为services也是可以的</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl get service</span><br><span class=line>NAME                       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class=line>kubernetes                 172.16.0.1       &LTnone>        443/TCP          1d</span><br><span class=line>nginx-service-clusterip    172.16.77.193    &LTnone>        8001/TCP         1h</span><br><span class=line>nginx-service-nodeport     172.16.234.94    &LTnodes>       8000:32172/TCP   59m</span><br><span class=line>tomcat-service-clusterip   172.16.144.116   &LTnone>        8801/TCP         14m</span><br><span class=line>tomcat-service-nodeport    172.16.183.234   &LTnodes>       8880:31960/TCP   11m</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>将命令中的service变为services也是可以的</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl -s http://172.16.60.220:8080 get service</span><br><span class=line>NAME                       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class=line>kubernetes                 172.16.0.1       &LTnone>        443/TCP          1d</span><br><span class=line>nginx-service-clusterip    172.16.77.193    &LTnone>        8001/TCP         1h</span><br><span class=line>nginx-service-nodeport     172.16.234.94    &LTnodes>       8000:32172/TCP   1h</span><br><span class=line>tomcat-service-clusterip   172.16.144.116   &LTnone>        8801/TCP         17m</span><br><span class=line>tomcat-service-nodeport    172.16.183.234   &LTnodes>       8880:31960/TCP   14m</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>或者  (后面的sed表示 打印奇数行)</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl get services -o json|grep '"name":'|sed -n '1~2p'</span><br><span class=line>                "name": "kubernetes",</span><br><span class=line>                "name": "nginx-service-clusterip",</span><br><span class=line>                "name": "nginx-service-nodeport",</span><br><span class=line>                "name": "tomcat-service-clusterip",</span><br><span class=line>                "name": "tomcat-service-nodeport",</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>4) 查看replicationControllers清单 (同理可以将命令中的replicationControllers变为replicationController也是可以的)</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl get replicationControllers</span><br><span class=line>NAME                DESIRED   CURRENT   READY     AGE</span><br><span class=line>nginx-controller    2         2         2         2h</span><br><span class=line>tomcat-controller   2         2         2         1h</span><br><span class=line></span><br><span class=line>[root@k8s-master01 ~]# kubectl -s http://172.16.60.220:8080 get replicationControllers</span><br><span class=line>NAME                DESIRED   CURRENT   READY     AGE</span><br><span class=line>nginx-controller    2         2         2         2h</span><br><span class=line>tomcat-controller   2         2         2         1h</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>5) 查看rc和namespace</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl get rc,namespace</span><br><span class=line>NAME                   DESIRED   CURRENT   READY     AGE</span><br><span class=line>rc/nginx-controller    2         2         2         2h</span><br><span class=line>rc/tomcat-controller   2         2         2         1h</span><br><span class=line></span><br><span class=line>NAME             STATUS    AGE</span><br><span class=line>ns/default       Active    1d</span><br><span class=line>ns/kube-system   Active    1d</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>6) 查看pod和svc(和service一样)</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl get pods,svc</span><br><span class=line>NAME                         READY     STATUS    RESTARTS   AGE</span><br><span class=line>po/nginx-controller-d97wj    1/1       Running   0          2h</span><br><span class=line>po/nginx-controller-lf11n    1/1       Running   0          2h</span><br><span class=line>po/tomcat-controller-35kzb   1/1       Running   0          1h</span><br><span class=line>po/tomcat-controller-lsph4   1/1       Running   0          1h</span><br><span class=line></span><br><span class=line>NAME                           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class=line>svc/kubernetes                 172.16.0.1       &LTnone>        443/TCP          1d</span><br><span class=line>svc/nginx-service-clusterip    172.16.77.193    &LTnone>        8001/TCP         2h</span><br><span class=line>svc/nginx-service-nodeport     172.16.234.94    &LTnodes>       8000:32172/TCP   2h</span><br><span class=line>svc/tomcat-service-clusterip   172.16.144.116   &LTnone>        8801/TCP         1h</span><br><span class=line>svc/tomcat-service-nodeport    172.16.183.234   &LTnodes>       8880:31960/TCP   1h</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>7) 以json格式输出pod的详细信息.</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl get pods</span><br><span class=line>NAME                      READY     STATUS    RESTARTS   AGE</span><br><span class=line>nginx-controller-d97wj    1/1       Running   0          2h</span><br><span class=line>nginx-controller-lf11n    1/1       Running   0          2h</span><br><span class=line>tomcat-controller-35kzb   1/1       Running   0          1h</span><br><span class=line>tomcat-controller-lsph4   1/1       Running   0          1h</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>注意下面命令中的pods的名称可以通过上面命令查看</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl get po nginx-controller-d97wj -o json</span><br><span class=line>{</span><br><span class=line>    "apiVersion": "v1",</span><br><span class=line>    "kind": "Pod",</span><br><span class=line>    "metadata": {</span><br><span class=line>        "annotations": {</span><br><span class=line>...................</span><br><span class=line>...................</span><br><span class=line>        "hostIP": "172.16.60.222",</span><br><span class=line>        "phase": "Running",</span><br><span class=line>        "podIP": "192.168.100.2",</span><br><span class=line>        "startTime": "2019-03-15T14:40:18Z"</span><br><span class=line>    }</span><br><span class=line>}</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>还可以输出其它格式和方法(kubectl get -h查看帮助)</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl get -h</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>8) 查看指定pod跑在哪个node上</span></span><br><span class=line>[root@k8s-master01 ~]# kubectl get po nginx-controller-d97wj -o wide</span><br><span class=line>NAME                     READY     STATUS    RESTARTS   AGE       IP              NODE</span><br><span class=line>nginx-controller-d97wj   1/1       Running   0          2h        192.168.100.2   k8s-node02</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>9) 获取指定json或ymal格式的KEY数据,custom-columns=XXXXX（自定义列名）:.status.hostIP（以“点开始”，然后写路径就可以）</span></span><br><span class=line>注意: 下面命令中的nginx-controller-d97wj是pod单元名称 (kubectl get pods 可以查看pods)</span><br><span class=line>[root@k8s-master01 ~]# kubectl get po nginx-controller-d97wj -o custom-columns=HOST-IP:.status.hostIP,POD-IP:.status.podIP</span><br><span class=line>HOST-IP         POD-IP</span><br><span class=line>172.16.60.222   192.168.100.2</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>10) describe方法</span></span><br><span class=line>describe类似于get，同样用于获取resource的相关信息。不同的是，get获得的是更详细的resource个性的详细信息，describe获得的是resource集群相关的信息。</span><br><span class=line>describe命令同get类似，但是describe不支持-o选项，对于同一类型resource，describe输出的信息格式，内容域相同。</span><br><span class=line></span><br><span class=line>需要注意:  如果发现是查询某个resource的信息，使用get命令能够获取更加详尽的信息。但是如果想要查询某个resource的状态，如某个pod并不是在running状态，</span><br><span class=line>这时需要获取更详尽的状态信息时，就应该使用describe命令。</span><br><span class=line></span><br><span class=line>[root@k8s-master01 ~]# kubectl describe po nginx-controller-d97wj</span><br><span class=line>Name:           nginx-controller-d97wj</span><br><span class=line>Namespace:      default</span><br><span class=line>Node:           k8s-node02/172.16.60.222</span><br><span class=line>Start Time:     Fri, 15 Mar 2019 22:40:18 +0800</span><br><span class=line>Labels:         name=nginx</span><br><span class=line>Status:         Running</span><br><span class=line>IP:             192.168.100.2</span><br><span class=line>Controllers:    ReplicationController/nginx-controller</span><br><span class=line>Containers:</span><br><span class=line>  nginx:</span><br><span class=line>    Container ID:               docker://8ae4502b4e62120322de98aa532e653d3d2e058ffbb0b842e0f265621bebbe61</span><br><span class=line>    Image:                      172.16.60.220:5000/nginx</span><br><span class=line>    Image ID:                   docker-pullable://172.16.60.220:5000/nginx@sha256:7734a210432278817f8097acf2f72d20e2ccc7402a0509810c44b3a8bfe0094a</span><br><span class=line>    Port:                       80/TCP</span><br><span class=line>    State:                      Running</span><br><span class=line>      Started:                  Fri, 15 Mar 2019 22:40:19 +0800</span><br><span class=line>    Ready:                      True</span><br><span class=line>    Restart Count:              0</span><br><span class=line>    Volume Mounts:              &LTnone></span><br><span class=line>    Environment Variables:      &LTnone></span><br><span class=line>Conditions:</span><br><span class=line>  Type          Status</span><br><span class=line>  Initialized   True</span><br><span class=line>  Ready         True</span><br><span class=line>  PodScheduled  True</span><br><span class=line>No volumes.</span><br><span class=line>QoS Class:      BestEffort</span><br><span class=line>Tolerations:    &LTnone></span><br><span class=line>No events.</span><br><span class=line></span><br><span class=line>11) create创建</span><br><span class=line>kubectl命令用于根据文件或输入创建集群resource。如果已经定义了相应resource的yaml或son文件，直接kubectl create -f filename即可创建文件内定义的</span><br><span class=line>resource。也可以直接只用子命令[namespace/secret/configmap/serviceaccount]等直接创建相应的resource。从追踪和维护的角度出发，建议使用json或</span><br><span class=line>yaml的方式定义资源。</span><br><span class=line></span><br><span class=line>命令格式:</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl create -f 文件名</span></span><br><span class=line></span><br><span class=line>12) replace更新替换资源</span><br><span class=line>replace命令用于对已有资源进行更新、替换。如前面create中创建的nginx，当我们需要更新resource的一些属性的时候，如果修改副本数量，增加、修改label，</span><br><span class=line>更改image版本，修改端口等。都可以直接修改原yaml文件，然后执行replace命令。</span><br><span class=line></span><br><span class=line>需要注意: 名字不能被更更新。另外，如果是更新label，原有标签的pod将会与更新label后的rc断开联系，有新label的rc将会创建指定副本数的新的pod，但是默认</span><br><span class=line>并不会删除原来的pod。所以此时如果使用get po将会发现pod数翻倍，进一步check会发现原来的pod已经不会被新rc控制，此处只介绍命令不详谈此问题，好奇者可自行实验。</span><br><span class=line></span><br><span class=line>命令格式:</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl replace -f nginx-rc.yaml</span></span><br><span class=line></span><br><span class=line>13) patch</span><br><span class=line>如果一个容器已经在运行，这时需要对一些容器属性进行修改，又不想删除容器，或不方便通过replace的方式进行更新。kubernetes还提供了一种在容器运行时，直接</span><br><span class=line>对容器进行修改的方式，就是patch命令。 如创建pod的label是app=nginx-2，如果在运行过程中，需要把其label改为app=nginx-3。</span><br><span class=line>这个patch命令如下：</span><br><span class=line>[root@k8s-master01 ~]# kubectl patch pod nginx-controller-d97wj -p '{"metadata":{"labels":{"app":"nginx-3"}}}'</span><br><span class=line>"nginx-controller-d97wj" patched</span><br><span class=line></span><br><span class=line>14) edit</span><br><span class=line>edit提供了另一种更新resource源的操作，通过edit能够灵活的在一个common的resource基础上，发展出更过的significant resource。</span><br><span class=line>例如，使用edit直接更新前面创建的pod的命令为：</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl edit po nginx-controller-d97wj</span></span><br><span class=line></span><br><span class=line>上面命令的效果等效于：</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get po nginx-controller-d97wj -o yaml >> /tmp/nginx-tmp.yaml</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>vim /tmp/nginx-tmp.yaml             // 这此文件里做一些修改</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl replace -f /tmp/nginx-tmp.yaml</span></span><br><span class=line></span><br><span class=line>15) Delete</span><br><span class=line>根据resource名或label删除resource。</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl delete -f nginx-rc.yaml</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl delete po nginx-controller-d97wj</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl delete po nginx-controller-lf11n</span></span><br><span class=line></span><br><span class=line>16) apply</span><br><span class=line>apply命令提供了比patch，edit等更严格的更新resource的方式。通过apply，用户可以将resource的configuration使用source control的方式维护在版本库中。</span><br><span class=line>每次有更新时，将配置文件push到server，然后使用kubectl apply将更新应用到resource。kubernetes会在引用更新前将当前配置文件中的配置同已经应用的配置</span><br><span class=line>做比较，并只更新更改的部分，而不会主动更改任何用户未指定的部分。</span><br><span class=line></span><br><span class=line>apply命令的使用方式同replace相同，不同的是，apply不会删除原有resource，然后创建新的。apply直接在原有resource的基础上进行更新。同时kubectl apply</span><br><span class=line>还会resource中添加一条注释，标记当前的apply。类似于git操作。</span><br><span class=line></span><br><span class=line>17) logs</span><br><span class=line>logs命令用于显示pod运行中，容器内程序输出到标准输出的内容。跟docker的logs命令类似。如果要获得tail -f 的方式，也可以使用-f选项。</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl logs nginx-controller-d97wj</span></span><br><span class=line></span><br><span class=line>18) rolling-update</span><br><span class=line>rolling-update是一个非常重要的命令，对于已经部署并且正在运行的业务，rolling-update提供了不中断业务的更新方式。rolling-update每次起一个新的pod，</span><br><span class=line>等新pod完全起来后删除一个旧的pod，然后再起一个新的pod替换旧的pod，直到替换掉所有的pod。</span><br><span class=line></span><br><span class=line>rolling-update需要确保新的版本有不同的name，Version和label，否则会报错 。</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl rolling-update nginx-controller -f nginx-rc.yaml</span></span><br><span class=line></span><br><span class=line>如果在升级过程中，发现有问题还可以中途停止update，并回滚到前面版本</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl rolling-update nginx-controller --rollback</span></span><br><span class=line></span><br><span class=line>rolling-update还有很多其他选项提供丰富的功能，如--update-period指定间隔周期，使用时可以使用-h查看help信息.</span><br><span class=line></span><br><span class=line>19) scale  (注意下面的nginx-controller 是在nginx-rc.yaml文件中定义的name名称)</span><br><span class=line>scale用于程序在负载加重或缩小时副本进行扩容或缩小，如前面创建的nginx有两个副本，可以轻松的使用scale命令对副本数进行扩展或缩小。</span><br><span class=line>扩展副本数到4：</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl scale rc nginx-controller --replicas=4</span></span><br><span class=line></span><br><span class=line>重新缩减副本数到2：</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl scale rc nginx-controller --replicas=2</span></span><br><span class=line></span><br><span class=line>20) autoscale</span><br><span class=line>scale虽然能够很方便的对副本数进行扩展或缩小，但是仍然需要人工介入，不能实时自动的根据系统负载对副本数进行扩、缩。autoscale命令提供了自动根据pod负载</span><br><span class=line>对其副本进行扩缩的功能。</span><br><span class=line></span><br><span class=line>autoscale命令会给一个rc指定一个副本数的范围，在实际运行中根据pod中运行的程序的负载自动在指定的范围内对pod进行扩容或缩容。如前面创建的nginx，可以用</span><br><span class=line>如下命令指定副本范围在1~4</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl autoscale rc nginx-controller --min=1 --max=4</span></span><br><span class=line></span><br><span class=line>21) attach</span><br><span class=line>attach命令类似于docker的attach命令，可以直接查看容器中以daemon形式运行的进程的输出，效果类似于logs -f，退出查看使用ctrl-c。如果一个pod中有多个容器，</span><br><span class=line>要查看具体的某个容器的的输出，需要在pod名后使用-c containers name指定运行的容器。如下示例的命令为查看kube-system namespace中的kube-dns-v9-rcfuk pod</span><br><span class=line>中的skydns容器的输出。</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl attach kube-dns-v9-rcfuk -c skydns --namespace=kube-system</span></span><br><span class=line></span><br><span class=line>22) exec</span><br><span class=line>exec命令同样类似于docker的exec命令，为在一个已经运行的容器中执行一条shell命令，如果一个pod容器中，有多个容器，需要使用-c选项指定容器。</span><br><span class=line></span><br><span class=line>23) run</span><br><span class=line>类似于docker的run命令，直接运行一个image。</span><br><span class=line></span><br><span class=line>24) cordon, drain, uncordon</span><br><span class=line>这三个命令是正式release的1.2新加入的命令，三个命令一起介绍，是因为三个命令配合使用可以实现节点的维护。在1.2之前，因为没有相应的命令支持，如果要维护一个</span><br><span class=line>节点，只能stop该节点上的kubelet将该节点退出集群，是集群不在将新的pod调度到该节点上。如果该节点上本生就没有pod在运行，则不会对业务有任何影响。如果该节</span><br><span class=line>点上有pod正在运行，kubelet停止后，master会发现该节点不可达，而将该节点标记为notReady状态，不会将新的节点调度到该节点上。同时，会在其他节点上创建新的</span><br><span class=line>pod替换该节点上的pod。这种方式虽然能够保证集群的健壮性，但是任然有些暴力，如果业务只有一个副本，而且该副本正好运行在被维护节点上的话，可能仍然会造成业</span><br><span class=line>务的短暂中断。</span><br><span class=line></span><br><span class=line>1.2中新加入的这3个命令可以保证维护节点时，平滑的将被维护节点上的业务迁移到其他节点上，保证业务不受影响。如下图所示是一个整个的节点维护的流程（为了方便</span><br><span class=line>demo增加了一些查看节点信息的操作）：</span><br><span class=line>1- 首先查看当前集群所有节点状态，可以看到共四个节点都处于ready状态；</span><br><span class=line>2- 查看当前nginx两个副本分别运行在d-node1和k-node2两个节点上；</span><br><span class=line>3- 使用cordon命令将d-node1标记为不可调度；</span><br><span class=line>4- 再使用kubectl get nodes查看节点状态，发现d-node1虽然还处于Ready状态，但是同时还被禁能了调度，这意味着新的pod将不会被调度到d-node1上。</span><br><span class=line>5- 再查看nginx状态，没有任何变化，两个副本仍运行在d-node1和k-node2上；</span><br><span class=line>6- 执行drain命令，将运行在d-node1上运行的pod平滑的赶到其他节点上；</span><br><span class=line>7- 再查看nginx的状态发现，d-node1上的副本已经被迁移到k-node1上；这时候就可以对d-node1进行一些节点维护的操作，如升级内核，升级Docker等；</span><br><span class=line>8- 节点维护完后，使用uncordon命令解锁d-node1，使其重新变得可调度；8）检查节点状态，发现d-node1重新变回Ready状态</span><br><span class=line><span class="meta prompt_"></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get nodes</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get po -o wide</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl cordon d-node1</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get nodes</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get po -o wide</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl drain d-node1</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get po -o wide</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl uncordon</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl uncordon d-node1</span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get nodes</span></span><br><span class=line></span><br><span class=line>25) 查看某个pod重启次数(这个是参考)</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get pod nginx-controller-d97wj --template=<span class=string>"{{range .status.containerStatuses}}{{.name}}:{{.restartCount}}{{end}}"</span></span></span><br><span class=line></span><br><span class=line>26) 查看pod生命周期</span><br><span class=line>[root@k8s-master01 ~]# kubectl get pod nginx-controller-d97wj --template="{{.status.phase}}"</span><br><span class=line>Running</span><br><span class=line></span><br><span class=line>四、日常维护命令</span><br><span class=line>=============================================================================================================</span><br><span class=line>kubectl get pods</span><br><span class=line>kubectl get rc</span><br><span class=line>kubectl get service</span><br><span class=line>kubectl get componentstatuses</span><br><span class=line>kubectl get endpoints</span><br><span class=line>kubectl cluster-info</span><br><span class=line>kubectl create -f redis-master-controller.yaml</span><br><span class=line>kubectl delete -f redis-master-controller.yaml</span><br><span class=line>kubectl delete pod nginx-772ai</span><br><span class=line>kubectl logs -f pods/heapster-xxxxx -n kube-system                     #查看日志</span><br><span class=line>kubectl scale rc redis-slave --replicas=3                              #修改RC的副本数量，来实现Pod的动态缩放</span><br><span class=line>etcdctl cluster-health                                                 #检查网络集群健康状态</span><br><span class=line>etcdctl --endpoints=http://172.16.60.220:2379 cluster-health           #带有安全认证检查网络集群健康状态</span><br><span class=line>etcdctl member list</span><br><span class=line>etcdctl set /k8s/network/config '{ "Network": "10.1.0.0/16" }'</span><br><span class=line>etcdctl get /k8s/network/config</span><br><span class=line></span><br><span class=line></span><br><span class=line>五、基础进阶</span><br><span class=line>=============================================================================================================</span><br><span class=line>kubectl get services kubernetes-dashboard -n kube-system           #查看所有service</span><br><span class=line>kubectl get deployment kubernetes-dashboard -n kube-system         #查看所有发布</span><br><span class=line>kubectl get pods --all-namespaces                                  #查看所有pod</span><br><span class=line>kubectl get pods -o wide --all-namespaces                          #查看所有pod的IP及节点</span><br><span class=line>kubectl get pods -n kube-system | grep dashboard</span><br><span class=line>kubectl describe service/kubernetes-dashboard --namespace="kube-system"</span><br><span class=line>kubectl describe pods/kubernetes-dashboard-349859023-g6q8c --namespace="kube-system"       #指定类型查看</span><br><span class=line>kubectl describe pod nginx-772ai                                   #查看pod详细信息</span><br><span class=line>kubectl scale rc nginx --replicas=5                                #动态伸缩</span><br><span class=line>kubectl scale deployment redis-slave --replicas=5                  #动态伸缩</span><br><span class=line>kubectl scale --replicas=2 -f redis-slave-deployment.yaml          #动态伸缩</span><br><span class=line>kubectl exec -it tomcat-controller-35kzb /bin/bash                 #进入容器</span><br><span class=line>kubectl label nodes k8s-node01 zone=north                #增加节点lable值 spec.nodeSelector: zone: north, 指定pod在哪个节点</span><br><span class=line>kubectl get nodes -lzone                                 #获取zone的节点</span><br><span class=line>kubectl label pod tomcat-controller-35kzb role=master    #增加lable值 [key]=[value]</span><br><span class=line>kubectl label pod tomcat-controller-35kzb role-                       #删除lable值</span><br><span class=line>kubectl label pod tomcat-controller-35kzb role=backend --overwrite    #修改lable值</span><br><span class=line>kubectl rolling-update redis-master -f redis-master-controller-v2.yaml      #配置文件滚动升级</span><br><span class=line>kubectl rolling-update redis-master --image=redis-master:2.0                #命令升级</span><br><span class=line>kubectl rolling-update redis-master --image=redis-master:1.0 --rollback     #pod版本回滚</span><br><span class=line></span><br><span class=line>六、yaml使用及命令</span><br><span class=line>=============================================================================================================</span><br><span class=line>kubectl create -f nginx-deployment.yaml   #创建deployment资源</span><br><span class=line>kubectl get deploy      #查看deployment</span><br><span class=line>kubectl get rs          #查看ReplicaSet</span><br><span class=line>kubectl get pods --show-labels   #查看pods所有标签。可以添加"-all-namespaces" 或者 "-n kube-system"表示查看所有命名空间或某一命名空间里pods的标签</span><br><span class=line>kubectl get pods -l app=nginx    #根据标签查看pods</span><br><span class=line></span><br><span class=line>kubectl set image deployment/nginx-deployment nginx=nginx:1.11     #滚动更新镜像</span><br><span class=line>或者</span><br><span class=line>kubectl edit deployment/nginx-deployment</span><br><span class=line>或者</span><br><span class=line>kubectl apply -f nginx-deployment.yaml                             #也表示对yaml修改后进行更新操作，更新到kubernetes集群配置中</span><br><span class=line></span><br><span class=line>kubectl rollout status deployment/nginx-deployment                 #实时观察发布状态：</span><br><span class=line></span><br><span class=line>kubectl rollout history deployment/nginx-deployment                #查看deployment历史修订版本</span><br><span class=line>kubectl rollout history deployment/nginx-deployment --revision=3</span><br><span class=line></span><br><span class=line>kubectl rollout undo deployment/nginx-deployment                   #回滚到以前版本</span><br><span class=line>kubectl rollout undo deployment/nginx-deployment --to-revision=3</span><br><span class=line></span><br><span class=line>kubectl scale deployment nginx-deployment --replicas=10            #扩容deployment的Pod副本数量</span><br><span class=line></span><br><span class=line>kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80     #设置启动扩容/缩容</span><br><span class=line></span><br><span class=line>七、命名空间</span><br><span class=line>=============================================================================================================</span><br><span class=line>kubectl get namespace                            #获取k8s的命名空间</span><br><span class=line>kubectl get pod --namespace =[命令空间名称]        #获取对应命名空间内的pod，"--namespace"可以写成"-c"</span><br><span class=line>kubectl --namespace [命令空间名称] logs [pod名称] -c 容器名称    #获取对应namespace中对应pod的日志，如果不加"-c 容器名称",则默认查看的是该pod下第一个容器的日志</span><br><span class=line></span><br><span class=line>pod维护示例：</span><br><span class=line>查看某个命令空间下的pod</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get pods -n namespace</span></span><br><span class=line></span><br><span class=line>在没有pod 的yaml文件时，强制重启某个pod</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get pod podname -n namespace -o yaml | kubectl replace --force -f -</span></span><br><span class=line></span><br><span class=line>查看某个pod重启次数(这个是参考)</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get pod podname -n namespace --template=<span class=string>"{{range .status.containerStatuses}}{{.name}}:{{.restartCount}}{{end}}"</span></span></span><br><span class=line></span><br><span class=line>查看pod生命周期</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl get pod podname --template=<span class=string>"{{.status.phase}}"</span></span></span><br><span class=line></span><br><span class=line>查看kube-space命令空间下的pod</span><br><span class=line>[root@m7-autocv-gpu01 ~]# kubectl get pods -n kube-system -o wide|grep -E 'elasticsearch|fluentd|kibana'</span><br><span class=line>elasticsearch-logging-0                  1/1     Running   0          5h9m    172.30.104.6   m7-autocv-gpu03   &LTnone></span><br><span class=line>elasticsearch-logging-1                  1/1     Running   0          4h59m   172.30.232.8   m7-autocv-gpu02   &LTnone></span><br><span class=line>fluentd-es-v2.2.0-mkkcf                  1/1     Running   0          5h9m    172.30.104.7   m7-autocv-gpu03   &LTnone></span><br><span class=line>kibana-logging-f6fc77549-nlxfg           1/1     Running   0          42s     172.30.96.7    m7-autocv-gpu01   &LTnone></span><br><span class=line></span><br><span class=line>[root@m7-autocv-gpu01 ~]# kubectl get pod kibana-logging-f6fc77549-nlxfg -n kube-system -o yaml | kubectl replace --force -f -</span><br><span class=line>pod "kibana-logging-f6fc77549-d47nc" deleted</span><br><span class=line>pod/kibana-logging-f6fc77549-d47nc replaced</span><br><span class=line></span><br><span class=line>[root@m7-autocv-gpu01 ~]#  kubectl get pod kibana-logging-f6fc77549-nlxfg -n kube-system --template="{{range .status.containerStatuses}}{{.name}}:{{.restartCount}}{{end}}"</span><br><span class=line>kibana-logging:0</span><br><span class=line></span><br><span class=line>[root@m7-autocv-gpu01 ~]# kubectl get pod kibana-logging-f6fc77549-nlxfg -n kube-system --template="{{.status.phase}}"</span><br><span class=line>Running</span><br><span class=line></span><br><span class=line>八、进入pod内的容器</span><br><span class=line>=============================================================================================================</span><br><span class=line>kubernetes中登录pod中的容器，如下，kevintest-f857f78ff-dlp24是pod名称，webha是命名空间</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl -n webha <span class=built_in>exec</span> -it kevintest-f857f78ff-dlp24 -- bash       <span class=comment>#登录后终端信息中显示主机名</span></span></span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl -n webha <span class=built_in>exec</span> -it kevintest-f857f78ff-dlp24 sh            <span class=comment>#登录后终端信息中不显示主机名</span></span></span><br><span class=line></span><br><span class=line>如果pod中有多个容器，则默认登录到第一个容器中。</span><br><span class=line>也可以通过-c参数制定登录到哪个容器中, 比如进入kevintest-f857f78ff-dlp24的nginx_bo容器</span><br><span class=line><span class="meta prompt_"># </span><span class=language-bash>kubectl -n webha <span class=built_in>exec</span> -it kevintest-f857f78ff-dlp24 -c nginx_bo -- bash</span></span><br></pre></table></figure><h1 id=七、Kubernetes集群部署失败的一般原因><a class=headerlink href=#七、Kubernetes集群部署失败的一般原因 title=七、Kubernetes集群部署失败的一般原因></a>七、Kubernetes集群部署失败的一般原因</h1><h2 id=1-错误的容器镜像-非法的仓库权限><a title="1. 错误的容器镜像/非法的仓库权限" class=headerlink href=#1-错误的容器镜像-非法的仓库权限></a><strong>1. 错误的容器镜像/非法的仓库权限</strong></h2><p>其中两个最普遍的问题是：a) 指定了错误的容器镜像；b) 使用私有镜像却不提供仓库认证信息。这在首次使用 Kubernetes 或者绑定 CI/CD 环境时尤其棘手。看个例子:<figure class="highlight shell"><table><tr><td class=code><pre><span class=line>首先我们创建一个名为 fail 的 deployment，它指向一个不存在的 Docker 镜像：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl run fail --image=rosskukulinski/dne:v1.0.0</span></span><br><span class=line></span><br><span class=line>然后我们查看 Pods，可以看到有一个状态为 ErrImagePull 或者 ImagePullBackOff 的 Pod：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl get pods</span></span><br><span class=line>NAME                    READY     STATUS             RESTARTS   AGE</span><br><span class=line>fail-1036623984-hxoas   0/1       ImagePullBackOff   0          2m</span><br><span class=line></span><br><span class=line>想查看更多信息，可以 describe 这个失败的 Pod：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl describe pod fail-1036623984-hxoas</span></span><br><span class=line></span><br><span class=line>查看 describe 命令的输出中 Events 这部分，我们可以看到如下内容：</span><br><span class=line>Events:</span><br><span class=line>FirstSeen    LastSeen    Count   From                        SubObjectPath       Type        Reason      Message</span><br><span class=line>---------    --------    -----   ----                        -------------       --------    ------      -------</span><br><span class=line>5m        5m      1   {default-scheduler }                            Normal      Scheduled   Successfully assigned fail-1036623984-hxoas to gke-nrhk-1-default-pool-a101b974-wfp7</span><br><span class=line>5m        2m      5   {kubelet gke-nrhk-1-default-pool-a101b974-wfp7} spec.containers{fail}   Normal      Pulling     pulling image "rosskukulinski/dne:v1.0.0"</span><br><span class=line>5m        2m      5   {kubelet gke-nrhk-1-default-pool-a101b974-wfp7} spec.containers{fail}   Warning     Failed      Failed to pull image "rosskukulinski/dne:v1.0.0": Error: image rosskukulinski/dne not found</span><br><span class=line>5m        2m      5   {kubelet gke-nrhk-1-default-pool-a101b974-wfp7}             Warning     FailedSync  Error syncing pod, skipping: failed to "StartContainer" for "fail" with ErrImagePull: "Error: image rosskukulinski/dne not found"</span><br><span class=line></span><br><span class=line>5m    11s 19  {kubelet gke-nrhk-1-default-pool-a101b974-wfp7} spec.containers{fail}   Normal  BackOff     Back-off pulling image "rosskukulinski/dne:v1.0.0"</span><br><span class=line>5m    11s 19  {kubelet gke-nrhk-1-default-pool-a101b974-wfp7}             Warning FailedSync  Error syncing pod, skipping: failed to "StartContainer" for "fail" with ImagePullBackOff: "Back-off pulling image \"rosskukulinski/dne:v1.0.0\""</span><br><span class=line></span><br><span class=line>显示错误的那句话：Failed to pull image "rosskukulinski/dne:v1.0.0": Error: image rosskukulinski/dne not found 告诉我们 Kubernetes无法找到镜像 rosskukulinski/dne:v1.0.0。</span><br><span class=line></span><br><span class=line>因此问题变成：为什么 Kubernetes 拉不下来镜像？</span><br><span class=line></span><br><span class=line>除了网络连接问题外，还有三个主要元凶：</span><br><span class=line>- 镜像 tag 不正确</span><br><span class=line>- 镜像不存在（或者是在另一个仓库）</span><br><span class=line>- Kubernetes 没有权限去拉那个镜像</span><br><span class=line></span><br><span class=line>如果你没有注意到你的镜像 tag 的拼写错误，那么最好就用你本地机器测试一下。</span><br><span class=line></span><br><span class=line>通常我会在本地开发机上，用 docker pull 命令，带上 完全相同的镜像 tag，来跑一下。比如上面的情况，我会运行命令 docker pull rosskukulinski/dne:v1.0.0。</span><br><span class=line>如果这成功了，那么很可能 Kubernetes 没有权限去拉取这个镜像。参考镜像拉取 Secrets 来解决这个问题。</span><br><span class=line>如果失败了，那么我会继续用不显式带 tag 的镜像测试 - docker pull rosskukulinski/dne - 这会尝试拉取 tag 为 latest 的镜像。如果这样成功，表明原来指定的 tag 不存在。这可能是人为原因，拼写错误，或者 CI/CD 的配置错误。</span><br><span class=line></span><br><span class=line>如果 docker pull rosskukulinski/dne（不指定 tag）也失败了，那么我们碰到了一个更大的问题：我们所有的镜像仓库中都没有这个镜像。默认情况下，Kubernetes 使用 Dockerhub 镜像仓库，如果你在使用 Quay.io，AWS ECR，或者 Google Container Registry，你要在镜像地址中指定这个仓库的 URL，比如使用 Quay，镜像地址就变成 quay.io/rosskukulinski/dne:v1.0.0。</span><br><span class=line></span><br><span class=line>如果你在使用 Dockerhub，那你应该再次确认你发布镜像到 Dockerhub 的系统，确保名字和 tag 匹配你的 deployment 正在使用的镜像。</span><br><span class=line></span><br><span class=line>注意：观察 Pod 状态的时候，镜像缺失和仓库权限不正确是没法区分的。其它情况下，Kubernetes 将报告一个 ErrImagePull 状态。</span><br></pre></table></figure><h2 id=2-应用启动之后又挂掉><a title="2. 应用启动之后又挂掉" class=headerlink href=#2-应用启动之后又挂掉></a><strong>2. 应用启动之后又挂掉</strong></h2><p>无论你是在 Kubernetes 上启动新应用，还是迁移应用到已存在的平台，应用在启动之后就挂掉都是一个比较常见的现象。看个例子:<figure class="highlight shell"><table><tr><td class=code><pre><span class=line>我们创建一个 deployment，它的应用会在1秒后挂掉：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl run crasher --image=rosskukulinski/crashing-app</span></span><br><span class=line></span><br><span class=line>我们看一下 Pods 的状态：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl get pods</span></span><br><span class=line>NAME                       READY     STATUS             RESTARTS   AGE</span><br><span class=line>crasher-2443551393-vuehs   0/1       CrashLoopBackOff   2          54s</span><br><span class=line></span><br><span class=line>CrashLoopBackOff 告诉我们，Kubernetes 正在尽力启动这个 Pod，但是一个或多个容器已经挂了，或者正被删除。</span><br><span class=line></span><br><span class=line>让我们 describe 这个 Pod 去获取更多信息：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl describe pod crasher-2443551393-vuehs</span></span><br><span class=line>Name:        crasher-2443551393-vuehs</span><br><span class=line>Namespace:    fail</span><br><span class=line>Node:        gke-nrhk-1-default-pool-a101b974-wfp7/10.142.0.2</span><br><span class=line>Start Time:    Fri, 10 Feb 2017 14:20:29 -0500</span><br><span class=line>Labels:        pod-template-hash=2443551393</span><br><span class=line>    run=crasher</span><br><span class=line>Status:        Running</span><br><span class=line>IP:        10.0.0.74</span><br><span class=line>Controllers:    ReplicaSet/crasher-2443551393</span><br><span class=line>Containers:</span><br><span class=line>crasher:</span><br><span class=line>Container ID:    docker://51c940ab32016e6d6b5ed28075357661fef3282cb3569117b0f815a199d01c60</span><br><span class=line>Image:        rosskukulinski/crashing-app</span><br><span class=line>Image ID:        docker://sha256:cf7452191b34d7797a07403d47a1ccf5254741d4bb356577b8a5de40864653a5</span><br><span class=line>Port:</span><br><span class=line>State:        Terminated</span><br><span class=line>  Reason:        Error</span><br><span class=line>  Exit Code:    1</span><br><span class=line>  Started:        Fri, 10 Feb 2017 14:22:24 -0500</span><br><span class=line>  Finished:        Fri, 10 Feb 2017 14:22:26 -0500</span><br><span class=line>Last State:        Terminated</span><br><span class=line>  Reason:        Error</span><br><span class=line>  Exit Code:    1</span><br><span class=line>  Started:        Fri, 10 Feb 2017 14:21:39 -0500</span><br><span class=line>  Finished:        Fri, 10 Feb 2017 14:21:40 -0500</span><br><span class=line>Ready:        False</span><br><span class=line>Restart Count:    4</span><br><span class=line>...</span><br><span class=line></span><br><span class=line>好可怕，Kubernetes 告诉我们这个 Pod 正被 Terminated，因为容器里的应用挂了。我们还可以看到应用的 Exit Code 是 1。后面我们可能还会看到一个 OOMKilled 错误。</span><br><span class=line></span><br><span class=line>我们的应用正在挂掉？为什么？</span><br><span class=line></span><br><span class=line>首先我们查看应用日志。假定你发送应用日志到 stdout（事实上你也应该这么做），你可以使用 kubectl logs 看到应用日志:</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl logs crasher-2443551393-vuehs</span></span><br><span class=line></span><br><span class=line>不幸的是，这个 Pod 没有任何日志。这可能是因为我们正在查看一个新起的应用实例，因此我们应该查看前一个容器：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl logs crasher-2443551393-vuehs --previous</span></span><br><span class=line></span><br><span class=line>什么！我们的应用仍然不给我们任何东西。这个时候我们应该给应用加点启动日志了，以帮助我们定位这个问题。我们也可以本地运行一下这个容器，以确定是否缺失环境变量或者挂载卷。</span><br></pre></table></figure><h2 id=3-缺失-ConfigMap-或者-Secret><a title="3. 缺失 ConfigMap 或者 Secret" class=headerlink href=#3-缺失-ConfigMap-或者-Secret></a><strong>3. 缺失 ConfigMap 或者 Secret</strong></h2><p>Kubernetes 最佳实践建议通过 ConfigMaps 或者 Secrets 传递应用的运行时配置。这些数据可以包含数据库认证信息，API endpoints，或者其它配置信息。一个常见的错误是，创建的 deployment 中引用的 ConfigMaps 或者 Secrets 的属性不存在，有时候甚至引用的 ConfigMaps 或者 Secrets 本身就不存在。<h3 id=缺失-ConfigMap><a title="缺失 ConfigMap" class=headerlink href=#缺失-ConfigMap></a><strong>缺失 ConfigMap</strong></h3><p>第一个例子，我们将尝试创建一个 Pod，它加载 ConfigMap 数据作为环境变量：<figure class="highlight shell"><table><tr><td class=code><pre><span class=line><span class="meta prompt_"># </span><span class=language-bash>configmap-pod.yaml</span></span><br><span class=line>apiVersion: v1</span><br><span class=line>kind: Pod</span><br><span class=line>metadata:</span><br><span class=line>name: configmap-pod</span><br><span class=line>spec:</span><br><span class=line>containers:</span><br><span class=line>- name: test-container</span><br><span class=line>  image: gcr.io/google_containers/busybox</span><br><span class=line>  command: [ "/bin/sh", "-c", "env" ]</span><br><span class=line>  env:</span><br><span class=line>    - name: SPECIAL_LEVEL_KEY</span><br><span class=line>      valueFrom:</span><br><span class=line>        configMapKeyRef:</span><br><span class=line>          name: special-config</span><br><span class=line>          key: special.how</span><br><span class=line></span><br><span class=line></span><br><span class=line>让我们创建一个 Pod：kubectl create -f configmap-pod.yaml。在等待几分钟之后，我们可以查看我们的 Pod：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl get pods</span></span><br><span class=line>NAME            READY     STATUS              RESTARTS   AGE</span><br><span class=line>configmap-pod   0/1       RunContainerError   0          3s</span><br><span class=line></span><br><span class=line>Pod 状态是 RunContainerError 。我们可以使用 kubectl describe 了解更多：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl describe pod configmap-pod</span></span><br><span class=line>[...]</span><br><span class=line>Events:</span><br><span class=line>FirstSeen    LastSeen    Count   From                        SubObjectPath           Type        Reason      Message</span><br><span class=line>---------    --------    -----   ----                        -------------           --------    ------      -------</span><br><span class=line>20s        20s     1   {default-scheduler }                                Normal      Scheduled   Successfully assigned configmap-pod to gke-ctm-1-sysdig2-35e99c16-tgfm</span><br><span class=line>19s        2s      3   {kubelet gke-ctm-1-sysdig2-35e99c16-tgfm}   spec.containers{test-container} Normal      Pulling     pulling image "gcr.io/google_containers/busybox"</span><br><span class=line>18s        2s      3   {kubelet gke-ctm-1-sysdig2-35e99c16-tgfm}   spec.containers{test-container} Normal      Pulled      Successfully pulled image "gcr.io/google_containers/busybox"</span><br><span class=line>18s        2s      3   {kubelet gke-ctm-1-sysdig2-35e99c16-tgfm}                   Warning     FailedSync  Error syncing pod, skipping: failed to "StartContainer" for "test-container" with RunContainerError: "GenerateRunContainerOptions: configmaps \"special-config\" not found"</span><br><span class=line></span><br><span class=line>Events 章节的最后一条告诉我们什么地方错了。Pod 尝试访问名为 special-config 的 ConfigMap，但是在该 namespace 下找不到。一旦我们创建这个 ConfigMap，Pod 应该重启并能成功拉取运行时数据。</span><br><span class=line></span><br><span class=line>在 Pod 规格说明中访问 Secrets 作为环境变量会产生相似的错误，就像我们在这里看到的 ConfigMap错误一样。</span><br></pre></table></figure><p>但是假如你通过 Volume 来访问 Secrets 或者 ConfigMap会发生什么呢？<h3 id=缺失-Secrets><a title="缺失 Secrets" class=headerlink href=#缺失-Secrets></a><strong>缺失 Secrets</strong></h3><p>下面是一个pod规格说明，它引用了名为 myothersecret 的 Secrets，并尝试把它挂为卷:<figure class="highlight shell"><table><tr><td class=code><pre><span class=line><span class="meta prompt_"># </span><span class=language-bash>missing-secret.yaml</span></span><br><span class=line>apiVersion: v1</span><br><span class=line>kind: Pod</span><br><span class=line>metadata:</span><br><span class=line>name: secret-pod</span><br><span class=line>spec:</span><br><span class=line>containers:</span><br><span class=line>- name: test-container</span><br><span class=line>  image: gcr.io/google_containers/busybox</span><br><span class=line>  command: [ "/bin/sh", "-c", "env" ]</span><br><span class=line>  volumeMounts:</span><br><span class=line>    - mountPath: /etc/secret/</span><br><span class=line>      name: myothersecret</span><br><span class=line>restartPolicy: Never</span><br><span class=line>volumes:</span><br><span class=line>- name: myothersecret</span><br><span class=line>  secret:</span><br><span class=line>    secretName: myothersecret</span><br><span class=line></span><br><span class=line>让我们用 kubectl create -f missing-secret.yaml 来创建一个 Pod。</span><br><span class=line></span><br><span class=line>几分钟后，我们 get Pods，可以看到 Pod 仍处于 ContainerCreating 状态：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl get pods</span></span><br><span class=line>NAME            READY     STATUS              RESTARTS   AGE</span><br><span class=line>secret-pod   0/1       ContainerCreating   0          4h</span><br><span class=line></span><br><span class=line>这就奇怪了。我们 describe 一下，看看到底发生了什么：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl describe pod secret-pod</span></span><br><span class=line>Name:        secret-pod</span><br><span class=line>Namespace:    fail</span><br><span class=line>Node:        gke-ctm-1-sysdig2-35e99c16-tgfm/10.128.0.2</span><br><span class=line>Start Time:    Sat, 11 Feb 2017 14:07:13 -0500</span><br><span class=line>Labels:</span><br><span class=line>Status:        Pending</span><br><span class=line>IP:</span><br><span class=line>Controllers:</span><br><span class=line></span><br><span class=line>[...]</span><br><span class=line></span><br><span class=line>Events:</span><br><span class=line>FirstSeen    LastSeen    Count   From                        SubObjectPath   Type        Reason      Message</span><br><span class=line>---------    --------    -----   ----                        -------------   --------    ------      -------</span><br><span class=line>18s        18s     1   {default-scheduler }                        Normal      Scheduled   Successfully assigned secret-pod to gke-ctm-1-sysdig2-35e99c16-tgfm</span><br><span class=line>18s        2s      6   {kubelet gke-ctm-1-sysdig2-35e99c16-tgfm}           Warning     FailedMount MountVolume.SetUp failed for volume "kubernetes.io/secret/337281e7-f065-11e6-bd01-42010af0012c-myothersecret" (spec.Name: "myothersecret") pod "337281e7-f065-11e6-bd01-42010af0012c" (UID: "337281e7-f065-11e6-bd01-42010af0012c") with: secrets "myothersecret" not found</span><br><span class=line></span><br><span class=line>Events 章节再次解释了问题的原因。它告诉我们 Kubelet 无法从名为 myothersecret 的 Secret 挂卷。为了解决这个问题，我们可以创建 myothersecret ，它包含必要的安全认证信息。一旦 myothersecret 创建完成，容器也将正确启动。</span><br></pre></table></figure><h2 id=4-活跃度-就绪状态探测失败><a title="4. 活跃度/就绪状态探测失败" class=headerlink href=#4-活跃度-就绪状态探测失败></a><strong>4. 活跃度/就绪状态探测失败</strong></h2><p>在 Kubernetes 中处理容器问题时，需要注意的是：你的容器应用是 running 状态，不代表它在工作！？<p>Kubernetes 提供了两个基本特性，称作<strong>活跃度探测</strong>和<strong>就绪状态探测</strong>。本质上来说，活跃度/就绪状态探测将定期地执行一个操作（例如发送一个 HTTP 请求，打开一个 tcp 连接，或者在你的容器内运行一个命令），以确认你的应用和你预想的一样在工作。<p>如果活跃度探测失败，Kubernetes 将杀掉你的容器并重新创建一个。如果就绪状态探测失败，这个 Pod 将不会作为一个服务的后端 endpoint，也就是说不会流量导到这个 Pod，直到它变成 Ready。<p>如果你试图部署变更你的活跃度/就绪状态探测失败的应用，滚动部署将一直悬挂，因为它将等待你的所有 Pod 都变成 Ready。<p>这个实际是怎样的情况？以下是一个 Pod 规格说明，它定义了活跃度/就绪状态探测方法，都是基于8080端口对 /healthy 路由进行健康检查：<figure class="highlight shell"><table><tr><td class=code><pre><span class=line>apiVersion: v1</span><br><span class=line>kind: Pod</span><br><span class=line>metadata:</span><br><span class=line>name: liveness-pod</span><br><span class=line>spec:</span><br><span class=line>containers:</span><br><span class=line>- name: test-container</span><br><span class=line>  image: rosskukulinski/leaking-app</span><br><span class=line>  livenessProbe:</span><br><span class=line>    httpGet:</span><br><span class=line>      path: /healthz</span><br><span class=line>      port: 8080</span><br><span class=line>    initialDelaySeconds: 3</span><br><span class=line>    periodSeconds: 3</span><br><span class=line>  readinessProbe:</span><br><span class=line>    httpGet:</span><br><span class=line>      path: /healthz</span><br><span class=line>      port: 8080</span><br><span class=line>    initialDelaySeconds: 3</span><br><span class=line>    periodSeconds: 3</span><br><span class=line></span><br><span class=line>让我们创建这个 Pod：kubectl create -f liveness.yaml，过几分钟后查看发生了什么：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl get pods</span></span><br><span class=line>NAME           READY     STATUS    RESTARTS   AGE</span><br><span class=line>liveness-pod   0/1       Running   4          2m</span><br><span class=line></span><br><span class=line>2分钟以后，我们发现 Pod 仍然没处于 Ready 状态，并且它已被重启了4次。让我们 describe 一下查看更多信息：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl describe pod liveness-pod</span></span><br><span class=line>Name:        liveness-pod</span><br><span class=line>Namespace:    fail</span><br><span class=line>Node:        gke-ctm-1-sysdig2-35e99c16-tgfm/10.128.0.2</span><br><span class=line>Start Time:    Sat, 11 Feb 2017 14:32:36 -0500</span><br><span class=line>Labels:</span><br><span class=line>Status:        Running</span><br><span class=line>IP:        10.108.88.40</span><br><span class=line>Controllers:</span><br><span class=line>Containers:</span><br><span class=line>test-container:</span><br><span class=line>Container ID:    docker://8fa6f99e6fda6e56221683249bae322ed864d686965dc44acffda6f7cf186c7b</span><br><span class=line>Image:        rosskukulinski/leaking-app</span><br><span class=line>Image ID:        docker://sha256:7bba8c34dad4ea155420f856cd8de37ba9026048bd81f3a25d222fd1d53da8b7</span><br><span class=line>Port:</span><br><span class=line>State:        Running</span><br><span class=line>  Started:        Sat, 11 Feb 2017 14:40:34 -0500</span><br><span class=line>Last State:        Terminated</span><br><span class=line>  Reason:        Error</span><br><span class=line>  Exit Code:    137</span><br><span class=line>  Started:        Sat, 11 Feb 2017 14:37:10 -0500</span><br><span class=line>  Finished:        Sat, 11 Feb 2017 14:37:45 -0500</span><br><span class=line>[...]</span><br><span class=line>Events:</span><br><span class=line>FirstSeen    LastSeen    Count   From                        SubObjectPath           Type        Reason      Message</span><br><span class=line>---------    --------    -----   ----                        -------------           --------    ------      -------</span><br><span class=line>8m        8m      1   {default-scheduler }                                Normal      Scheduled   Successfully assigned liveness-pod to gke-ctm-1-sysdig2-35e99c16-tgfm</span><br><span class=line>8m        8m      1   {kubelet gke-ctm-1-sysdig2-35e99c16-tgfm}   spec.containers{test-container} Normal      Created     Created container with docker id 0fb5f1a56ea0; Security:[seccomp=unconfined]</span><br><span class=line>8m        8m      1   {kubelet gke-ctm-1-sysdig2-35e99c16-tgfm}   spec.containers{test-container} Normal      Started     Started container with docker id 0fb5f1a56ea0</span><br><span class=line>7m        7m      1   {kubelet gke-ctm-1-sysdig2-35e99c16-tgfm}   spec.containers{test-container} Normal      Created     Created container with docker id 3f2392e9ead9; Security:[seccomp=unconfined]</span><br><span class=line>7m        7m      1   {kubelet gke-ctm-1-sysdig2-35e99c16-tgfm}   spec.containers{test-container} Normal      Killing     Killing container with docker id 0fb5f1a56ea0: pod "liveness-pod_fail(d75469d8-f090-11e6-bd01-42010af0012c)" container "test-container" is unhealthy, it will be killed and re-created.</span><br><span class=line>8m    16s 10  {kubelet gke-ctm-1-sysdig2-35e99c16-tgfm}   spec.containers{test-container} Warning Unhealthy   Liveness probe failed: Get http://10.108.88.40:8080/healthz: dial tcp 10.108.88.40:8080: getsockopt: connection refused</span><br><span class=line>8m    1s  85  {kubelet gke-ctm-1-sysdig2-35e99c16-tgfm}   spec.containers{test-container} Warning Unhealthy   Readiness probe failed: Get http://10.108.88.40:8080/healthz: dial tcp 10.108.88.40:8080: getsockopt: connection refused</span><br><span class=line></span><br><span class=line>Events 章节再次救了我们。我们可以看到活跃度探测和就绪状态探测都失败了。关键的一句话是 container "test-container" is unhealthy, it will be killed and re-created。这告诉我们 Kubernetes 正在杀这个容器，因为容器的活跃度探测失败了。</span><br><span class=line></span><br><span class=line>这里有三种可能性：</span><br><span class=line>- 你的探测不正确，健康检查的 URL 是否改变了？</span><br><span class=line>- 你的探测太敏感了， 你的应用是否要过一会才能启动或者响应？</span><br><span class=line>- 你的应用永远不会对探测做出正确响应，你的数据库是否配置错了</span><br><span class=line></span><br><span class=line>查看 Pod 日志是一个开始调测的好地方。一旦你解决了这个问题，新的 deployment 应该就能成功了。</span><br></pre></table></figure><h2 id=5-超出CPU-内存的限制><a title="5. 超出CPU/内存的限制" class=headerlink href=#5-超出CPU-内存的限制></a><strong>5. 超出CPU/内存的限制</strong></h2><p>Kubernetes 赋予集群管理员限制 Pod 和容器的 CPU 或内存数量的能力。作为应用开发者，你可能不清楚这个限制，导致 deployment 失败的时候一脸困惑。我们试图部署一个未知 CPU/memory 请求限额的 deployment：<figure class="highlight shell"><table><tr><td class=code><pre><span class=line><span class="meta prompt_"># </span><span class=language-bash>gateway.yaml</span></span><br><span class=line>apiVersion: extensions/v1beta1</span><br><span class=line>kind: Deployment</span><br><span class=line>metadata:</span><br><span class=line>name: gateway</span><br><span class=line>spec:</span><br><span class=line>template:</span><br><span class=line>metadata:</span><br><span class=line>  labels:</span><br><span class=line>    app: gateway</span><br><span class=line>spec:</span><br><span class=line>  containers:</span><br><span class=line>    - name: test-container</span><br><span class=line>      image: nginx</span><br><span class=line>      resources:</span><br><span class=line>        requests:</span><br><span class=line>          memory: 5Gi</span><br><span class=line></span><br><span class=line>你会看到我们设了 5Gi 的资源请求。让我们创建这个 deployment：kubectl create -f gateway.yaml。</span><br><span class=line></span><br><span class=line>现在我们可以看到我们的 Pod：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl get pods</span></span><br><span class=line>No resources found.</span><br><span class=line></span><br><span class=line></span><br><span class=line>为啥，让我们用 describe 来观察一下我们的 deployment：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl describe deployment/gateway</span></span><br><span class=line>Name:            gateway</span><br><span class=line>Namespace:        fail</span><br><span class=line>CreationTimestamp:    Sat, 11 Feb 2017 15:03:34 -0500</span><br><span class=line>Labels:            app=gateway</span><br><span class=line>Selector:        app=gateway</span><br><span class=line>Replicas:        0 updated | 1 total | 0 available | 1 unavailable</span><br><span class=line>StrategyType:        RollingUpdate</span><br><span class=line>MinReadySeconds:    0</span><br><span class=line>RollingUpdateStrategy:    0 max unavailable, 1 max surge</span><br><span class=line>OldReplicaSets:</span><br><span class=line>NewReplicaSet:        gateway-764140025 (0/1 replicas created)</span><br><span class=line>Events:</span><br><span class=line>FirstSeen    LastSeen    Count   From                SubObjectPath   Type        Reason          Message</span><br><span class=line>---------    --------    -----   ----                -------------   --------    ------          -------</span><br><span class=line>4m        4m      1   {deployment-controller }            Normal      ScalingReplicaSet   Scaled up replica set gateway-764140025 to 1</span><br><span class=line></span><br><span class=line>基于最后一行，我们的 deployment 创建了一个 ReplicaSet（gateway-764140025） 并把它扩展到 1。这个是用来管理 Pod 生命周期的实体。我们可以 describe 这个 ReplicaSet：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl describe rs/gateway-764140025</span></span><br><span class=line>Name:        gateway-764140025</span><br><span class=line>Namespace:    fail</span><br><span class=line>Image(s):    nginx</span><br><span class=line>Selector:    app=gateway,pod-template-hash=764140025</span><br><span class=line>Labels:        app=gateway</span><br><span class=line>    pod-template-hash=764140025</span><br><span class=line>Replicas:    0 current / 1 desired</span><br><span class=line>Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed</span><br><span class=line>No volumes.</span><br><span class=line>Events:</span><br><span class=line>FirstSeen    LastSeen    Count   From                SubObjectPath   Type        Reason      Message</span><br><span class=line>---------    --------    -----   ----                -------------   --------    ------      -------</span><br><span class=line>6m        28s     15  {replicaset-controller }            Warning     FailedCreate    Error creating: pods "gateway-764140025-" is forbidden: [maximum memory usage per Pod is 100Mi, but request is 5368709120., maximum memory usage per Container is 100Mi, but request is 5Gi.]</span><br></pre></table></figure><p>上面可知，集群管理员设置了每个 Pod 的最大内存使用量为 100Mi。你可以运行 kubectl describe limitrange 来查看当前租户的限制。<p>那么现在就有3个选择：<ul><li>要求你的集群管理员提升限额；<li>减少 deployment 的请求或者限额设置；<li>直接编辑限额；</ul><h2 id=6-资源配额><a title="6. 资源配额" class=headerlink href=#6-资源配额></a>6. 资源配额</h2><p>和资源限额类似，Kubernetes 也允许管理员给每个 namespace 设置资源配额。这些配额可以在 Pods，Deployments，PersistentVolumes，CPU，内存等资源上设置软性或者硬性限制。让我们看看超出资源配额后会发生什么。以下是我们的 deployment 例子:<figure class="highlight shell"><table><tr><td class=code><pre><span class=line><span class="meta prompt_"># </span><span class=language-bash>test-quota.yaml</span></span><br><span class=line>apiVersion: extensions/v1beta1</span><br><span class=line>kind: Deployment</span><br><span class=line>metadata:</span><br><span class=line>name: gateway-quota</span><br><span class=line>spec:</span><br><span class=line>template:</span><br><span class=line>spec:</span><br><span class=line>  containers:</span><br><span class=line>    - name: test-container</span><br><span class=line>      image: nginx</span><br><span class=line></span><br><span class=line>我们可用 kubectl create -f test-quota.yaml 创建，然后观察我们的 Pods：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl get pods</span></span><br><span class=line>NAME                            READY     STATUS    RESTARTS   AGE</span><br><span class=line>gateway-quota-551394438-pix5d   1/1       Running   0          16s</span><br><span class=line></span><br><span class=line>看起来很好，现在让我们扩展到 3 个副本：kubectl scale deploy/gateway-quota --replicas=3，然后再次观察 Pods：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl get pods</span></span><br><span class=line>NAME                            READY     STATUS    RESTARTS   AGE</span><br><span class=line>gateway-quota-551394438-pix5d   1/1       Running   0          9m</span><br><span class=line></span><br><span class=line>啊，我们的pod去哪了？让我们观察一下 deployment：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl describe deploy/gateway-quota</span></span><br><span class=line>Name:            gateway-quota</span><br><span class=line>Namespace:        fail</span><br><span class=line>CreationTimestamp:    Sat, 11 Feb 2017 16:33:16 -0500</span><br><span class=line>Labels:            app=gateway</span><br><span class=line>Selector:        app=gateway</span><br><span class=line>Replicas:        1 updated | 3 total | 1 available | 2 unavailable</span><br><span class=line>StrategyType:        RollingUpdate</span><br><span class=line>MinReadySeconds:    0</span><br><span class=line>RollingUpdateStrategy:    1 max unavailable, 1 max surge</span><br><span class=line>OldReplicaSets:</span><br><span class=line>NewReplicaSet:        gateway-quota-551394438 (1/3 replicas created)</span><br><span class=line>Events:</span><br><span class=line>FirstSeen    LastSeen    Count   From                SubObjectPath   Type        Reason          Message</span><br><span class=line>---------    --------    -----   ----                -------------   --------    ------          -------</span><br><span class=line>9m        9m      1   {deployment-controller }            Normal      ScalingReplicaSet   Scaled up replica set gateway-quota-551394438 to 1</span><br><span class=line>5m        5m      1   {deployment-controller }            Normal      ScalingReplicaSet   Scaled up replica set gateway-quota-551394438 to 3</span><br><span class=line></span><br><span class=line>在最后一行，我们可以看到 ReplicaSet 被告知扩展到 3 。我们用 describe 来观察一下这个 ReplicaSet 以了解更多信息：</span><br><span class=line>kubectl describe replicaset gateway-quota-551394438</span><br><span class=line>Name:        gateway-quota-551394438</span><br><span class=line>Namespace:    fail</span><br><span class=line>Image(s):    nginx</span><br><span class=line>Selector:    app=gateway,pod-template-hash=551394438</span><br><span class=line>Labels:        app=gateway</span><br><span class=line>    pod-template-hash=551394438</span><br><span class=line>Replicas:    1 current / 3 desired</span><br><span class=line>Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed</span><br><span class=line>No volumes.</span><br><span class=line>Events:</span><br><span class=line>FirstSeen    LastSeen    Count   From                SubObjectPath   Type        Reason          Message</span><br><span class=line>---------    --------    -----   ----                -------------   --------    ------          -------</span><br><span class=line>11m        11m     1   {replicaset-controller }            Normal      SuccessfulCreate    Created pod: gateway-quota-551394438-pix5d</span><br><span class=line>11m        30s     33  {replicaset-controller }            Warning     FailedCreate        Error creating: pods "gateway-quota-551394438-" is forbidden: exceeded quota: compute-resources, requested: pods=1, used: pods=1, limited: pods=1</span><br></pre></table></figure><p>上面可以看出，我们的 ReplicaSet 无法创建更多的 pods 了，因为配额限制了：exceeded quota: compute-resources, requested: pods=1, used: pods=1, limited: pods=1。<p>和资源限额类似，我们现在也有3个选项：<ul><li>要求集群管理员提升该 namespace 的配额<li>删除或者收缩该 namespace 下其它的 deployment<li>直接编辑配额</ul><h2 id=7-集群资源不足><a title="7. 集群资源不足" class=headerlink href=#7-集群资源不足></a>7. 集群资源不足</h2><p>除非你的集群开通了集群自动伸缩功能，否则总有一天你的集群中 CPU 和内存资源会耗尽。这不是说 CPU 和内存被完全使用了,而是指它们被 Kubernetes 调度器完全使用了。如同我们在第 5 点看到的，集群管理员可以限制开发者能够申请分配给 pod 或者容器的 CPU 或者内存的数量。聪明的管理员也会设置一个默认的 CPU/内存 申请数量，在开发者未提供申请额度时使用。<p>如果你所有的工作都在 default 这个 namespace 下工作，你很可能有个默认值 100m 的容器 CPU申请额度，对此你甚至可能都不清楚。运行 kubectl describe ns default 检查一下是否如此。我们假定你的 Kubernetes 集群只有一个包含 CPU 的节点。你的 Kubernetes 集群有 1000m 的可调度 CPU。当前忽略其它的系统 pods（kubectl -n kube-system get pods），你的单节点集群能部署 10 个 pod(每个 pod 都只有一个包含 100m 的容器)。<p>10 Pods _ (1 Container _ 100m) = 1000m == Cluster CPUs<p>当你扩大到 11 个的时候，会发生什么？下面是一个申请 1CPU（1000m）的 deployment 例子<figure class="highlight yaml"><table><tr><td class=code><pre><span class=line><span class=comment># cpu-scale.yaml</span></span><br><span class=line><span class=attr>apiVersion:</span> <span class=string>extensions/v1beta1</span></span><br><span class=line><span class=attr>kind:</span> <span class=string>Deployment</span></span><br><span class=line><span class=attr>metadata:</span></span><br><span class=line><span class=attr>name:</span> <span class=string>cpu-scale</span></span><br><span class=line><span class=attr>spec:</span></span><br><span class=line><span class=attr>template:</span></span><br><span class=line><span class=attr>metadata:</span></span><br><span class=line>  <span class=attr>labels:</span></span><br><span class=line>    <span class=attr>app:</span> <span class=string>cpu-scale</span></span><br><span class=line><span class=attr>spec:</span></span><br><span class=line>  <span class=attr>containers:</span></span><br><span class=line>    <span class=bullet>-</span> <span class=attr>name:</span> <span class=string>test-container</span></span><br><span class=line>      <span class=attr>image:</span> <span class=string>nginx</span></span><br><span class=line>      <span class=attr>resources:</span></span><br><span class=line>        <span class=attr>requests:</span></span><br><span class=line>          <span class=attr>cpu:</span> <span class=number>1</span></span><br></pre></table></figure><figure class="highlight shell"><table><tr><td class=code><pre><span class=line>我把这个应用部署到有 2 个可用 CPU 的集群。除了我的 cpu-scale 应用，Kubernetes 内部服务也在消耗 CPU 和内存。</span><br><span class=line></span><br><span class=line>我们可以用 kubectl create -f cpu-scale.yaml 部署这个应用，并观察 pods：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl get pods</span></span><br><span class=line>NAME                        READY     STATUS    RESTARTS   AGE</span><br><span class=line>cpu-scale-908056305-xstti   1/1       Running   0          5m</span><br><span class=line></span><br><span class=line>第一个 pod 被调度并运行了。我们看看扩展一个会发生什么：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl scale deploy/cpu-scale --replicas=2</span></span><br><span class=line>deployment "cpu-scale" scaled</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl get pods</span></span><br><span class=line>NAME                        READY     STATUS    RESTARTS   AGE</span><br><span class=line>cpu-scale-908056305-phb4j   0/1       Pending   0          4m</span><br><span class=line>cpu-scale-908056305-xstti   1/1       Running   0          5m</span><br><span class=line></span><br><span class=line>我们的第二个pod一直处于 Pending，被阻塞了。我们可以 describe 这第二个 pod 查看更多的信息:</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl describe pod cpu-scale-908056305-phb4j</span></span><br><span class=line>Name:        cpu-scale-908056305-phb4j</span><br><span class=line>Namespace:    fail</span><br><span class=line>Node:        gke-ctm-1-sysdig2-35e99c16-qwds/10.128.0.4</span><br><span class=line>Start Time:    Sun, 12 Feb 2017 08:57:51 -0500</span><br><span class=line>Labels:        app=cpu-scale</span><br><span class=line>    pod-template-hash=908056305</span><br><span class=line>Status:        Pending</span><br><span class=line>IP:</span><br><span class=line>Controllers:    ReplicaSet/cpu-scale-908056305</span><br><span class=line>[...]</span><br><span class=line>Events:</span><br><span class=line>FirstSeen    LastSeen    Count   From            SubObjectPath   Type        Reason          Message</span><br><span class=line>---------    --------    -----   ----            -------------   --------    ------          -------</span><br><span class=line>3m        3m      1   {default-scheduler }            Warning     FailedScheduling    pod (cpu-scale-908056305-phb4j) failed to fit in any node</span><br><span class=line>fit failure on node (gke-ctm-1-sysdig2-35e99c16-wx0s): Insufficient cpu</span><br><span class=line>fit failure on node (gke-ctm-1-sysdig2-35e99c16-tgfm): Insufficient cpu</span><br><span class=line>fit failure on node (gke-ctm-1-sysdig2-35e99c16-qwds): Insufficient cpu</span><br></pre></table></figure><p>Events 模块告诉我们 Kubernetes 调度器（default-scheduler）无法调度这个 pod 因为它无法匹配任何节点。它甚至告诉我们每个节点哪个扩展点失败了（Insufficient cpu）。<p>那么我们如何解决这个问题？如果你太渴望你申请的 CPU/内存 的大小，你可以减少申请的大小并重新部署。当然，你也可以请求你的集群管理员扩展这个集群（因为很可能你不是唯一一个碰到这个问题的人）。<p>现在你可能会想：我们的 Kubernetes 节点是在我们的云提供商的自动伸缩群组里，为什么他们没有生效呢？原因是，你的云提供商没有深入理解 Kubernetes 调度器是做啥的。利用 Kubernetes 的集群自动伸缩能力允许你的集群根据调度器的需求自动伸缩它自身。如果你在使用 GCE，集群伸缩能力是一个 beta 特性。<h2 id=8-持久化卷挂载失败><a title="8. 持久化卷挂载失败" class=headerlink href=#8-持久化卷挂载失败></a><strong>8. 持久化卷挂载失败</strong></h2><p>另一个常见错误是创建了一个引用不存在的持久化卷（PersistentVolumes）的 deployment。不论你是使用 PersistentVolumeClaims（你应该使用这个！），还是直接访问持久化磁盘，最终结果都是类似的。<p>下面是我们的测试 deployment，它想使用一个名为 my-data-disk 的 GCE 持久化卷：<figure class="highlight shell"><table><tr><td class=code><pre><span class=line><span class="meta prompt_"># </span><span class=language-bash>volume-test.yaml</span></span><br><span class=line>apiVersion: extensions/v1beta1</span><br><span class=line>kind: Deployment</span><br><span class=line>metadata:</span><br><span class=line>name: volume-test</span><br><span class=line>spec:</span><br><span class=line>template:</span><br><span class=line>metadata:</span><br><span class=line>  labels:</span><br><span class=line>    app: volume-test</span><br><span class=line>spec:</span><br><span class=line>  containers:</span><br><span class=line>    - name: test-container</span><br><span class=line>      image: nginx</span><br><span class=line>      volumeMounts:</span><br><span class=line>      - mountPath: /test</span><br><span class=line>        name: test-volume</span><br><span class=line>  volumes:</span><br><span class=line>  - name: test-volume</span><br><span class=line>    # This GCE PD must already exist (oops!)</span><br><span class=line>    gcePersistentDisk:</span><br><span class=line>      pdName: my-data-disk</span><br><span class=line>      fsType: ext4</span><br><span class=line></span><br><span class=line>让我们创建这个 deployment：kubectl create -f volume-test.yaml，过几分钟后查看 pod：</span><br><span class=line>kubectl get pods</span><br><span class=line>NAME                           READY     STATUS              RESTARTS   AGE</span><br><span class=line>volume-test-3922807804-33nux   0/1       ContainerCreating   0          3m</span><br><span class=line></span><br><span class=line>3 分钟的等待容器创建时间是很长了。让我们用 describe 来查看这个 pod，看看到底发生了什么：</span><br><span class=line><span class="meta prompt_">$ </span><span class=language-bash>kubectl describe pod volume-test-3922807804-33nux</span></span><br><span class=line>Name:        volume-test-3922807804-33nux</span><br><span class=line>Namespace:    fail</span><br><span class=line>Node:        gke-ctm-1-sysdig2-35e99c16-qwds/10.128.0.4</span><br><span class=line>Start Time:    Sun, 12 Feb 2017 09:24:50 -0500</span><br><span class=line>Labels:        app=volume-test</span><br><span class=line>    pod-template-hash=3922807804</span><br><span class=line>Status:        Pending</span><br><span class=line>IP:</span><br><span class=line>Controllers:    ReplicaSet/volume-test-3922807804</span><br><span class=line>[...]</span><br><span class=line>Volumes:</span><br><span class=line>test-volume:</span><br><span class=line>Type:    GCEPersistentDisk (a Persistent Disk resource in Google Compute Engine)</span><br><span class=line>PDName:    my-data-disk</span><br><span class=line>FSType:    ext4</span><br><span class=line>Partition:    0</span><br><span class=line>ReadOnly:    false</span><br><span class=line>[...]</span><br><span class=line>Events:</span><br><span class=line>FirstSeen    LastSeen    Count   From                        SubObjectPath   Type        Reason      Message</span><br><span class=line>---------    --------    -----   ----                        -------------   --------    ------      -------</span><br><span class=line>4m        4m      1   {default-scheduler }                        Normal      Scheduled   Successfully assigned volume-test-3922807804-33nux to gke-ctm-1-sysdig2-35e99c16-qwds</span><br><span class=line>1m        1m      1   {kubelet gke-ctm-1-sysdig2-35e99c16-qwds}           Warning     FailedMount Unable to mount volumes for pod "volume-test-3922807804-33nux_fail(e2180d94-f12e-11e6-bd01-42010af0012c)": timeout expired waiting for volumes to attach/mount for pod "volume-test-3922807804-33nux"/"fail". list of unattached/unmounted volumes=[test-volume]</span><br><span class=line>1m        1m      1   {kubelet gke-ctm-1-sysdig2-35e99c16-qwds}           Warning     FailedSync  Error syncing pod, skipping: timeout expired waiting for volumes to attach/mount for pod "volume-test-3922807804-33nux"/"fail". list of unattached/unmounted volumes=[test-volume]</span><br><span class=line>3m        50s     3   {controller-manager }                       Warning     FailedMount Failed to attach volume "test-volume" on node "gke-ctm-1-sysdig2-35e99c16-qwds" with: GCE persistent disk not found: diskName="my-data-disk" zone="us-central1-a"</span><br><span class=line></span><br></pre></table></figure><p>Events 模块留有我们一直在寻找的线索。我们的 pod 被正确调度到了一个节点（Successfully assigned volume-test-3922807804-33nux to gke-ctm-1-sysdig2-35e99c16-qwds），但是那个节点上的 kubelet 无法挂载期望的卷 test-volume。那个卷本应该在持久化磁盘被关联到这个节点的时候就被创建了，但是，正如我们看到的，controller-manager 失败了：Failed to attach volume "test-volume" on node "gke-ctm-1-sysdig2-35e99c16-qwds" with: GCE persistent disk not found: diskName="my-data-disk" zone="us-central1-a"。<p>最后一条信息相当清楚了：为了解决这个问题，我们需要在 GKE 的 us-central1-a 区中创建一个名为 my-data-disk 的持久化卷。一旦这个磁盘创建完成，controller-manager 将挂载这块磁盘，并启动容器创建过程。<h2 id=9-校验错误><a title="9. 校验错误" class=headerlink href=#9-校验错误></a><strong>9. 校验错误</strong></h2><p>看着整个 build-test-deploy 任务到了 deploy 步骤却失败了，原因竟是 Kubernetes 对象不合法。还有什么比这更让人沮丧的！<figure class="highlight plaintext"><table><tr><td class=code><pre><span class=line>你可能之前也碰到过这种错误:</span><br><span class=line>$ kubectl create -f test-application.deploy.yaml</span><br><span class=line>error: error validating "test-application.deploy.yaml": error validating data: found invalid field resources for v1.PodSpec; if you choose to ignore these errors, turn validation off with --validate=false</span><br><span class=line></span><br><span class=line>在这个例子中，我尝试创建以下 deployment：</span><br><span class=line># test-application.deploy.yaml</span><br><span class=line>apiVersion: extensions/v1beta1</span><br><span class=line>kind: Deployment</span><br><span class=line>metadata:</span><br><span class=line>name: test-app</span><br><span class=line>spec:</span><br><span class=line>template:</span><br><span class=line>metadata:</span><br><span class=line>  labels:</span><br><span class=line>    app: test-app</span><br><span class=line>spec:</span><br><span class=line>  containers:</span><br><span class=line>  - image: nginx</span><br><span class=line>    name: nginx</span><br><span class=line>  resources:</span><br><span class=line>    limits:</span><br><span class=line>      cpu: 100m</span><br><span class=line>      memory: 200Mi</span><br><span class=line>    requests:</span><br><span class=line>      cpu: 100m</span><br><span class=line>      memory: 100Mi</span><br></pre></table></figure><p>一眼望去，这个 YAML 文件是正确的，但错误消息会证明是有用的。错误说的是 found invalid field resources for v1.PodSpec，再仔细看一下 v1.PodSpec， 我们可以看到 resource 对象变成了 v1.PodSpec的一个子对象。事实上它应该是 v1.Container 的子对象。在把 resource 对象缩进一层后，这个 deployment 对象就可以正常工作了。<p>除了查找缩进错误，另一个常见的错误是写错了对象名（比如 peristentVolumeClaim 写成了 persistentVolumeClaim），这样的错误有时会很费你的时间！<p>为了能在早期就发现这些错误，我推荐在 pre-commit 钩子或者构建的测试阶段添加一些校验步骤。例如，你可以：<br><strong>1.</strong> 用 python -c 'import yaml,sys;yaml.safe_load(sys.stdin)' < test-application.deployment.yaml 验证 YAML 格式<br><strong>2.</strong> 使用标识 --dry-run 来验证 Kubernetes API 对象，比如这样：kubectl create -f test-application.deploy.yaml --dry-run --validate=true<p><strong>重要提醒：</strong>校验 Kubernetes 对象的机制是在服务端的校验，这意味着 kubectl 必须有一个在工作的 Kubernetes 集群与之通信。不幸的是，当前 kubectl 还没有客户端的校验选项，但是已经有 issue（kubernetes/kubernetes #29410 和 kubernetes/kubernetes #11488）在跟踪这个缺失的特性了。<h2 id=10-容器镜像没有更新><a title="10. 容器镜像没有更新" class=headerlink href=#10-容器镜像没有更新></a><strong>10. 容器镜像没有更新</strong></h2><p>可能使用 Kubernetes 的大多数人都碰到过这个问题，它也确实是一个难题。<p>这个场景就像下面这样：<br><strong>1.</strong> 使用一个镜像 tag（比如：rosskulinski/myapplication:v1） 创建一个 deployment<br><strong>2.</strong> 注意到 myapplication 镜像中存在一个 bug<br><strong>3.</strong> 构建了一个新的镜像，并推送到了相同的 tag（rosskukulinski/myapplication:v1）<br><strong>4.</strong> 删除了所有 myapplication 的 pods，新的实例被 deployment 创建出了<br><strong>5.</strong> 发现 bug 仍然存在<br><strong>6.</strong> 重复 3-5 步直到你抓狂为止<p>这个问题关系到 Kubernetes 在启动 pod 内的容器时是如何决策是否做 docker pull 动作的。<p>在 v1.Container 说明中，有一个选项 ImagePullPolicy：<figure class="highlight plaintext"><table><tr><td class=code><pre><span class=line>Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.</span><br></pre></table></figure><p>因为我们把我们的镜像 tag 标记为 :v1，默认的镜像拉取策略是 IfNotPresent。Kubelet 在本地已经有一份 rosskukulinski/myapplication:v1 的拷贝了，因此它就不会在做 docker pull 动作了。当新的 pod 出现的时候，它仍然使用了老的有问题的镜像。<p>有三个方法来解决这个问题：<br><strong>1.</strong> 切成 :latest tag（千万不要这么做！）<br><strong>2.</strong> deployment 中指定 ImagePullPolicy: Always<br><strong>3.</strong> 使用唯一的 tag（比如基于你的代码版本控制器的 commit id）<p>在开发阶段或者要快速验证原型的时候，我会指定 ImagePullPolicy: Always 这样我可以使用相同的 tag 来构建和推送。然而，在我的产品部署阶段，我使用基于 Git SHA-1 的唯一 tag。这样很容易查到产品部署的应用使用的源代码。<p>所以说，当使用kubernetes时，我们有这么多地方要当心，一般来说，大部分常见的部署失败都可以用下面的命令定位出来：<br><strong>1.</strong> kubectl describe deployment/<deployname><br><strong>2.</strong> kubectl describe replicaset/<rsname><br><strong>3.</strong> kubectl get pods<br><strong>4.</strong> kubectl describe pod/<podname><br><strong>5.</strong> kubectl logs <podname> --previous <p>下面是一个bash脚本，它在 CI/CD 的部署过程中任何失败的时候，都可以跑。在 Jenkins等的构建输出中，将显示有用的 Kubernetes 信息，帮助开发者快速找到任何明显的问题。</p> <figure class="highlight plaintext"><table><tr><td class=code><pre><span class=line>#!/bin/bash</span><br><span class=line></span><br><span class=line>if [ -z "$1" ]</span><br><span class=line>then</span><br><span class=line>  echo "ERROR: No deployment specified"</span><br><span class=line>  exit 1</span><br><span class=line>fi</span><br><span class=line></span><br><span class=line>DEPLOY=${1}</span><br><span class=line>NAMESPACE=${2:=default}</span><br><span class=line></span><br><span class=line>printf "\n\nOk - Let's figure out why this deployment might have failed"</span><br><span class=line></span><br><span class=line>printf "\n\n------------------------------\n\n"</span><br><span class=line></span><br><span class=line>printf "> kubectl describe deployment ${DEPLOY} --namespace=${NAMESPACE}\n\n"</span><br><span class=line>kubectl describe deployment ${DEPLOY} --namespace=${NAMESPACE}</span><br><span class=line></span><br><span class=line>printf "\n\n------------------------------\n\n"</span><br><span class=line></span><br><span class=line>CURRENT_GEN=$(kubectl get deployment ${DEPLOY} --namespace=${NAMESPACE} -o jsonpath='{.metadata.generation}')</span><br><span class=line>OBS_GEN=$(kubectl get deployment ${DEPLOY} --namespace=${NAMESPACE} -o jsonpath='{.status.observedGeneration}')</span><br><span class=line>REPLICAS=$(kubectl get deployment ${DEPLOY} --namespace=${NAMESPACE} -o jsonpath='{.status.replicas}')</span><br><span class=line>UPDATED_REPLICAS=$(kubectl get deployment ${DEPLOY} --namespace=${NAMESPACE} -o jsonpath='{.status.updatedReplicas}')</span><br><span class=line>AVAILABLE_REPLICAS=$(kubectl get deployment ${DEPLOY} --namespace=${NAMESPACE} -o jsonpath='{.status.availableReplicas}')</span><br><span class=line></span><br><span class=line>if [ "$AVAILABLE_REPLICAS" == "$REPLICAS" ] && \</span><br><span class=line>   [ "$UPDATED_REPLICAS" == "$REPLICAS" ] ; then</span><br><span class=line></span><br><span class=line>  printf "Available Replicas (${AVAILABLE_REPLICAS}) equals Current Replicas (${REPLICAS}) \n"</span><br><span class=line>  printf "Updated Replicas (${UPDATED_REPLICAS}) equals Current Replicas (${REPLICAS}). \n"</span><br><span class=line>  printf "Are you sure the deploy failed?\n\n"</span><br><span class=line>  exit 0</span><br><span class=line>fi</span><br><span class=line></span><br><span class=line>if [ "$AVAILABLE_REPLICAS" != "$REPLICAS" ] ; then</span><br><span class=line>  printf "Available Replicas (${AVAILABLE_REPLICAS}) does not equal Current Replicas (${REPLICAS}) \n"</span><br><span class=line>fi</span><br><span class=line></span><br><span class=line>if [ "$UPDATED_REPLICAS" != "$REPLICAS" ] ; then</span><br><span class=line>  printf "Updated Replicas (${UPDATED_REPLICAS}) does not equal Current Replicas (${REPLICAS}) \n"</span><br><span class=line>fi</span><br><span class=line></span><br><span class=line>printf "\n\n------------------------------\n\n"</span><br><span class=line></span><br><span class=line>NEW_RS=$(kubectl describe deploy ${DEPLOY} --namespace=${NAMESPACE} | grep "NewReplicaSet" | awk '{print $2}')</span><br><span class=line>POD_HASH=$(kubectl get rs ${NEW_RS} --namespace=${NAMESPACE} -o jsonpath='{.metadata.labels.pod-template-hash}')</span><br><span class=line></span><br><span class=line>printf "Pods for this deployment:\n\n"</span><br><span class=line>printf "> kubectl get pods  --namespace=${NAMESPACE} -l pod-template-hash=${POD_HASH}\n\n"</span><br><span class=line>kubectl get pods --namespace=${NAMESPACE} -l pod-template-hash=${POD_HASH}</span><br><span class=line></span><br><span class=line>printf "\n\n------------------------------\n\n"</span><br><span class=line></span><br><span class=line>printf "Detailed pods for this deployment:\n\n"</span><br><span class=line></span><br><span class=line>printf "> kubectl describe pods  --namespace=${NAMESPACE} -l pod-template-hash=${POD_HASH}\n\n"</span><br><span class=line>kubectl describe pods --namespace=${NAMESPACE} -l pod-template-hash=${POD_HASH}</span><br><span class=line></span><br><span class=line>printf "\n\n------------------------------\n\n"</span><br><span class=line>printf "Containers that are currently 'waiting':\n\n"</span><br><span class=line>printf "> kubectl get pods --namespace=${NAMESPACE} -l pod-template-hash=${POD_HASH} -o jsonpath='...'\n"</span><br><span class=line>kubectl get pods --namespace=${NAMESPACE} -l pod-template-hash=${POD_HASH} -o jsonpath='{"\n"}{range .items[*]}{@.metadata.name}:{"\n"}{range @.status.conditions[*]}{"\t"}{@.lastTransitionTime}: {@.type}={@.status}{"\n"}{end}{"\n"}{"\tWaiting Containers\n"}{range @.status.containerStatuses[?(@.state.waiting)]}{"\t\tName: "}{@.name}{"\n\t\tImage: "}{@.image}{"\n\t\tState: Waiting"}{"\n\t\tMessage: "}{@.state.waiting.message}{"\n\t\tReason: "}{@.state.waiting.reason}{end}{"\n"}{end}'</span><br><span class=line></span><br><span class=line>printf "\n\n------------------------------\n\n"</span><br><span class=line></span><br><span class=line>printf "Pods with Terminated state\n\n"</span><br><span class=line></span><br><span class=line>printf "> kubectl get pods --namespace=${NAMESPACE} -l pod-template-hash=${POD_HASH} -o jsonpath='...'\n"</span><br><span class=line>kubectl get pods --namespace=${NAMESPACE} -l pod-template-hash=${POD_HASH} -o jsonpath='{"\n"}{range .items[*]}{"\n"}{@.metadata.name}:{"\n"}{"\n\tTerminated Containers\n"}{range @.status.containerStatuses[?(@.lastState.terminated)]}{"\t\tName: "}{@.name}{"\n\t\tImage: "}{@.image}{"\n\t\texitCode: "}{@.lastState.terminated.exitCode}{"\n\t\tReason: "}{@.lastState.terminated.reason}{"\n"}{end}{"\n"}{end}'</span><br><span class=line></span><br><span class=line>printf "\n\n------------------------------\n\n"</span><br><span class=line></span><br><span class=line>printf "Trying to get previous logs from each Terminated pod\n\n"</span><br><span class=line></span><br><span class=line>kubectl get pods --namespace=${NAMESPACE} -l pod-template-hash=${POD_HASH} --no-headers | awk '{print $1}' | xargs -I pod sh -c "printf \"pod\n\n\"; kubectl --namespace=${NAMESPACE} logs --previous --tail=100 --timestamps pod; printf \"\n\n\""</span><br><span class=line></span><br></pre></table></figure> <footer class=post-footer><div class=post-copyright><ul><li class=post-copyright-author><strong>本文作者： </strong>JiaHe<li class=post-copyright-link><strong>原文链接：</strong> <a title="Kubernetes 运维学习笔记" href=https://www.cnblogs.com/kevingrace/p/5575666.html rel=noopener target=_blank>https://www.cnblogs.com/kevingrace/p/5575666.html</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=post-tags><a href=/tags/%E8%BF%90%E7%BB%B4/ rel=tag># 运维</a><a href=/tags/Kubernetes/ rel=tag># Kubernetes</a></div><div class=post-nav><div class=post-nav-item><a href=/archives/b84bf15e.html rel=prev title=Properties中读取斜杠(\)的问题及解决办法> <i class="fa fa-angle-left"></i> Properties中读取斜杠(\)的问题及解决办法 </a></div><div class=post-nav-item><a href=/archives/cda714ca.html rel=next title=Docker私有仓库Harbor介绍和部署记录> Docker私有仓库Harbor介绍和部署记录 <i class="fa fa-angle-right"></i> </a></div></div></footer> <footer class=footer><div class=footer-inner><div class=beian><a href=https://beian.miit.gov.cn/ rel=noopener target=_blank>湘ICP备19011756号-1 </a></div><div class=copyright>© 2021 – <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>JiaHe</span></div><div class=wordcount><span class=post-meta-item> <span class=post-meta-item-icon> <i class="fa fa-chart-line"></i> </span> <span title=站点总字数>331k</span> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span> <span title=站点阅读时长>20:05</span> </span></div><div class=busuanzi-count><span class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div><div class=powered-by>由 <a href=https://hexo.io/ rel=noopener target=_blank>Hexo</a> & <a href=https://theme-next.js.org/ rel=noopener target=_blank>NexT.Gemini</a> 强力驱动</div></div></footer> <div class="toggle sidebar-toggle" role=button><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div> <div class=sidebar-dimmer></div> <a class="book-mark-link book-mark-link-fixed" role=button></a> <noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript> <script alpha=0.6 size=300 src=https://use.sevencdn.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js zindex=-1></script> <script crossorigin integrity=sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY= src=https://use.sevencdn.com/ajax/libs/animejs/3.2.1/anime.min.js></script> <script crossorigin integrity=sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8= src=https://use.sevencdn.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js></script> <script crossorigin integrity=sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc= src=https://use.sevencdn.com/ajax/libs/lozad.js/1.16.0/lozad.min.js></script> <script crossorigin integrity=sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I= src=https://use.sevencdn.com/ajax/libs/pangu/4.0.7/pangu.min.js></script> <script src=https://use.sevencdn.com/ajax/libs/hexo-theme-next/8.21.1/comments.min.js></script><script src=https://use.sevencdn.com/ajax/libs/hexo-theme-next/8.21.1/utils.min.js></script><script src=https://use.sevencdn.com/ajax/libs/hexo-theme-next/8.21.1/sidebar.min.js></script><script src=https://use.sevencdn.com/ajax/libs/hexo-theme-next/8.21.1/next-boot.min.js></script><script src=https://use.sevencdn.com/ajax/libs/hexo-theme-next/8.21.1/bookmark.min.js></script> <script crossorigin integrity=sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc= src=https://use.sevencdn.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js></script> <script src=https://use.sevencdn.com/ajax/libs/hexo-theme-next/8.21.1/third-party/search/local-search.min.js></script> <script class=next-config data-name=pdf type=application/json>{"object_url":{"url":"https://use.sevencdn.com/ajax/libs/pdfobject/2.3.0/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script> <script src=https://use.sevencdn.com/ajax/libs/hexo-theme-next/8.21.1/third-party/tags/pdf.min.js></script> <script src=https://use.sevencdn.com/ajax/libs/hexo-theme-next/8.21.1/third-party/fancybox.min.js></script> <script async src=https://cn.vercount.one/js></script> <script crossorigin integrity=sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM= src=https://use.sevencdn.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js></script> <script class=next-config data-name=quicklink type=application/json>{"enable":true,"home":false,"archive":true,"delay":true,"timeout":3000,"priority":true,"ignores":null,"url":"http://xiaojianzheng.cn/archives/cf9b7124.html"}</script> <script src=https://use.sevencdn.com/ajax/libs/hexo-theme-next/8.21.1/third-party/quicklink.min.js></script> 